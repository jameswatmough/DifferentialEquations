---
author: James  Watmough
date: "`r Sys.Date()`"
---

{{< include texmacros.qmd >}}

# Second order Linear Differential Equations {#sec-2ndorder}

## The general theory of second order Linear Differential Equations {#sec-2ndordergt}

The first part of this course deals with second order linear differential
equations with constant coefficients.  However before we zero in on solution
techniques for these equations it is worthwhile to take a brief look at the
more general theory.  

Many physical problems can be cast as second order differential equations of the form
$\ddot{x} = f(t,x,\dot{x})$
with $\ddot{x}$ interpreted as acceleration, $x$ as position, and $\dot{x}$ as velocity.
These often arise as initial value problems, when combined with initial conditions of
the form $x(0) = x_0$, $\dot{x}(0) = y_0$.     
Some cases appear as boundary value problems with the conditions prescribed at two different
values of the independent variable.
$x(0) = a$, $x(1) = b$.

### Differential Equations and Linear Algebra

A second order linear differential equation has the form.
$$P(x)\frac{d^2y}{dx^2} + Q(x)\frac{dy}{dx} + R(x)y = G(x),$$ {#eq-genericode}
where the functions $P$, $Q$, $R$ and $G$ are specified.
To motivate these as **linear** we need to review a few concepts from linear algebra.  In particular,  we need to learn to view functions as vectors.

One key concept from linear algebra is a **linear combination**.
Suppose $y_1, y_2, \dots, y_n$ are $n$ functions defined on some interval $[a,b]$.  For example, the set $1, x, x^2, x^3, x^4, x^5$ with $0 \le x \le 1$ is a set of 6 functions defined on the interval $[0,1]$.
Using the jargon we learned in our first linear algebra course, we can construct any polynomial of degree 5 or lower by taking **linear combinations** of the 6 **basis** functions.
Specifically, if $c_0, c_1, \dots, c_5$ be any 6 numbers, then
$y(x) = c_0 + c_1x + \dots + c_5x^5$ is a polynomial.  If $c_5\ne0$, $y$ is a polynomial of degree 5, otherwise, it has a lower degree.
We can do the same constructions for any set of functions.  Given a set of functions $\{y_1, y_2, \dots, y_n\}$ all with the same domain, $[a,b]$, and a set of **scalars** $\{c_1, \dots, c_n\}$, the **linear combination**
$y = c_1y_1 + \dots + c_ny_n$ is a function with the same domain.  
From our calculus course, we know that if $y_1, \dot, y_n$ are all continuous on $[a,b]$, then the linear combination is also continuous.
In short,  functions are vectors, and all the jargon we learned in linear algebra can be applied to functions.

::: {#def-linindep}
The set of functions $\{y_1, y_2, \dots, y_n\}$ is said to be **linearly independent** if the only scalars $c_1,\dots,c_n$ for which
$c_1y_1 + \dots + c_ny_n = 0$ is $c_1=c_2=\dots=c_n=0$.
:::

::: {#exm-linindep-1}
To show the two functions $\sin x$ and $\cos x$ defined for $0 \le x \le \pi$ are linearly independent, we need to show that $c_1\sin x + c_2\cos x = 0$ for all $x\in[0,\pi]$ only if $c_1=c_2=0$.  Since $\sin 0 = 0$ and $\cos 0 = 1$, setting $x=0$ implies $c_2=0$.  Similarly, setting $x=\pi/2$ implies $c_1=0$.  Hence,  the two functions are linearly independent.
:::

So why do we call @eq-genericode a **linear** differential equation?
It is useful to introduce a shorthand notation for the left hand side 
of @eq-genericode.
In more advanced courses this would be introduced as a linear operator
and some fancy theorems would be invoked.
An operator is simply a mathematical object that maps functions to other
functions in the same way the functions of first year calculus map numbers to
other numbers.
Suppose $y$ is twice differentiable, then we can define a new function, $Ly$,
by 
\begin{equation}
  Ly = P\frac{d^2y}{dx^2} + Q\frac{dy}{dx} + Ry.
\end{equation}
We use this to define the operator $L$.
Simply put,  given a known, twice differentiable function $y$, the operation $L$ returns a new function which we refer to simply as $Ly$.
The  linearity of the operator refers to the fact that $L$ is a **linear transformation**.
That is, for any two twice-differentiable functions $u$ and $v$ and any two real numbers (scalars) $a$ and
$b$, $L(au+bv) = aLu + bLv$.


### The solution set

A solution to the second order differential equation @eq-genericode, is any twice differentiable function $y(x)$
that satisfies @eq-genericode.
Since we are looking for $y$ as a function of $x$, it makes sense to refer to 
$x$ as our **independent variable**
and $y$ as our **dependent variable**.
The linearity of the equation makes some more jargon from linear algebra useful.  In particular,

* $P$, $Q$ and $R$ are the **coefficients** of the equation,
* the equation is **homogeneous** if $G=0$,
* the equation is **nonhomogeneous** if $G\ne 0$,
* the equation is **constant coefficient** if $P$, $Q$ and $R$ constants.

The right hand side, $G$, is typically an applied force or current or something similar.  Hence we refer to $G$ as the **forcing function**.

There are two theorems that are relevant here.

::: {#thm-1}
If $y_1$ and $y_2$ are solutions of the homogeneous problem
then for any constants $c_1$ and $c_2$, the linear combination $y = c_1y_1 + c_2y_2$ is also a solution of the homogeneous problem.
:::

In the language of linear algebra, any linear combination of two solutions is also a solution.
The proof follows almost immediately from substituting the linear combination of the two solutions
into the differential equation.

::: {#thm-homog}
If $y_1$ and $y_2$ are two linearly independent solutions to the homogeneous problem and $y_p$ is any solution to the
nonhomogeneous problem, then $y = c_1y_1 + c_2y_2 + y_p$ is also a solution to the nonhomogeneous problem
for any constant $a$.
:::

Again,  the proof is simply a matter of plugging the proposed solution into the differential equation.

::: {#thm-nonhomog}
If $y_1$ and $y_2$ are two linear independent solutions of the homogeneous problem and $y_p$ is any solution to the
nonhomogeneous problem, 
then every solution of the homogeneous problem can be written 
in the form $y = c_1y_1 + c_2y_2 + y_p$ for some real numbers $c_1$ and $c_2$.
:::

In short,  this theorem states that if we can find two linearly independent
solutions, then we know all the solutions to the homogeneous problem,  and if
we also know any one solution to the nonhomogeneous problem, we know them all.
The form of the solution set should look very familiar.  It has the same
structure as the set of solutions to a system of linear algebraic equations.  

The proof of this theorem is beyond the scope of this course.  However, we will be able to give a sketch of the proof later in the term.


Many of  the theorems from linear algebra apply to the operator $L$.  We refer
to $Ly=0$ as the homogeneous, or complementary problem, and $Ly = G$ as the
nonhomogeneous problem.  As with linear systems,  if $y_p$ is a solution to the
nonhomogeneous problem and $y_h$ is any solution to the homogeneous problem,
then $y = cy_h + y_p$ is also a solution to the nonhomogeneous problem, for any
constant $c$.   This can be easily verified by direct substitution.  Further,
if $y_1$ and $y_2$ are both solutions to the homogeneous problem, then any
function of the form $y = c_1y_1 + c_2 y_2$ is also a solution to the
homogeneous problem.   Note the slight difference in the wording of these
results from the theorems stated above.  When $L$ is a second order linear
differential operator, such as arises from second order linear differential
equation, then it can further be shown that the homogeneous problem $Ly=0$ has
a two dimensional solution set.  That is, we can always find two linearly
independent solutions, $y_1$ and $y_2$ and every solution to $Ly=0$ can be
written as a linear combination of these.  This follows from a theorem for the
existence and uniqueness of solutions to systems of first order differential
equations, which will be sketched later in these notes.

Later in the notes we will also encounter the eigenvalue problem $Ly=\lambda y$, where we are interested in finding values of the constant $\lambda$ for which solutions to the eigenvalue problem exist.  These typically arise in boundary value problems, and the eigenvalues correspond to modes of oscillation in the physical system.

![Mass spring system](figures/ODE-spring-figure.pdf "Diagram of a Mass-Spring System"){#fig-mass-spring}

::: {#exm-MassSpring}
  Consider the mass-spring system of Figure @fig-mass-spring with a mass of $m$ attached to a spring with spring constant $k$.
  A differential equation model for the system can be obtained either by energy balance or force balance.  The energy of the 
  system is 
  \begin{align*}
    \text{Total Energy} &= \text{Kinetic Energy} + \text{Spring Potential} \\
    &= \frac{1}{2}m\dot{x}^2 + \frac{1}{2}kx^2
  \end{align*}
  Differentiating and assuming the total energy is conserved (constant) leads to the second order differential equation
  $0 = m\dot{x}\ddot{x} + kx\dot{x}$
  which simplifies to 
  $m\ddot{x} + kx = 0.$
  Since both $k$ and $m$ are positive constants, the solutions to the equation are functions whose second derivatives are negative multiples of themselves: $\ddot{x} = -\frac{k}{m}x$.  These are sines and cosines of the appropriate period:
  $$x_1(t) = \cos \tfrac{k}{m} t, \qquad x_2(t) = \sin \tfrac{k}{m} t.$$
  The general solution to the differential equation is any linear combination of these two solutions:
  $x(t) = c_1\cos \tfrac{k}{m} t \, + \, c_2 \sin \tfrac{k}{m} t.$
:::

## Linear Second order differential equations with constant coefficients {#sec-2ndordercc}

Consider the problem

$$ay'' + by' +cy = 0$$ {#eq-generic-ccoef}

Based on our experience with first order equations (a=0),
we look for a solution of the form $y=e^{rx}$.
If we can find two such solutions, we've got them all!
First, we substitute the ansatz, $y(x) = e^{rx}$, into @eq-genericode
\begin{align*}
  a\frac{d^2}{dx^2}\left(e^{rx}\right) + b\frac{d}{dx}\left(e^{rx}\right) +ce^{rx} &= 0 \\
  ar^2e^{rx} + bre^{rx} +ce^{rx} &= 0 \\
  \left(ar^2 + br +c\right)e^{rx} &= 0 
\end{align*}
Factoring out $e^{rx}$ we see that $r$ must satisfy
  $ar^2 + br +c = 0$.
That is,
$$r = \frac{-b \pm \sqrt{b^2-4ac}}{2a}.$$

There are three cases to consider.

**Case I:**  two real and distinct roots ($b^2-4ac>0$)

  The general solution has the form 
  $$y = c_1e^{r_1x} + c_2e^{r_2x}$$
  where $r_1$ and $r_2$ are the two real roots.

**Case II:** one real root ($b^2 - 4ac = 0$)

  Here we only have one independent solution $y_1 = c_1e^{rx}$ with $r=-b/(2a)$.

  At some point in history, someone stumbled upon a second independent solution, 
  $y_2 = xe^{rx}$, with $r=-b/(2a)$.    While this it is not at all obvious why
  this should be a solution and how someone would come up with it,  it is easy
  to verify that is is a solution and that $y_1$ and $y_2$ are linearly independent.

  We leave it as an exercise to show, by direct substitution, that $y_2$ solves [@eq-generic-ccoef].
  That is, show that 
$$a\frac{d^2}{dx^2}\left(xe^{rx}\right) + b\frac{d}{dx}\left(xe^{rx}\right) +cxe^{rx} = 0$$
provided $b^2 - 4ac =0$ and $r=b/(2a)$.

  To show the two solutions are linearly independent,  consider the combination
  $$\alpha e^{rx} + \beta xe^{rx} = 0.$$

  The key concept with linear independence of functions, is that the equality
  must hold for all $x$ in the domain of the functions.  In this case, since
  $e^{rx} > 0$, it follows that $\alpha + \beta x = 0$,  which holds only for
  $x=-\alpha/\beta$.  Hence the two functions are linearly independent on 
  any interval of nonzero length, which is all we care about.

  In summary,  the general solution in case II has the form
  $$y = (c_1+c_2x)e^{rx} $$
  with $r = -b/(2a)$.

**Case III:**  complex roots ($b^2 - 4ac < 0$)

In this case, we have two complex roots, which we will express as
  $r_1 = \alpha - \beta i$ and
  $r_2 = \alpha + \beta i$.
  For those who are comfortable with complex functions, you will be happy to 
  know that the two functions $e^{r_1 x}$ and $e^{r_2 x}$ are indeed linearly 
  independent.  
  The only catch is that we started with a problem involving real variables and
  real functions and it would be nice to have real solutions.  Fortunately,
  it is possible to show that the real and complex parts of the complex solutions
  are also linearly independent solutions.  Hence, the general solution has the form

  $$y(x) =  \left( c_1 \cos \beta x  + c_2 \sin  \beta x  \right) e^{\alpha x}$$

## Initial and boundary value problems

The general form of the solution has two arbitrary constants.  Thus it is a two-parameter family of solutions.   In practice,  we are often interested in a solution  satisfying additional constraints.  These are typically posed as either initial conditions or boundary conditions.  Initial conditions
are given as constraints of the form $y(0) = y_1$ and $y'(0) = y_2$ for some specified constants $y_1$ and $y_2$.  Boundary conditions are typically given as $y(0) = y_0$ and $y(1) = y_1$, but could involve derivatives of $y$ at the boundaries.

::: {#exm-SecondOrderLinear-1}
  Find a solution to $y'' + 2y' - 3 = 2x$ satisfying the initial conditions $y(0) = 0$ and $y'(0) = 1$. 
:::

## Summary

To find two linearly independent solutions to a homogeneous second order linear differential equation with constant coefficients,
$ay'' + by' + cy' = 0,$
first compute the characteristic roots
$r = \frac{-b \pm \sqrt{b^2-4ac}}{2a}.$
second,  determine which of the following three cases holds and write down the appropriate solutions:
\paragraph{Case I:} 
Two distinct real roots ($b^2 > 4ac$):
$y_1 = e^{r_1t} \qquad y_2 = e^{r_2t}$
with $r_1 = \frac{-b - \sqrt{b^2-4ac}}{2a}$, and $r_2 = \frac{-b + \sqrt{b^2-4ac}}{2a}$.
\paragraph{Case II:} 
A double root ($b^2 = 4ac$):
$y_1 = e^{rt} \qquad y_2 = te^{rt}$
with $r = \frac{-b}{2a}$.
\paragraph{Case III:} 
Complex roots ($b^2 < 4ac$):
$y_1 = e^{\alpha t} \cos \beta t \qquad y_2 = e^{\alpha t} \sin \beta t,$
with $\alpha = -\frac{b}{2a}$, and $\beta = \frac{\sqrt{4ac-b^2}}{2a}$.

## Discussion

Note that we refer to [@eq-genericode] as linear when it is linear in the dependent variable.
It may be nonlinear in the independent variable.  For example,  
applying Kirchoff's law to a simple electric circuit yields the model
$$LQ''(t) + RQ'(t) + \frac{1}{C}Q(t) = E(t)$$
where the dependent variable, $Q$, is the charge, and is assumed to depend on the time, $t$. The parameters $L$, $R$ and $C$ are the inductance, resistance and capacitance of the circuit and $E$ is an applied voltage.    If the fixed resistance $R$ is replaced by a variable resistance, $R(t)$, the equation is still linear in the dependent variable.  

More generally,  a differential equation is an equation involving a function and some of its derivatives.  
For example,  a simple model for the angular deflection $\theta(t)$ of a pendulum of length $l$ is 
$$l\theta''(t) + g\sin \theta(t) = 0,$$
which arises from balancing kinetic and potential energies.
This equation is nonlinear in the dependent variable $\theta$.  For small angles, we can approximate $\sin(\theta)$ by $\theta$ leading to the linear second order constant coefficient model
$$l\theta''(t) + g\theta(t) = 0.$$   Most systems are nonlinear.  However most analyses begin with the study of linear approximations.  Hence the importance of studying linear differential equations.

## Theoretical Considerations

A good treatment of the existence and uniqueness of solutions to differential equations can be found in Hartman.
This material is approachable, but requires a deeper exposure to linear algebra than you received in Math 1503. 

Consider the general Second order linear ODE of [@eq-genericode] with $G=0$.
Let $w(x) = y'(x)$ and write the single equation as 

$$P(x)w'(x) + Q(x)w(x) + R(x)y(x) = 0,$$

which leads to the linear system
\begin{align}
  w'(x) &=  - \frac{Q(x)}{P(x)}w(x) - \frac{R(x)}{P(x)}y(x), \\
  y'(x) &= w(x)
\end{align}
Hartman's text deals with the more general equation 
$\vec{y}' = f(x,\vec{y})$
where $\vec{y}$ and $f$ are vector functions.  The result for our simpler linear system
is that the initial value problem has a unique solution provided $Q/P$ and $R/P$ are both continuous.

## Finding particular solutions of  the nonhomogeneous problem

### Reduction of order

Consider the general linear second order differential equation,
$P(x)y'' + Q(x)y' + R(x)y = G(x)$,
and suppose we know one solution, $y_1$ to the homogeneous problem ($G=0$).
A well-established trick to finding more solutions to the differential equation is to look for solutions
of the form $y(x) = u(x)y_1(x)$.  
\begin{align*}
  P \left(u''y_1 + 2u'y_1'+uy_1''\right) + Q\left(u'y_1+uy_1'\right) + Ruy_1 &= G \\
  P \left(u''y_1 + 2u'y_1'\right) + Qu'y_1 + Puy_1''+Quy_1' +Ruy_1 &= G \\
  P \left(u''y_1 + 2u'y_1'\right) + Qu'y_1  = G 
\end{align*}
Since $y_1$ is a known solution, to the nonhomogeneous problem,  the last few terms cancel leaving us with
an equation that only involves the derivatives of $u$.  Setting $v = u'$ reduces the order to a first order
differential equation in $v$:
$Py_1 v' + \left(2y_1' + Qy_1\right)v  = G.$
This equation is first order and linear, and can be solved using an integrating factor.


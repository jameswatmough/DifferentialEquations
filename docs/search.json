[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Differential Equations in Mathematical Ecology",
    "section": "",
    "text": "Preamble\nThese pages contain lecture notes for a mix of undergraduate courses on differential equations. They were originally used for a first course on differential equations for engineers. Hence the second order equations for circuits and a pendulum and the odd snippet of python. The notes are currently (Winter 2025) being updated with notes from Math 4142/6142 at the University of New Brunswick, which is intended as a second course in differential equations for upper level undergraduates in mathematics and graduate students with an interest in ecological dynamics.",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "M3503-lec01-ODEintro.html",
    "href": "M3503-lec01-ODEintro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Motivating examples\nThe art of applied mathematics involves distilling a description of the state a system into relationships between variables. For example, consider a simple pendulum consisting of a mass on a rigid rod constrained to swing in a plane. The various quantities of interest are the mass, \\(m\\), the length of the rod, \\(l\\), the horizontal displacement, \\(x\\), the vertical displacement, \\(y\\), both measured, for example, relative to the fixed end of the rod, and the angle of displacement, \\(\\theta\\). These quantities are not independent. We know \\(x^2+y^2 = l^2\\), and we can choose to measure angle so that \\(x = \\sin \\theta\\) and \\(y = \\cos \\theta\\). Our description so far is static, and our equations are algebraic.\nTo describe the dynamics of the pendulum, we need to extend our mathematical description to include velocities, accelerations, forces and energies. If we neglect the mass of the rod, we can hypothesize that the component of the gravitational force perpendicular to the rod acts to accelerate the mass radially. \\[\\text{mass}\\times\\text{radial acceleration} = \\text{radial force of gravity}\\] \\[ml\\ddot{\\theta} = - m g \\sin \\theta \\tag{1.1}\\] Here \\(g\\) denotes the gravitational constant and we use the double dot notation to denote the second derivative with respect to time. If we assume the angle is small, we can approximate \\(\\sin \\theta\\) by \\(\\theta\\) and Equation Equation 1.1 simplifies to \\[ml\\ddot{\\theta} = - m g \\theta \\tag{1.2}\\]\nOur goal in this course is to understand differential equations like Equation 1.2. We’ll proceed more or less in the following order.\nAn ordinary differential equation is simply an equation involving an independent variable, say \\(t\\), a dependant variable, \\(y\\), and its derivatives, \\(\\frac{dy}{dt}, \\frac{d^2y}{dt^2}, \\frac{d^3y}{dt^3}, \\dots\\) The order of a differential equation refers to the highest derivative present. For example, \\[\\frac{dy}{dt} = a\\sin(bt)y(1-y)\\] is a first order nonlinear ordinary differential equation; \\[m\\frac{d^2y}{dt^2} + r\\frac{dy}{dt} + ky = a\\cos(t)\\] is a second order linear differential equation. The term linear refers to the linear dependence of the equation on the dependant variable and its derivatives.\nA differential equation, or a system of differential equations is said to be linear if it is linear in the dependent variable(s) and their derivatives. Thus, the equation governing the motion of a damped oscillator subject to an external force \\(F\\), \\(my'' - cy' + ky = F(t)\\), is a linear second order differential equation, even if the forcing term \\(F\\) is nonlinear in the independent variable \\(t\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "M3503-lec01-ODEintro.html#motivating-examples",
    "href": "M3503-lec01-ODEintro.html#motivating-examples",
    "title": "1  Introduction",
    "section": "",
    "text": "We’ll start with simpler first order differential equations, some of which you’ll have seen in your Calculus courses.\n\nNext we’ll look at higher order differential equations and systems of first order differential equations.\nMost of our focus will be on linear differential equations and initial value problems.\nWe’ll use Laplace transforms to handle impulsive systems, like circuits with switches.\nWe’ll briefly look at Fourier series and boundary value problems.\n\n\nExample 1.1 The motion of a mass on a spring is modelled as a balance of potential energy, \\(\\frac{1}{2}kx^2\\), which is assumed proportional to the mass’s squared displacement from an equilibrium position, and kinetic energy, \\(\\frac{1}{2}m\\dot{x}^2\\) which is assumed proportional to the mass’s squared velocity, or squared rate of change of displacement.\nConservation of energy implies the sum of these two quantities remains constant. You may have seen this expressed in terms of forces: \\(m\\ddot{x} + kx = 0\\).\nHere the displacement, \\(x\\), is our dependent variable. The independent variable, time, is implicit. We will refer to the mass, \\(m\\), and the spring constant \\(k\\) as our parameters. The differential equation is often accompanied by initial conditions. For example releasing the mass from rest with an initial displacement \\(x_0\\) translates to the conditions \\(x(0) = x_0\\), \\(\\dot{x}(0) = 0\\).\n\n\nExample 1.2 Exponential growth is modelled as a solution to the simple ode \\(y' = ry\\). More complex logistic growth is the solution to the equation \\(y' = ry(1-y/K)\\).\n\n\nExample 1.3 Conservation of charge in an electrical circuit can be modelled as constraints between voltage drops, currents and their rates of change.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "M3503-lec01-ODEintro.html#sec-intro-linear",
    "href": "M3503-lec01-ODEintro.html#sec-intro-linear",
    "title": "1  Introduction",
    "section": "1.2 Linear Differential Equations",
    "text": "1.2 Linear Differential Equations\nThe main focus of the first part of these notes is on linear ordinary differential equations. This is for two simple reasons: (1) the mathematical theory for nonlinear equations is built on the simpler, more complete theory for linear equations, and (2) important concepts, such as system stability and responses to perturbations, are based on linear approximations to nonlinear equations. For example, a balance of forces acting on a mass swinging on a string (a pendulum) leads to the nonlinear differential equation \\[l\\theta''(t) + g\\sin \\theta(t) = 0,\\] where \\(\\theta(t)\\) is the angular displacement of the mass at time \\(t\\), \\(l\\) is the string length and \\(g\\) is our gravitational acceleration. You should quickly verify that the constant function \\(\\theta(t) = 0\\) for all \\(t\\) is a solution to the equation. If the initial displacement is small, we expect the angle to stay close to zero. Thus we approximate the nonlinear term, \\(g\\sin(\\theta)\\), by its tangent line, \\(g\\theta\\), resulting in the more familiar linear differential equation \\[l\\theta''(t) + g\\theta(t) = 0.\\] This one we can solve almost by inspection, since we know that sine and cosine functions behave this way. Indeed, if we guess that \\(\\theta(t) = \\sin(\\omega t)\\) we can find the frequency, \\(\\omega\\), with some simple algebra: \\[\\begin{align*}\n    l\\theta''(t) + g\\theta(t) &= 0 \\\\\n    l\\frac{d^2}{d\\theta^2}\\sin(\\omega t) + g\\sin(\\omega t) &= 0 \\\\\n    -l\\omega^2\\sin(\\omega t) + g\\sin(\\omega t) &= 0 \\\\\n    \\left(-l\\omega^2 + g\\right)\\sin(\\omega t)  &= 0\n\\end{align*}\\] Thus either \\(\\sin(\\omega t) =0\\) for all \\(t\\), implying \\(\\omega = 0\\), or \\(\\omega = \\sqrt{g/l}\\). The first case is the trivial zero solution we already guessed, the second case is the natural frequency of the pendulum. We will cover this in more depth later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "M3503-lec01-ODEintro.html#sec-intro-systems",
    "href": "M3503-lec01-ODEintro.html#sec-intro-systems",
    "title": "1  Introduction",
    "section": "1.3 Higher order differential equations and systems of differential equations",
    "text": "1.3 Higher order differential equations and systems of differential equations\nAn \\(n^{\\text{th}}\\) order differential equation can be written \\[{\\cal{F}}\\left(t,x,\\frac{dx}{dt},\\dots,\\frac{d^nx}{dt^n}\\right) = 0,\\] or if we solve for the highest derivative, \\[\\frac{d^nx}{dt^n} = F(t,x,\\frac{dx}{dt},\\dots,\\frac{d^i_{n-1}x}{dt^{n-1}}).\\] Such an equation can also be written as a system of first order equations by introducing new dependent variables for the derivatives: \\[\\begin{align*}\n  x'_0 &= x_1,  \\\\\n  x'_1 &= x_2,  \\\\\n  x'_2 &= x_3,  \\\\\n    x'_{n-1} &=  F(t,x_0,x_1,\\dots,x_{n-1}).\n\\end{align*}\\] Here, \\(x_i\\) is the \\(i^{\\text{th}}\\) derivative of \\(x\\), and \\(x_0\\) is our new name for the original variable. Since higher order equations can be cast as systems of first order equations, we will spend much of the course dealing with such systems. Notationally, we just allow the dependent variable to be a vector: \\(x' = f(t,x)\\), where \\(f:{\\mathbb R}\\times{\\mathbb R}^n\\to {\\mathbb R}^n\\).\nGeometrically, the solution to a system of first order equations, \\(x(t)\\), describes a curve in \\({\\mathbb R}^n\\) parameterized by \\(t\\). The derivative \\(x'(t)\\) is the vector tangent to this curve at \\((t,x(t))\\). Hence, the solution to the differential equation is the curve \\(x(t)\\), which is everywhere tangent to \\(f(t,x(t))\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html",
    "href": "ode-intro-geom.html",
    "title": "2  Scalar first order differential equations",
    "section": "",
    "text": "2.1 Direction fields\nThe [OpenStax Calculus][] text covers most of the material we need for first order scalar1 differential equations. You should have covered all (or at least most) of OpenStax Calculus Volume 1 Chapter 4 in Math 1013. More specifically, you should be familiar with viewing a differential equation as a direction field (or vector field) Section 2.1. You should also master the two simplest solution tricks for first order equations: separable first order equations, and linear first order equations. These require nothing more than a basic mastery of integration.\nA solution to a first order differential equation, \\(y' = f(x,y)\\), is a function, \\(y(x)\\), whose slope at \\(x\\) is \\(f(x,y)\\). One way to visualize this is to view \\(f\\) as a direction field. That is, \\(f\\) assigns a slope, or direction, to every point in the \\(x\\)-\\(y\\) plane.\nShow the code\nusing Plots\nusing LaTeXStrings\n\nplot_font = \"Computer Modern\"\ndefault(fontfamily=plot_font,\n        linewidth=2, framestyle=:box, label=nothing, grid=false)\n\n\nPrecompiling Plots...\n    603.7 ms  ✓ LaTeXStrings\n    547.8 ms  ✓ SimpleBufferStream\n    574.9 ms  ✓ Pipe\n    584.2 ms  ✓ StatsAPI\n    626.4 ms  ✓ Contour\n    756.4 ms  ✓ Measures\n    965.7 ms  ✓ URIs\n    504.4 ms  ✓ PtrArrays\n    552.6 ms  ✓ DataAPI\n    660.0 ms  ✓ NaNMath\n   1317.8 ms  ✓ Grisu\n    842.3 ms  ✓ Statistics\n    847.6 ms  ✓ OrderedCollections\n    576.5 ms  ✓ Reexport\n    822.3 ms  ✓ Requires\n    593.3 ms  ✓ BitFlags\n   1859.2 ms  ✓ Format\n    608.4 ms  ✓ TensorCore\n   1519.8 ms  ✓ IrrationalConstants\n   1010.4 ms  ✓ TranscodingStreams\n    646.4 ms  ✓ StableRNGs\n    819.2 ms  ✓ Unzip\n    727.7 ms  ✓ DelimitedFiles\n    786.0 ms  ✓ ExceptionUnwrapping\n    908.3 ms  ✓ LoggingExtras\n   1273.6 ms  ✓ ConcurrentUtilities\n    992.0 ms  ✓ DocStringExtensions\n   1006.0 ms  ✓ OpenSSL_jll\n   2006.7 ms  ✓ MbedTLS\n   1086.1 ms  ✓ Xorg_libICE_jll\n   1004.2 ms  ✓ Wayland_protocols_jll\n   1156.2 ms  ✓ Libffi_jll\n   1348.3 ms  ✓ Libuuid_jll\n   1299.3 ms  ✓ LLVMOpenMP_jll\n   3030.6 ms  ✓ UnicodeFun\n   1309.8 ms  ✓ Ogg_jll\n   1457.6 ms  ✓ fzf_jll\n   1243.5 ms  ✓ mtdev_jll\n   1391.9 ms  ✓ JpegTurbo_jll\n   4721.3 ms  ✓ MacroTools\n   1519.5 ms  ✓ x264_jll\n   1226.0 ms  ✓ Libiconv_jll\n   1789.7 ms  ✓ x265_jll\n   1241.3 ms  ✓ Graphite2_jll\n   1292.8 ms  ✓ EpollShim_jll\n   1465.9 ms  ✓ FriBidi_jll\n   1410.6 ms  ✓ Xorg_libXau_jll\n   1227.5 ms  ✓ libpng_jll\n   1174.4 ms  ✓ Xorg_libXdmcp_jll\n   1302.0 ms  ✓ LAME_jll\n   1307.3 ms  ✓ libaom_jll\n   1348.2 ms  ✓ Zstd_jll\n   1134.0 ms  ✓ Opus_jll\n   1320.6 ms  ✓ Expat_jll\n   1366.3 ms  ✓ LZO_jll\n   1071.8 ms  ✓ Xorg_xtrans_jll\n   1241.2 ms  ✓ Bzip2_jll\n   1319.4 ms  ✓ Libmount_jll\n   1209.8 ms  ✓ libfdk_aac_jll\n   1259.4 ms  ✓ LERC_jll\n   1242.3 ms  ✓ gperf_jll\n   1362.3 ms  ✓ XZ_jll\n   1301.6 ms  ✓ libevdev_jll\n   1430.2 ms  ✓ Libgpg_error_jll\n   1205.6 ms  ✓ Xorg_libpthread_stubs_jll\n    930.2 ms  ✓ AliasTables\n    946.8 ms  ✓ Missings\n    866.7 ms  ✓ Showoff\n   1023.2 ms  ✓ CodecZlib\n   1754.6 ms  ✓ Statistics → SparseArraysExt\n   1515.9 ms  ✓ LogExpFunctions\n   1272.0 ms  ✓ Xorg_libSM_jll\n   3520.5 ms  ✓ JSON\n   1384.5 ms  ✓ Pixman_jll\n   3690.5 ms  ✓ RecipesBase\n   1782.3 ms  ✓ libvorbis_jll\n   1304.1 ms  ✓ XML2_jll\n   1332.3 ms  ✓ Dbus_jll\n   4716.0 ms  ✓ DataStructures\n   1810.9 ms  ✓ FreeType2_jll\n   2903.2 ms  ✓ JLFzf\n   1561.2 ms  ✓ eudev_jll\n   5806.2 ms  ✓ FixedPointNumbers\n   5404.2 ms  ✓ OpenSSL\n   1893.0 ms  ✓ Libtiff_jll\n   1374.9 ms  ✓ Libgcrypt_jll\n   1166.3 ms  ✓ SortingAlgorithms\n   1380.1 ms  ✓ Gettext_jll\n   1574.3 ms  ✓ Wayland_jll\n   1351.6 ms  ✓ libinput_jll\n   1709.5 ms  ✓ Fontconfig_jll\n   1294.3 ms  ✓ XSLT_jll\n   1552.7 ms  ✓ Glib_jll\n   6419.9 ms  ✓ Latexify\n   3189.4 ms  ✓ ColorTypes\n    943.1 ms  ✓ ColorTypes → StyledStringsExt\n   2219.7 ms  ✓ Xorg_libxcb_jll\n   1549.1 ms  ✓ Latexify → SparseArraysExt\n   1049.1 ms  ✓ Xorg_xcb_util_jll\n   4657.8 ms  ✓ StatsBase\n   1300.6 ms  ✓ Xorg_libX11_jll\n   1291.3 ms  ✓ Xorg_xcb_util_keysyms_jll\n   1293.6 ms  ✓ Xorg_xcb_util_renderutil_jll\n   1373.3 ms  ✓ Xorg_xcb_util_image_jll\n   1411.2 ms  ✓ Xorg_xcb_util_wm_jll\n   1413.2 ms  ✓ Xorg_libXrender_jll\n   4485.8 ms  ✓ ColorVectorSpace\n   1387.4 ms  ✓ Xorg_libXext_jll\n   1484.5 ms  ✓ Xorg_libXfixes_jll\n   1470.9 ms  ✓ Xorg_libxkbfile_jll\n   1416.5 ms  ✓ Xorg_xcb_util_cursor_jll\n   1379.8 ms  ✓ Xorg_libXinerama_jll\n   1464.3 ms  ✓ Xorg_libXrandr_jll\n   1280.3 ms  ✓ Xorg_libXi_jll\n   1361.2 ms  ✓ Xorg_libXcursor_jll\n   1580.4 ms  ✓ Libglvnd_jll\n   1705.4 ms  ✓ Cairo_jll\n   1140.0 ms  ✓ Xorg_xkbcomp_jll\n    902.2 ms  ✓ HarfBuzz_jll\n   1109.4 ms  ✓ Xorg_xkeyboard_config_jll\n   1483.8 ms  ✓ libass_jll\n   1597.3 ms  ✓ Pango_jll\n   9637.1 ms  ✓ Colors\n   1314.4 ms  ✓ xkbcommon_jll\n   1672.9 ms  ✓ FFMPEG_jll\n   1168.8 ms  ✓ Vulkan_Loader_jll\n   1244.0 ms  ✓ libdecor_jll\n   1046.2 ms  ✓ FFMPEG\n   1288.1 ms  ✓ GLFW_jll\n   1427.4 ms  ✓ Qt6Base_jll\n   1325.8 ms  ✓ Qt6ShaderTools_jll\n   1542.7 ms  ✓ GR_jll\n   4726.1 ms  ✓ ColorSchemes\n   2782.6 ms  ✓ Qt6Declarative_jll\n   1444.1 ms  ✓ Qt6Wayland_jll\n  38562.6 ms  ✓ Unitful\n   3047.2 ms  ✓ UnitfulLatexify\n  29475.0 ms  ✓ HTTP\n  15530.8 ms  ✓ PlotUtils\n   4579.7 ms  ✓ GR\n   3078.2 ms  ✓ PlotThemes\n   4315.2 ms  ✓ RecipesPipeline\n  56373.0 ms  ✓ Plots\n   2987.5 ms  ✓ Plots → UnitfulExt\n  144 dependencies successfully precompiled in 113 seconds. 43 already precompiled.\nShow the code\nf(t,x) = 2*t-1-3*x\n\nt = collect(1:.3:4)\nx = collect(0:.3:2)\ndirfield(t,x) = [1; f(t,x)]/10\nfig = quiver(\n        repeat(t,length(x)),\n        repeat(x,inner=length(t)),\n        quiver=dirfield\n      )\nplot!(fig,ylabel = \"x\")\nplot!(fig,xlabel = \"t\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Direction field for \\(f:(t,x)\\to 2t -1 - 3x\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html#terminology-and-notation",
    "href": "ode-intro-geom.html#terminology-and-notation",
    "title": "2  Scalar first order differential equations",
    "section": "2.2 Terminology and Notation",
    "text": "2.2 Terminology and Notation\nWe will focus on equations of the form \\[\\dot{x} = f(x,t,\\mu), \\quad x\\in U\\subset{\\mathbb R}^n, \\quad t\\in{\\mathbb R}, \\quad \\mu\\in V\\subset{\\mathbb R}^p. \\tag{2.1}\\] Here, the over-dot refers to differentiation with respect to the independent variable, \\(t\\), \\(x\\) is our vector of dependent variables, and \\(\\mu\\) is a vector of model parameters.\nBy a solution to Equation 2.1 we mean a differentiable function \\(\\phi\\) from some interval of time, \\(I \\subset {\\mathbb R}\\), into \\({\\mathbb R}^n\\) such that \\(\\dot{\\phi}(t) = f(\\phi(t),t,\\mu)\\). Geometrically, \\(\\phi\\) is a curve in \\({\\mathbb R}^n\\) parameterized by \\(t\\) and tangent to the vector \\(f\\) at each point. We will refer to \\(U\\), or \\({\\mathbb R}^n\\), as the state space (or sometimes phase space) of the system, and \\(f:U\\to{\\mathbb R}^n\\) as a vector field on \\(U\\) (or sometimes direction field). Note, when convenient, we will use \\(x\\) to denote the solution instead of introducing \\(\\phi\\) or another new symbol. Most often it is clear from context whether \\(x\\) is referring to a point in state space, or a curve through state space parameterized by \\(t\\).\nWe use \\(\\phi(t,t_0,x_0)\\) to denote the solution passing through the point \\(x_0\\) at time \\(t_0\\), If we are also interested in the dependence on the parameters, we will use \\(\\phi(t,t_0,x_0,\\mu)\\), or if we are only interested in the dependence of the solution on the parameters, \\(\\phi(t,\\mu)\\). If the initial time is clear from the problem, usually when \\(t_0=0\\), then we will use \\(\\phi(t,x_0)\\) or \\(\\phi(t,x_0,\\mu)\\).\nThe solution curve through state space will sometimes be referred to as a trajectory through \\(x_0\\) at time \\(t_0\\), usually with the notation \\(\\phi(t,t_0,x_0)\\). The set of points comprising the trajectory will be referred to as the orbit, \\(O(x_0)\\), through \\(x_0\\) at time \\(t_0\\).\nThis terminology relies assumes the existence and uniqueness of solutions. Suppose \\(f\\) is \\(r\\)-times differentiable in \\(x\\), \\(t\\) and \\(\\mu\\) with \\(r\\ge 1\\) and each derivative continuous, then given any \\(t_0\\in{\\mathbb R}\\) and \\(x_0\\in U\\) there is a unique solution through \\(x_0\\) at time \\(t_0\\), and this solution is \\(r\\)-times differentiable in \\(t\\), \\(x_0\\), \\(t_0\\) and \\(\\mu\\).\n\nExample 2.1 The simple pendulum \\[\\ddot{x}  - x = 0\\] can be cast as a system of first order equations with the introduction of a new dependent variable \\(y = \\dot{x}\\)\n\\[\\begin{aligned}\n  \\dot{x} &= y \\\\\n  \\dot{y} &= -x\n\\end{aligned} \\tag{2.2}\\]\nThe state of the pendulum is completely described by its angle and angular velocity, \\((x,y) \\in {\\mathbb R}^2\\). The solution passing through the initial state \\((x_0,0)\\) at time \\(t_0=0\\) is \\((x(t),y(t)) = (x_0\\cos t, -x_0\\sin t)\\). Note that the initial conditions are a pair of real numbers, \\((x_0,y_0)\\), and for this solution we have \\(y_0=0\\). The orbit, \\(O((x_0,0))\\), is the circle \\(x^2 + y^2 = x_0^2\\).\n\n\nExample 2.2 The damped pendulum \\[\\ddot{x} -\\alpha x\\dot{x} + \\sin(x) = \\sin(\\omega t)\\] can be cast as a system of first order equations with the introduction of a new dependent variable \\(y = \\dot{x}\\) \\[\\begin{aligned}\n  \\dot{x} &= y \\\\\n  \\dot{y} &= \\alpha xy - \\sin(x) + \\sin(\\omega t)\n\\end{aligned} \\tag{2.3}\\]\n\n\nExample 2.3 The simple SIR model\n\\[\\begin{aligned}\n  S' &= \\Lambda -\\mu S - \\beta SI, \\\\\n  I' &= \\beta SI -(\\alpha + \\mu)I, \\\\\n  R' &= \\alpha I - \\mu R,\n\\end{aligned} \\tag{2.4}\\]\nHere the state variables are populations of susceptible, \\(S\\), infected, \\(I\\), and recovered, \\(R\\) individuals. Our state spaces is the nonnegative cone (octant) of \\({\\mathbb R}^3\\), our parameter space is the positive cone of \\({\\mathbb R}^4\\), since all parameters are assumed to be positive.\nSince the dynamics of \\(S\\) and \\(I\\) decouple from those of \\(R\\), we can consider the simpler two state system separately.\n\n\nExample 2.4 The classic logistic growth model should be introduced as \\[\\frac{dN}{dt} = bN-(d+aN)N\\] where the population size, or density, \\(N\\) is our dependent variable, time, \\(t\\), is our independent variable, and \\(b\\), \\(d\\), and \\(a\\) are parameters representing birth, death, and increased mortality due to crowding, respectively. Assume the three parameters are positive real numbers.\nThis is more commonly written as \\(\\frac{dN}{dt} = rN(1-N/K)\\), with \\(r = b-d\\), \\(K = (b-d)/a\\), and the added assumption that \\(b&gt;d\\).\n\n\nExercise 2.1 First, suppose \\(b\\ne d\\) and show that we can, without loss of generality, rescale \\(t\\) by the net growth rate \\(r\\) giving rise to the equation \\[\\frac{dN}{dt} = N-\\frac{N^2}{K}\\]\nSecond, let \\(x(t) = \\bar{N}^{-1} N(t)\\), and show that \\(x\\) must satisfy the equation \\[\\frac{dx}{dt} = x-\\dfrac{\\bar{N}}{K}x^2\\]\n\nHence, setting \\(\\bar{N} = K\\) results in an ODE for \\(x\\) with no free parameters. Note, if \\(x(t)\\), with the rescaled time is a solution to this last equation, then \\(N(t^*) = \\bar{N}x(rt^*)\\) is a solution to the original equation with the original time scale.\n\nExercise 2.2 Repeat Exercise 2.1 under the assumption \\(b &lt; d\\). Be sure to keep the scales positive so that rescaled time and population have the same signs as the original variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html#sec-separable",
    "href": "ode-intro-geom.html#sec-separable",
    "title": "2  Scalar first order differential equations",
    "section": "2.3 First order separable equations",
    "text": "2.3 First order separable equations\nA first order linear differential equation of the form \\(\\frac{dy}{dt} = f(y)g(t)\\) is said to be separable. It can be solved by a single integration step. \\[\\begin{align*}\n  \\frac{dy}{dt} &= f(y)g(t) \\\\\n  \\frac{1}{f(y)}\\frac{dy}{dt} &= g(t) \\\\\n  \\int \\frac{1}{f(y)}\\frac{dy}{dt}\\,dt &= \\int g(t)\\,dt + C \\\\\n  \\int \\frac{1}{f(y)}\\,dy &= \\int g(t)\\,dt + C\n\\end{align*}\\]\n\nExample 2.5 Find the general solution to the differential equation \\(\\frac{dy}{dt} = ry\\cos(t)\\).\nSeparating variables and integrating leads to \\[\\begin{align}\n    \\int \\frac{1}{y}\\,dy &= \\int r\\cos(t)\\,dt +C \\\\\n    \\log |y| &= r\\sin t  + C \\\\\n    y(t) &= \\pm\\exp(C+r\\sin t )\n\\end{align}\\] The solution is usually given in the form \\(y(t) = y_0e^{r\\sin(t)}\\). Since \\(C\\) is an arbitrary constant, \\(y_0 = \\pm e^C\\) is also arbitrary.\n\n\n\n\n\n\n\nNote\n\n\n\nSince we divide by \\(y\\) on our first step in this example, we must assume \\(y\\ne0\\). However, it is easy to see by inspection that \\(y=0\\) is also a solution to the equation.\nHence, we can express the general solution as \\(y(t) = y_0\\exp(r\\sin t)\\) where \\(y_0\\) can be any real number.\n\n\n\nExample 2.6 Find the general solution to the differential equation \\(\\frac{dy}{dx} = \\frac{6x^2}{2y+\\cos y}\\).\nSeparating variables and integrating leads to \\[\\begin{align}\n  \\int (2y+\\cos y)\\,dy&= \\int 6x^2 \\, dx + C \\\\\n  y^2+\\sin y &= 2x^3  + C\n\\end{align}\\] In this case, we are not able to solve for \\(y\\) as a function of \\(x\\). We are left with \\(y\\) defined implicitly as a function of \\(x\\).\n\nDespite not being able to find an analytic expression for \\(y\\) as a function of \\(x\\) in this example, we can still make nice plots of the solutions. The trick is to notice that we can find the inverse of the solution. Several solutions are shown in Figure 2.2\n\n\nShow the code\ninversesoln(y,c) = cbrt.(y.*y +sin.(y)/2 .-c/2)\ny = range(-pi,pi,100)\nplot(legends=false)\nfor c in -1.2:.2:1.2\n  plot!(inversesoln(y,c),y)\nend\nplot!(xlabel='x')\nplot!(ylabel='y')\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sample solutions for Example 2.6\n\n\n\n\n\nExample 2.7 Solve the differential equation \\(\\frac{dy}{dx} = \\frac{x^2}{y^2}\\).\nSeparating variables and integrating leads to \\(\\frac{y^3}{3} = \\frac{x^3}{3} + C\\). Solving for the dependent variable leads to \\(y = \\sqrt[3]{x^3+3C}\\).\n\n\nExample 2.8 Solve the differential equation \\(\\frac{dy}{dx} = -\\frac{2x}{y}\\).\nSeparating variables and integrating leads to \\(\\frac{y^2}{2} = -x^2 + C\\), which is a family of ellipses.\n\n\nExample 2.9 Solve the differential equation \\(\\frac{du}{dt} = 2 + 2u + t + tu\\).\nHere \\(u\\) is our dependent variable. The equation is both linear and separable so we may choose either method. \\[\\begin{align*}\n   \\frac{du}{dt} &= (2+t)(1+u) \\\\\n   \\int \\frac{du}{1+u} &= \\int (2+t) \\, dt  \\\\\n   \\ln|1+u| &= 2t+\\frac{t^2}{2} + C \\\\\n   |1+u| &= A \\exp(2t+\\frac{t^2}{2}) \\\\\n   u &= -1 \\pm A \\exp(2t+\\frac{t^2}{2})\n\\end{align*}\\] Note the use of \\(\\pm\\) is not necessary. Since \\(A\\) could be positive or negative. It just serves to remind us that integration of \\(1/(1+u)\\) leads to two solutions.\n\n\nExample 2.10 Solve the differential equation \\(xy' + y = y^2\\). This is nonlinear, due to the \\(y^2\\) term. It is separable. \\[\\begin{align*}\n    \\frac{1}{y^2-y} y' &= \\frac{1}{x} \\\\\n    \\int \\frac{dy}{y^2-y} &= \\int \\frac{dx}{x} \\\\\n    \\int \\frac{1}{y-1} - \\frac{1}{y} dy &= \\int \\frac{dx}{x} \\\\\n     \\ln|y-1| - \\ln|y| &= \\ln|x| + C \\\\\n     \\ln\\frac{|y-1|}{|x||y|} &=  C \\\\\n     \\frac{|y-1|}{|x||y|} &=  A\n  \\end{align*}\\] there are in fact several solutions buried in this notation. For example, if \\(0&lt; y &lt; 1\\), then \\(\\frac{|y-1|}{|x||y|} = \\frac{1-y}{xy}\\), and we find \\(1/y - 1 = A/x \\rightarrow y = x/(A+x)\\), \\(A&gt;0\\).\n\nAs a further exercise, sketch a few sample solutions with \\(y&lt; 0\\), \\(0 &lt; y &lt; 1\\) and \\(y&gt;1\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html#sec-1storder-linear",
    "href": "ode-intro-geom.html#sec-1storder-linear",
    "title": "2  Scalar first order differential equations",
    "section": "2.4 Linear first order differential equations",
    "text": "2.4 Linear first order differential equations\nConsider a differential equation of the form \\(y'(t) +p(t) y(t) = q(t)\\) with initial data \\(y(t_0) = y_0\\). These can be integrated using an integrating factor. The trick is to find a factor, \\(u\\), which turns the left hand side of the equation into the derivative of something familiar. We multiply both sides of the equation by \\(u\\), \\[u(t)y'(t) +u(t)p(t) y(t) = u(t)q(t).\\] Notice that if we can choose \\(u\\) so that \\(up = u'\\), then the left hand side is the derivative of \\(uy\\): \\[\\left(uy\\right)' = uy' +u' y = uy' +up y =  uq.\\] We can solve this with a single integration \\[\\begin{gather}\n    \\frac{d}{dt}\\left(u(t)y(t)\\right) =  u(t)q(t) \\\\\n    u(t)y(t) =  \\int u(s)q(s) \\, ds \\\\\n    y(t) =  \\frac{1}{u(t)} \\int u(s)q(s)\\, ds\n\\end{gather}\\] There is an arbitrary constant resulting from in the integration of \\(uq\\). We’ll use use our initial conditions to determine that once we determined \\(u\\).\nReturning to \\(u'=pu\\), the differential equation for \\(u\\), we see it is separable and has solutions of the form \\(u(t) = \\exp(\\int p)\\). To make the result more readable, define \\(P(t) = \\int_{t_0}^t p(s) \\, ds\\) and take \\(u\\) as \\(u(t) = \\exp(P(t))\\). This is the solution that also satisfies \\(u(t_0)=1\\), which is convenient. Putting this together with our expression for \\(y\\), we find, in terms of \\(P\\), \\[y(t) = e^{-P(t)} \\int e^{P(s)}q(s)\\, ds\\] This is our general solution, since it still has the constant of integration hiding in the integral. Applying the initial conditions we find \\[y(t) = e^{-P(t)} \\left( y_0 + \\int_{t_0}^t e^{P(s)}q(s)\\, ds \\right)\\]\n\n\n\n\n\n\nWarning\n\n\n\nI advise against memorizing this formula. Instead memorize the technique. Applying the technique is often simpler than applying the formula, and we’ll extend the ideas behind the method later in the course.\n\n\n\nExample 2.11 Solve the initial value problem \\(t^2y' + 2ty = \\log t, \\ y(1) = 2\\).\nYou can probably guess the integrating factor by noticing the left hand side looks like the product rule applied to \\(t^2y\\). Specifically, \\[\\begin{align*}\n        t^2y' + 2ty &= \\log t \\\\\n        \\dfrac{d}{dt}\\left(t^2y\\right)  &= \\log t \\\\\n        \\left. s^2y(s) \\right|_{s=1}^{s=t}  &= \\int_1^t \\log s \\, ds \\\\\n        t^2y(t) - y(1)  &= t\\log(t) - t + 1 \\\\\n        y(t) &= \\frac{1}{t} \\log(t) - \\frac{1}{t} + \\frac{3}{t^2}    \n\\end{align*}\\]\n\n\n\n\n\n\n\nTip\n\n\n\nIf you don’t immediately recognize the integrating factor, first rewrite the problem as \\(y' + \\dfrac{2}{t} y = \\frac{1}{t^2}\\log t\\) The integrating factor can be computed as \\(u(t) = \\exp\\left(\\displaystyle\\int \\frac{2}{t} \\, dt\\right) = t^2\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html#poincaré-maps",
    "href": "ode-intro-geom.html#poincaré-maps",
    "title": "2  Scalar first order differential equations",
    "section": "2.5 Poincaré Maps",
    "text": "2.5 Poincaré Maps\n\nExample 2.12 Consider the logistic model with \\(r&gt;0\\) and periodic harvesting. \\[\\frac{dx}{dt} = rx(1-x) - h(1+\\sin(2\\pi t))\\] Here, to keep consistent with the example of Hirsch, Smale, and Devaney (Hirsch, Smale, and Devaney 2013, sec. 1.4) we’ve rescaled the population by \\(K\\) and rescaled time so that the period of harvesting is one.\n\nThis equation is neither linear, nor separable, so we have no tools at our disposal to find a nice form of the solution.\n\n\nShow the code\nusing DifferentialEquations\n\nusing Plots\nusing LaTeXStrings\n\nplot_font = \"Computer Modern\"\ndefault(fontfamily=plot_font,\n        linewidth=2, framestyle=:box, label=nothing, grid=false)\n\nf(x,p,t) = p.r*x*(1-x) - p.h*(1+sin(2*pi*t))\n\np = (r = 2., h = .2)\n\nt = collect(0:.2:2)\nx = collect(0:.2:1)\ndirfield(t,x) = [1; f(x,p,t)]/10\nfig = quiver(repeat(t,length(x)),repeat(x,inner=length(t)),quiver=dirfield)\nplot!(fig,ylabel = L\"\\textrm{Population, } \\textit{\\ x}\")\nplot!(fig,xlabel = L\"\\textrm{time, } \\textit{\\ t}\")\n\nx0 = 0.5\ntspan = (0.,2.)\nprob = ODEProblem(f,x0,tspan,p)\nfor x0 in .25:.25:1 \n  prob = remake(prob;u0 = x0)\n  sol = solve(prob)\n  plot!(fig,sol)\nend\n\nfig\n\n\nPrecompiling DifferentialEquations...\n    506.1 ms  ✓ DataValueInterfaces\n    532.8 ms  ✓ SimpleUnPack\n    571.5 ms  ✓ ConcreteStructs\n    581.0 ms  ✓ IteratorInterfaceExtensions\n    688.8 ms  ✓ ExprTools\n    799.2 ms  ✓ InverseFunctions\n    988.5 ms  ✓ ADTypes\n    495.4 ms  ✓ UnPack\n    492.9 ms  ✓ CommonSolve\n   1158.0 ms  ✓ FunctionWrappers\n    703.9 ms  ✓ PoissonRandom\n    650.3 ms  ✓ ManualMemory\n    657.9 ms  ✓ EnumX\n    576.6 ms  ✓ IfElse\n    727.0 ms  ✓ Inflate\n   1915.2 ms  ✓ OffsetArrays\n    572.2 ms  ✓ CommonWorldInvalidations\n    757.7 ms  ✓ ConstructionBase\n    586.6 ms  ✓ FastClosures\n    614.2 ms  ✓ StaticArraysCore\n    634.5 ms  ✓ MuladdMacro\n   1747.7 ms  ✓ RandomNumbers\n    791.0 ms  ✓ PositiveFactorizations\n    731.4 ms  ✓ CEnum\n    518.7 ms  ✓ SIMDTypes\n   2354.5 ms  ✓ FastLapackInterface\n    651.7 ms  ✓ CompositionsBase\n    680.9 ms  ✓ FastPower\n    805.3 ms  ✓ Adapt\n   1675.0 ms  ✓ FillArrays\n   1022.5 ms  ✓ EnzymeCore\n    907.7 ms  ✓ CommonSubexpressions\n   1377.1 ms  ✓ Distances\n   1115.7 ms  ✓ CpuId\n   2068.2 ms  ✓ GenericSchur\n    976.8 ms  ✓ SuiteSparse\n    876.4 ms  ✓ TruncatedStacktraces\n   1359.4 ms  ✓ SimpleTraits\n   1664.7 ms  ✓ KLU\n   1178.3 ms  ✓ OpenSpecFun_jll\n   1185.5 ms  ✓ Rmath_jll\n    760.7 ms  ✓ TableTraits\n   1360.2 ms  ✓ oneTBB_jll\n   2429.5 ms  ✓ Sundials_jll\n   2896.8 ms  ✓ ChainRulesCore\n   1038.9 ms  ✓ RuntimeGeneratedFunctions\n   2173.1 ms  ✓ QuadGK\n    905.9 ms  ✓ InverseFunctions → InverseFunctionsDatesExt\n    925.6 ms  ✓ LogExpFunctions → LogExpFunctionsInverseFunctionsExt\n   1278.3 ms  ✓ InverseFunctions → InverseFunctionsTestExt\n   2756.3 ms  ✓ IntelOpenMP_jll\n   1016.1 ms  ✓ Parameters\n    793.2 ms  ✓ FunctionWrappersWrappers\n   2391.8 ms  ✓ DifferentiationInterface\n    805.7 ms  ✓ ConstructionBase → ConstructionBaseLinearAlgebraExt\n   2527.1 ms  ✓ SparseMatrixColorings\n   2002.0 ms  ✓ ThreadingUtilities\n    873.5 ms  ✓ ADTypes → ADTypesConstructionBaseExt\n    978.8 ms  ✓ DiffResults\n   2241.7 ms  ✓ Static\n   1284.2 ms  ✓ CompositionsBase → CompositionsBaseInverseFunctionsExt\n   3439.3 ms  ✓ Sparspak\n   1548.1 ms  ✓ GPUArraysCore\n   1609.7 ms  ✓ ArrayInterface\n   2399.6 ms  ✓ Random123\n   6253.1 ms  ✓ TimerOutputs\n   1039.4 ms  ✓ OffsetArrays → OffsetArraysAdaptExt\n   1125.2 ms  ✓ FillArrays → FillArraysStatisticsExt\n    842.0 ms  ✓ EnzymeCore → AdaptExt\n    961.9 ms  ✓ ADTypes → ADTypesEnzymeCoreExt\n   1604.8 ms  ✓ FillArrays → FillArraysSparseArraysExt\n   1616.6 ms  ✓ Distances → DistancesSparseArraysExt\n   2331.8 ms  ✓ PDMats\n   1329.7 ms  ✓ ChainRulesCore → ChainRulesCoreSparseArraysExt\n   2028.0 ms  ✓ Rmath\n   1882.6 ms  ✓ Tables\n    980.1 ms  ✓ ADTypes → ADTypesChainRulesCoreExt\n   1066.1 ms  ✓ Distances → DistancesChainRulesCoreExt\n  14117.1 ms  ✓ Krylov\n    923.2 ms  ✓ DifferentiationInterface → DifferentiationInterfaceChainRulesCoreExt\n   2959.1 ms  ✓ LogExpFunctions → LogExpFunctionsChainRulesCoreExt\n   1456.0 ms  ✓ DifferentiationInterface → DifferentiationInterfaceSparseArraysExt\n   2610.5 ms  ✓ MKL_jll\n   1549.4 ms  ✓ Functors\n   1057.5 ms  ✓ BitTwiddlingConvenienceFunctions\n   6484.4 ms  ✓ SpecialFunctions\n   1918.4 ms  ✓ DifferentiationInterface → DifferentiationInterfaceSparseMatrixColoringsExt\n   2872.8 ms  ✓ Setfield\n   1094.4 ms  ✓ ArrayInterface → ArrayInterfaceChainRulesCoreExt\n    936.7 ms  ✓ SciMLStructures\n   1441.0 ms  ✓ MaybeInplace\n   2960.3 ms  ✓ CPUSummary\n    836.3 ms  ✓ ArrayInterface → ArrayInterfaceGPUArraysCoreExt\n   1040.4 ms  ✓ ArrayInterface → ArrayInterfaceStaticArraysCoreExt\n   1522.4 ms  ✓ ArrayInterface → ArrayInterfaceSparseArraysExt\n   4089.1 ms  ✓ StaticArrayInterface\n   4605.9 ms  ✓ Accessors\n   1736.3 ms  ✓ FillArrays → FillArraysPDMatsExt\n   2124.8 ms  ✓ HostCPUFeatures\n   1633.4 ms  ✓ LevyArea\n   1420.4 ms  ✓ DiffRules\n   2546.6 ms  ✓ HypergeometricFunctions\n   1475.7 ms  ✓ MaybeInplace → MaybeInplaceSparseArraysExt\n   1342.5 ms  ✓ PolyesterWeave\n   1079.3 ms  ✓ StaticArrayInterface → StaticArrayInterfaceOffsetArraysExt\n   4440.8 ms  ✓ SpecialFunctions → SpecialFunctionsChainRulesCoreExt\n   2043.8 ms  ✓ FiniteDiff\n   1069.8 ms  ✓ CloseOpenIntervals\n   1427.4 ms  ✓ Accessors → TestExt\n   1335.2 ms  ✓ Accessors → LinearAlgebraExt\n  18767.4 ms  ✓ StaticArrays\n   1723.0 ms  ✓ LayoutPointers\n   1506.2 ms  ✓ Accessors → DatesExt\n   1447.0 ms  ✓ DifferentiationInterface → DifferentiationInterfaceFiniteDiffExt\n   1989.0 ms  ✓ FiniteDiff → FiniteDiffSparseArraysExt\n   1563.8 ms  ✓ StaticArrays → StaticArraysStatisticsExt\n  11248.4 ms  ✓ SparseConnectivityTracer\n   4234.4 ms  ✓ SciMLOperators\n   5155.4 ms  ✓ StatsFuns\n   3836.5 ms  ✓ SymbolicIndexingInterface\n   2418.6 ms  ✓ StaticArrays → StaticArraysChainRulesCoreExt\n   2007.0 ms  ✓ ResettableStacks\n   1927.7 ms  ✓ ConstructionBase → ConstructionBaseStaticArraysExt\n   2199.1 ms  ✓ Adapt → AdaptStaticArraysExt\n   1727.4 ms  ✓ DifferentiationInterface → DifferentiationInterfaceStaticArraysExt\n   3935.3 ms  ✓ ArnoldiMethod\n   2396.1 ms  ✓ StaticArrayInterface → StaticArrayInterfaceStaticArraysExt\n   1926.0 ms  ✓ FiniteDiff → FiniteDiffStaticArraysExt\n   2338.0 ms  ✓ Accessors → StaticArraysExt\n   4877.6 ms  ✓ FastGaussQuadrature\n  12011.8 ms  ✓ ForwardDiff\n   2860.6 ms  ✓ StrideArraysCore\n   2848.9 ms  ✓ SparseConnectivityTracer → SparseConnectivityTracerNaNMathExt\n   2855.9 ms  ✓ SparseConnectivityTracer → SparseConnectivityTracerLogExpFunctionsExt\n   1142.5 ms  ✓ SciMLOperators → SciMLOperatorsStaticArraysCoreExt\n  40639.3 ms  ✓ MLStyle\n   1294.1 ms  ✓ StatsFuns → StatsFunsInverseFunctionsExt\n   1679.4 ms  ✓ SciMLOperators → SciMLOperatorsSparseArraysExt\n   3272.4 ms  ✓ SparseConnectivityTracer → SparseConnectivityTracerSpecialFunctionsExt\n   1298.8 ms  ✓ FastPower → FastPowerForwardDiffExt\n   1995.7 ms  ✓ ForwardDiff → ForwardDiffStaticArraysExt\n   3362.4 ms  ✓ StatsFuns → StatsFunsChainRulesCoreExt\n   1465.3 ms  ✓ PreallocationTools\n   1766.6 ms  ✓ DifferentiationInterface → DifferentiationInterfaceForwardDiffExt\n   4950.4 ms  ✓ RecursiveArrayTools\n   2234.9 ms  ✓ NLSolversBase\n   1375.8 ms  ✓ Polyester\n  35896.5 ms  ✓ ArrayLayouts\n   1481.7 ms  ✓ RecursiveArrayTools → RecursiveArrayToolsForwardDiffExt\n   1870.5 ms  ✓ RecursiveArrayTools → RecursiveArrayToolsSparseArraysExt\n   1437.8 ms  ✓ FastBroadcast\n   1778.4 ms  ✓ ArrayLayouts → ArrayLayoutsSparseArraysExt\n   4286.8 ms  ✓ LineSearches\n   1515.7 ms  ✓ RecursiveArrayTools → RecursiveArrayToolsFastBroadcastExt\n   2889.9 ms  ✓ MatrixFactorizations\n  11631.7 ms  ✓ Graphs\n   3066.3 ms  ✓ NLsolve\n  14208.3 ms  ✓ Distributions\n   2318.0 ms  ✓ VertexSafeGraphs\n   5975.9 ms  ✓ LazyArrays\n   3165.0 ms  ✓ Distributions → DistributionsTestExt\n   3169.5 ms  ✓ Distributions → DistributionsChainRulesCoreExt\n  23129.2 ms  ✓ VectorizationBase\n  13762.4 ms  ✓ Expronicon\n   8218.1 ms  ✓ Optim\n   3093.4 ms  ✓ LazyArrays → LazyArraysStaticArraysExt\n   1723.9 ms  ✓ SLEEFPirates\n   5143.4 ms  ✓ SparseDiffTools\n   1758.8 ms  ✓ SparseDiffTools → SparseDiffToolsPolyesterExt\n  15772.4 ms  ✓ SciMLBase\n   2400.8 ms  ✓ SciMLBase → SciMLBaseChainRulesCoreExt\n  31945.1 ms  ✓ BandedMatrices\n   6490.4 ms  ✓ SciMLJacobianOperators\n   2508.6 ms  ✓ FiniteDiff → FiniteDiffBandedMatricesExt\n   2518.4 ms  ✓ ArrayInterface → ArrayInterfaceBandedMatricesExt\n   2789.7 ms  ✓ BandedMatrices → BandedMatricesSparseArraysExt\n   2931.4 ms  ✓ MatrixFactorizations → MatrixFactorizationsBandedMatricesExt\n   3876.8 ms  ✓ LazyArrays → LazyArraysBandedMatricesExt\n  11570.8 ms  ✓ DiffEqBase\n   3188.0 ms  ✓ DiffEqBase → DiffEqBaseChainRulesCoreExt\n   3274.7 ms  ✓ DiffEqBase → DiffEqBaseSparseArraysExt\n   8798.8 ms  ✓ LineSearch\n   3826.2 ms  ✓ DiffEqBase → DiffEqBaseDistributionsExt\n   9604.9 ms  ✓ DiffEqCallbacks\n  16234.3 ms  ✓ NonlinearSolveBase\n   9610.4 ms  ✓ JumpProcesses\n   4843.4 ms  ✓ LineSearch → LineSearchLineSearchesExt\n  19291.3 ms  ✓ FastAlmostBandedMatrices\n  15518.2 ms  ✓ OrdinaryDiffEqCore\n   4442.6 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseSparseMatrixColoringsExt\n   4242.1 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseForwardDiffExt\n   9025.4 ms  ✓ SteadyStateDiffEq\n  11417.3 ms  ✓ DiffEqNoiseProcess\n   4098.4 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseBandedMatricesExt\n   4442.5 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseSparseArraysExt\n   3616.0 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseLineSearchExt\n   3090.5 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseDiffEqBaseExt\n   2901.5 ms  ✓ OrdinaryDiffEqCore → OrdinaryDiffEqCoreEnzymeCoreExt\n  53563.0 ms  ✓ LoopVectorization\n   4783.9 ms  ✓ OrdinaryDiffEqFunctionMap\n   5820.5 ms  ✓ OrdinaryDiffEqPRK\n   6195.8 ms  ✓ OrdinaryDiffEqExplicitRK\n   7221.7 ms  ✓ OrdinaryDiffEqStabilizedRK\n   8674.0 ms  ✓ OrdinaryDiffEqSSPRK\n   5600.1 ms  ✓ OrdinaryDiffEqQPRK\n   7177.4 ms  ✓ OrdinaryDiffEqRKN\n   6764.5 ms  ✓ OrdinaryDiffEqSymplecticRK\n   8228.6 ms  ✓ OrdinaryDiffEqHighOrderRK\n   2468.6 ms  ✓ LoopVectorization → SpecialFunctionsExt\n  42089.4 ms  ✓ Sundials\n   3298.2 ms  ✓ LoopVectorization → ForwardDiffExt\n   8842.3 ms  ✓ OrdinaryDiffEqFeagin\n  20935.2 ms  ✓ OrdinaryDiffEqTsit5\n   3081.7 ms  ✓ TriangularSolve\n  14138.5 ms  ✓ OrdinaryDiffEqLowOrderRK\n  11718.8 ms  ✓ OrdinaryDiffEqLowStorageRK\n   5323.0 ms  ✓ OrdinaryDiffEqNordsieck\n   5325.8 ms  ✓ OrdinaryDiffEqAdamsBashforthMoulton\n 117794.4 ms  ✓ ExponentialUtilities\n   1937.4 ms  ✓ ExponentialUtilities → ExponentialUtilitiesStaticArraysExt\n  13720.9 ms  ✓ RecursiveFactorization\n  77287.2 ms  ✓ SimpleNonlinearSolve\n   3731.8 ms  ✓ SimpleNonlinearSolve → SimpleNonlinearSolveChainRulesCoreExt\n  60798.4 ms  ✓ OrdinaryDiffEqVerner\n   3423.9 ms  ✓ OrdinaryDiffEqLinear\n  40873.7 ms  ✓ LinearSolve\n   3593.9 ms  ✓ LinearSolve → LinearSolveRecursiveArrayToolsExt\n   3665.4 ms  ✓ LinearSolve → LinearSolveEnzymeExt\n   4669.0 ms  ✓ LinearSolve → LinearSolveFastAlmostBandedMatricesExt\n   5448.9 ms  ✓ NonlinearSolveBase → NonlinearSolveBaseLinearSolveExt\n   6088.9 ms  ✓ LinearSolve → LinearSolveBandedMatricesExt\n   5155.5 ms  ✓ OrdinaryDiffEqDifferentiation\n   8571.4 ms  ✓ OrdinaryDiffEqExtrapolation\n  30926.8 ms  ✓ NonlinearSolveFirstOrder\n  29316.3 ms  ✓ OrdinaryDiffEqRosenbrock\n   6877.0 ms  ✓ BoundaryValueDiffEqCore\n  60267.8 ms  ✓ NonlinearSolve\n   6174.7 ms  ✓ NonlinearSolve → NonlinearSolveNLsolveExt\n   9328.2 ms  ✓ NonlinearSolve → NonlinearSolveBandedMatricesExt\n  10798.9 ms  ✓ OrdinaryDiffEqNonlinearSolve\n  12399.7 ms  ✓ OrdinaryDiffEqPDIRK\n  12794.0 ms  ✓ OrdinaryDiffEqStabilizedIRK\n  13075.7 ms  ✓ OrdinaryDiffEqIMEXMultistep\n  15536.0 ms  ✓ OrdinaryDiffEqSDIRK\n   7809.9 ms  ✓ OrdinaryDiffEqExponentialRK\n  33370.4 ms  ✓ OrdinaryDiffEqFIRK\n  26022.8 ms  ✓ OrdinaryDiffEqBDF\n  45629.8 ms  ✓ OrdinaryDiffEqDefault\n 123401.4 ms  ✓ BoundaryValueDiffEqMIRK\n  11077.6 ms  ✓ OrdinaryDiffEq\n  16474.6 ms  ✓ DelayDiffEq\n  22183.6 ms  ✓ BoundaryValueDiffEqShooting\n  22525.3 ms  ✓ StochasticDiffEq\n 152888.9 ms  ✓ BoundaryValueDiffEqFIRK\n  12771.3 ms  ✓ BoundaryValueDiffEq\n  11030.7 ms  ✓ DifferentialEquations\n  256 dependencies successfully precompiled in 411 seconds. 60 already precompiled.\nPrecompiling SpecialFunctionsExt...\n    583.5 ms  ✓ ColorVectorSpace → SpecialFunctionsExt\n  1 dependency successfully precompiled in 1 seconds. 54 already precompiled.\nPrecompiling SparseMatrixColoringsColorsExt...\n    787.6 ms  ✓ SparseMatrixColorings → SparseMatrixColoringsColorsExt\n  1 dependency successfully precompiled in 1 seconds. 29 already precompiled.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Direction field for Example 2.12.\n\n\n\n\nLet \\(f(t,x) = rx(1-x) - h(1+\\sin(2\\pi t))\\) and denote the solution to \\(\\dot{x} = f(t,x)\\) with \\(x(0) = x_0\\) by \\(\\phi(t,x_0)\\). Since \\(f\\) is periodic in \\(t\\) with period one, it is useful to view the solution as a curve wrapping around the cylinder \\((0,1)\\times{\\mathbb R}\\). The uniqueness of solutions implies that there is a unique curve passing through each point on the cylinder (solutions don’t intersect each other). Since there are no constant solutions, it is natural to ask if there are any periodic solutions, and since we are forcing the system with period one, we look for solutions with period one. That is, solutions with \\(\\phi(1,x_0) = x_0\\). In short, we view \\(p(x_0) = \\phi(1,x_0)\\) as a map from \\({\\mathbb R}\\) to \\({\\mathbb R}\\) and look for fixed points of this map. These maps are referred to as Poincaré maps after Hénri Poincaré.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html#sec-1storder-exr",
    "href": "ode-intro-geom.html#sec-1storder-exr",
    "title": "2  Scalar first order differential equations",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\nYou should be familiar with material on basic differential equations in Chapter 4 of OpenStax Calculus Volume 2, especially problems 119-142 in Section 4.2 and problems 208-250 and 257-261 in Section 4.4.\n\nExercise 2.3 Solve the initial value problem \\(x^2y' = (2+x)y\\) with \\(y(1)=1\\).\n\n\n\nseparate and integrate \\[\\begin{align*}\nx^2y' &= (2+x)y \\\\\n\\frac{y'}{y} &= \\frac{2}{x^2}+\\frac{1}{x} \\\\\n\\int \\frac{y'}{y} \\, dx &= \\int \\frac{2}{x^2} \\, dx +\\int \\frac{1}{x}  \\, dx \\\\\n\\log y &= -\\frac{2}{x} + \\log x + C \\\\\n\\end{align*}\\]\napply initial conditions by substituting 1 for \\(x\\) and \\(y\\) \\[\\log 1 = -\\frac{2}{1} + \\log 1 + C \\Rightarrow C = 2\\]\nsolve relation for \\(y\\) in terms of \\(x\\) \\[\\begin{align*}\ny(x) &= \\exp\\left(-\\frac{2}{x} + \\log x + 2\\right) \\\\\n     &= x\\exp\\left(2-\\frac{2}{x}\\right)\n\\end{align*}\\]\n\n\n\nExercise 2.4  \n\n\n\n\n\n\n\n\nHirsch, Morris W., Stephen Smale, and Robert L. Devaney. 2013. Differential Equations, Dynamical Systems, and an Introduction to Chaos. Academic press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "ode-intro-geom.html#footnotes",
    "href": "ode-intro-geom.html#footnotes",
    "title": "2  Scalar first order differential equations",
    "section": "",
    "text": "By scalar I just mean a single equation, with a single dependent variable.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Scalar first order differential equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html",
    "href": "M3503-lec02-Systems.html",
    "title": "3  Systems of First order equations",
    "section": "",
    "text": "3.1 The General Nonlinear System: Existence and Uniqueness of Solutions\nConsider a system of differential equations of the form \\(y'(t) = f(t,y(t))\\), where \\(f:{\\mathbb R}\\times{\\mathbb R}^n\\to{\\mathbb R}^n\\) and we seek a solution \\(y:{\\mathbb R}\\to {\\mathbb R}^n\\). That is, \\(y\\) is a vector, or n-tuple of functions, and the function \\(f\\) takes a vector and a real number as inputs and returns a vector as an output. The equations are often accompanied by initial conditions of the form \\(y(t_0) = y_0\\). The solution is a curve in \\({\\mathbb R}^n\\) parameterized by \\(t\\) and passing through the point \\(y_0\\).\nSufficient conditions to guarantee the existence of a unique solution through any point are that \\(f\\) be continuous in \\(t\\) and differentiable in \\(y\\). However, a weaker condition than differentiability is also known to suffice.\nEquivalently, a function is Lipschitz continuous if the slopes of its secant lines are all bounded. For example, \\(|x|\\) is Lipschitz continuous; \\(x^{2/3}\\) is continuous but not Lipschitz since the secant lines tend towards vertical near the origin. Note, \\(x^{2/3}\\) is (locally) Lipschitz so long as the set of interest is bounded away from the origin. In one dimension, \\(\\left\\|x-y\\right\\|\\) can be assumed to be \\(|x-y|\\). In higher dimensions \\(\\left\\|\\cdot\\right\\|\\) is most often the standard Euclidian distance.\nThe classic result for the existence of solutions to our system of differential equations is as follows.\nVarious proofs of the above theorem can be found in most advanced books on differential equations.\nSystems of higher order differential equations can be expressed as systems of linear equations by introducing the derivatives as new state variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-general-nonlinear-system-existence-and-uniqueness-of-solutions",
    "href": "M3503-lec02-Systems.html#the-general-nonlinear-system-existence-and-uniqueness-of-solutions",
    "title": "3  Systems of First order equations",
    "section": "",
    "text": "Definition 3.1 (Lipschitz Continuity) A function \\(f\\) is said to be Lipschitz continuous on some subset \\(U\\) of its domain if there exists a constant \\(M\\) for which \\(\\left\\|f(x)-f(y)\\right\\| &lt; M\\left\\|x-y\\right\\|\\) whenever \\(x,y\\in U\\) with \\(x\\ne y\\).\n\n\n\n\nTheorem 3.1 If \\(f(t,y)\\) is continuous in the independent variable, \\(t\\), and Lipschitz continuous in \\(y\\), then the initial value problem \\(y' = f(t,y)\\), \\(y(t_0) = y_0\\) has a unique solution on some open interval \\(a &lt; t_0 &lt; b\\).\n\n\n\nExample 3.1 An example for which solutions exist, but are not unique is the simple scalar initial value problem \\(y' = y^{2/3}\\), \\(y(0) = 0\\). The function \\(y^{2/3}\\) is continuous for all \\(y\\), but has a cusp at the origin. It is not differentiable, nor is it Lipschitz continuous at the origin. Yet the differential equation is easily solved using the method of separation of variables, leading to the solutions \\(y(t) = \\frac{1}{3}(t+c)^3\\). Taking \\(c=0\\) gives a solution passing through the origin. A second obvious solution is \\(y(t) = 0\\). Moreover, we can splice these solutions together to get an infinite family of solutions to the initial value problem: \\[\\begin{equation}\n    y(t) = \\begin{cases}\n          \\frac{1}{3}(t+c_1)^3 & t &lt; -c_1  \\\\\n          0 & -c_1 \\le t \\le c_2 \\\\\n          \\frac{1}{3}(t-c_2)^3 & t &gt; c_2\n           \\end{cases}\n   \\end{equation}\\] with \\(c_1\\) and \\(c_2\\) arbitrary nonnegative constants.\n\n\n\nExample 3.2 Express the second order equation \\((t-3)y''(t) + t^2y'(t) + 7y(t) = f(t)\\) as a system of two first order equations.\nLet \\(z  = y'\\). Then the second order equation becomes \\((t-3)z'(t) + t^2z(t) + 7y(t) = f(t)\\). This together with the definition of \\(z\\) gives a pair of first order equations. Dividing through by \\((t-3)\\) puts the system in the basic form \\[\\begin{align*}\n    y'(t) &= z(t) \\\\\n    z'(t) &=  -\\frac{t^2}{t-3}z(t) - \\frac{7}{t-3}y(t) + f(t)\n  \\end{align*}\\] The right hand side is linear, and hence differentiable, in \\((y,z)\\) and continuous in \\(t\\) on any interval bounded away from \\(t=3\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-big-picture",
    "href": "M3503-lec02-Systems.html#the-big-picture",
    "title": "3  Systems of First order equations",
    "section": "4.1 The big picture",
    "text": "4.1 The big picture\nConsider the linear system of ordinary differential equations \\(x' = Ax\\) where \\(x(t) = \\left( x_1(t), x_2(t), \\dots, x_n(t) \\right)^T\\)and \\(A\\) is an \\(n\\times n\\) matrix of real numbers (constants). The general theory for finding all solutions to the linear system is based on an eigenvalue decomposition of the matrix \\(A\\). Let \\(r_1, r_2, \\dots r_m\\) be the \\(m\\) eigenvalues of \\(A\\). The theory of matrices and eigenvalues guarantees that \\(A\\) has at least one eigenvalue, and no more than \\(n\\) distinct eigenvalues, hence we can arrange things so that \\(1 \\le m \\le n\\) and all the \\(m\\) eigenvalues are distinct. That is, \\(r_i=r_j \\leftrightarrow i=j\\). A handy theorem from linear algebra states that there is an invertible matrix \\(S\\), for which \\(S^{-1}AS = \\begin{pmatrix} J_1 & & 0 \\\\ & \\ddots & \\\\ 0  & & J_m \\end{pmatrix}\\).\nWe know from linear algebra that each eigenvalue of \\(A\\) has a multiplicity.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-diagon",
    "href": "M3503-lec02-Systems.html#sec-diagon",
    "title": "3  Systems of First order equations",
    "section": "4.2 Real and Distinct Eigenvalues",
    "text": "4.2 Real and Distinct Eigenvalues\nConsider the system \\(y'=Ay\\) where \\(A\\) is a real \\(n\\times n\\) matrix, and suppose that \\(A\\) has a real eigenvalue \\(r\\) with an associated eigenvector \\(u\\). Then \\(y(t) = e^{rt}u\\) a solution. This is easy to verify by direct substitution.\nSuppose \\(u_1,\\dots,u_m\\) are \\(m\\) linearly independent eigenvectors of \\(A\\) with associated eigenvalues \\(r_1,\\dots,r_m\\). We look for a solution of the form \\[\\begin{equation}y(t) = \\sum_i^m x_i(t)u_i\\end{equation}\\] Substituting this form into the differential equation leads to \\[\\begin{equation} \\sum_i^m x'_i(t)u_i = A\\sum_i^m x_i(t)u_i = \\sum_i^m r_ix_i(t)u_i \\Rightarrow \\sum_i^m \\left(x'_i(t)-rx_i(t)\\right)u_i = 0 \\end{equation}\\] Since the vectors are linearly independent, it follows that \\(x'_i(t)-rx_i(t) = 0\\), \\(i=1,\\dots,m\\), which has the solutions \\(x_i(t) = c_ie^{r_it}\\), \\(i=1,\\dots,m\\), where each \\(c_i\\) is an arbitrary constant. Thus, \\[\\begin{equation}y(t) = \\sum_i^m c_iu_ie^{r_it} \\end{equation}\\] The effect of choosing the eigenvectors as a basis is to transform the system to a set of \\(m\\) decoupled equations that have well-known solutions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-complex",
    "href": "M3503-lec02-Systems.html#sec-complex",
    "title": "3  Systems of First order equations",
    "section": "4.3 Complex Eigenvalues",
    "text": "4.3 Complex Eigenvalues\nConsider the system \\(y'(t) = Ay(t)\\) where \\(A\\) is a real \\(n\\times n\\) matrix, and suppose that \\(A\\) has a complex eigenvalue \\(r = a+bi\\) with an associated eigenvector \\(u = v + iw\\). It follows by direct substitution that \\(v-iw\\) is also an eigenvector of \\(A\\) associated with the eigenvalue \\(a-ib\\).\nWe seek a solution of the form \\(y(t) = x_1(t) v + x_2(t) w\\) where \\(x_1\\) and \\(x_2\\) are scalar functions of \\(t\\). Direct computation yields that \\(A(x_1v + x_2w) = (ax_1 +bx_2)v + (-bx_1+ax_2)w\\). Hence, \\(x_1\\) and \\(x_2\\) must satisfy the pair of differential equations \\[\\begin{align*}\n    x_1'(t) &= ax_1(t) +bx_2(t) \\\\\n    x_2'(t) &= -bx_1(t) +ax_2(t)\n\\end{align*}\\] Since \\(e^{at}\\) is an integrating factor for both the above equations, \\[\\begin{align}\n  \\frac{d}{dt} \\left(x_1e^{-at}\\right) &= bx_2e^{-at}, \\label{intfactx1}\\\\\n\\frac{d}{dt} \\left(x_2e^{-at}\\right) &= -bx_1e^{-at}.\\label{intfactx2}\n\\end{align}\\] Differentiating the first of these equations and eliminating \\(x_2\\) using the second yields \\[\\begin{equation}\n\\frac{d^2}{dt^2} \\left(x_1e^{-at}\\right) = b\\frac{d}{dt}\\left( x_2e^{-at}\\right) = -b^2x_1e^{-at}.\n\\end{equation}\\] Hence, \\(x_1e^{-at} =  c_1\\cos(bt) + c_2\\sin(bt)\\), which implies \\(x_1(t)=  e^{at}\\left( c_1\\cos(bt) + c_2\\sin(bt)\\right)\\). To compute \\(x_2\\) we use as follows: \\[\\begin{align*}\nbx_2e^{-at}\n&=  \\frac{d}{dt} \\left(x_1e^{-at}\\right) \\\\\n&= \\frac{d}{dt} \\left( c_1\\cos(bt) + c_2\\sin(bt)\\right) \\\\\n&= \\left( -bc_1\\sin(bt) + bc_2\\cos(bt)\\right) \\\\\nx_2(t) &= e^{at} \\left( -c_1\\sin(bt) + c_2\\cos(bt)\\right)\n\\end{align*}\\] A nice way to memorize these solutions is to write them in a matrix form. \\[\\begin{equation}\n  \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} = e^{at}\\begin{pmatrix} \\cos(bt) & \\sin(bt) \\\\ -\\sin(bt) & \\cos(bt) \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\n\\end{equation}\\] The matrix on the right is the rotation matrix corresponding to counterclockwise rotation by the angle \\(bt\\). Hence, the solutions form a spiral in the \\(u\\)-\\(v\\) plane.\nReturning to the original coordinates, \\[\\begin{equation}\n  y(t) = c_1e^{at}\\left(v\\cos(bt) - w\\sin(bt)\\right) + c_2e^{at}\\left(v\\sin(bt) + w\\cos(bt)\\right)\n\\end{equation}\\] Note that up until now we have not specified the size of the original system. The solutions found above work regardless of the number of equations in the original system. If \\(A\\) is a \\(2\\times2\\) matrix, then the solution can be expressed compactly by defining \\(S\\) as the matrix whose columns are \\(v\\) and \\(w\\). In this case \\(y = Sx = e^{at}SR(bt)C\\) where \\(R(\\theta)\\) is the rotation matrix through angle \\(\\theta\\) and \\(C\\) is the transpose of \\(\\begin{pmatrix} c_1 & c_2\\end{pmatrix}\\). If we have initial conditions at \\(t=0\\), then \\(y(0) = SC\\), since \\(R(0)\\) is the identity (rotation by zero). Thus, \\(C= S^{-1}y(0)\\) and \\[\\begin{equation}\n  y(t) = e^{at}SR(bt)S^{-1}y(0).\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-geneig",
    "href": "M3503-lec02-Systems.html#sec-geneig",
    "title": "3  Systems of First order equations",
    "section": "4.4 Repeated Eigenvalues",
    "text": "4.4 Repeated Eigenvalues\nWe have seen how to find solutions if we know a sufficient number of linearly independent eigenvectors. In this section we tackle the case where the geometric multiplicity of an eigenvalue (the dimension of the Eigenspace) is less than its algebraic multiplicity. We do this by first finding a general solution for a simple case and then derive a method to transform, through a change of basis, any other system to a similar simple system.\n\n4.4.1 The second simplest case\nConsider the system \\[\\begin{align}\n  \\dot{x}_1(t) &= rx_1(t) + x_2(t),  \\label{simple1}\\\\\n  \\dot{x}_2(t) &= rx_2(t), \\label{simple2}\n\\end{align}\\] which when written in matrix form looks like \\[ \\begin{pmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\end{pmatrix}\n  = \\begin{pmatrix} r & 1 \\\\ 0 & r \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}  \\]\nThe matrix \\(J =  \\begin{pmatrix} r & 1 \\\\ 0 & r \\end{pmatrix}\\) has a single eigenvalue \\(r\\) with multiplicity 2. However, all eigenvectors of \\(J\\) are multiples of \\(\\vec{u} = \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix}\\). Hence, the eigenvalue \\(r\\) has geometric multiplicity one.\nTo solve the system, first integrate to obtain \\(x_2(t) = c_2e^{rt}\\), then substitute this into and integrate a second time using the integrating factor \\(e^{-rt}\\) to obtain \\[\\begin{align*}\n  \\dot{x}_1(t) = rx_1(t) &+ c_2e^{rt} \\\\\n  \\dot{x}_1(t) - rx_1(t) &=  c_2e^{rt} \\\\\n  \\frac{d}{dt}\\left({x}_1(t)e^{-rt}\\right) &=  c_2 \\\\\n  {x}_1(t)e^{-rt} &=  c_2t  + c_1\\\\\n  {x}_1(t) &=  c_2te^{rt}  + c_1e^{rt}\n\\end{align*}\\] This solution can be written in vector form as \\[\\begin{equation}\n  \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix}\n    = c_1 e^{rt}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    + c_2 \\left( te^{rt}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}  + e^{rt}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right)\n\\end{equation}\\] or in matrix form as \\[\\begin{equation}\n  \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix}\n    = \\begin{pmatrix} e^{rt}  & te^{rt}\\\\ 0 & e^{rt} \\end{pmatrix}\n      \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\n\\end{equation}\\] Both forms will be convenient at times.\n\n\n4.4.2 Generalized Eigenvectors\n\nDefinition 4.1 (Generalized eigenvector) A generalized eigenvector of \\(A\\) is a vector \\(v\\) for which \\[\\left(A-rI\\right)^m v = 0\\] for some positive integer \\(m\\).\n\nComments:\n\nClearly, any eigenvector of \\(A\\) is a generalized eigenvector (with \\(m=1\\)).\nIf \\(v\\) is a generalized eigenvector of \\(A\\), then the scalar \\(\\lambda\\) in the definition is an eigenvalue of \\(A\\). It also follows that \\(u = \\left(A-\\lambda I\\right)^{m-1}v\\) is an associated eigenvector.\n\n\nTheorem 4.1 If \\(\\lambda\\) is an eigenvalue of \\(A\\) with algebraic multiplicity \\(k\\), then the null space of \\((A-\\lambda I)^k\\) has dimension \\(k\\).\n\n\nTheorem 4.2 Every matrix \\(A\\) has a full set of generalized eigenvectors. That is, there is a (possibly complex) linearly independent set \\(v_1,\\dots,v_n\\) of generalized eigenvectors for every \\(n\\times n\\) matrix \\(A\\).\n\n\nExample 4.1 Consider the linear system \\(\\vec{y}' = A\\vec{y}\\) with \\(\\vec{y}(0) = \\vec{y}_0\\) and \\[\\begin{equation} A = \\begin{pmatrix} 4 & -4 \\\\  1 & 0 \\end{pmatrix} \\end{equation}\\] The characteristic polynomial for \\(A\\) is \\((4-r)(-r) -(-4)(1) = r^2 - 4r + 4 = (r-2)^2\\). Hence, \\(r_1 = 2\\) is an eigenvalue of \\(A\\) with algebraic multiplicity 2. To find the eigenvectors of \\(A\\) associated with \\(r_1\\) we construct and simplify\n\\[(A -r_1I) = \\begin{pmatrix} 4-2 & -4 \\\\  1 & 0-2 \\end{pmatrix}\n               = \\begin{pmatrix} 2 & -4 \\\\  1 & -2 \\end{pmatrix}\n               \\sim \\begin{pmatrix} 1 & -2 \\\\  0 & 0 \\end{pmatrix}\\] Thus the solutions to \\((A-2I)\\vec{u} = 0\\) are all multiples of \\(\\vec{u} = \\begin{pmatrix} 2 & 1 \\end{pmatrix}.\\) This is a one dimensional subspace. Hence \\(r_1\\) has geometric multiplicity 1. As it turns out, the particular generalized eigenvector we want is a solution to \\[(A -r_1I)\\vec{v}  = \\vec{u}.\\] \\(\\vec{v}\\) will be a generalized eigenvector since \\[(A -r_1I)^2\\vec{v}  = (A -r_1I)\\vec{u} = 0.\\] To find \\(\\vec{v}\\), we reduce the augmented matrix \\[\\left(\\begin{array}{cc|c} 2 & -4 & 2 \\\\ 1 & -2 & 1 \\end{array} \\right)\n     \\sim \\left(\\begin{array}{cc|c}  1 & -2 & 1\\\\ 0 & 0 & 0 \\end{array} \\right)\\] Thus, the entries of \\(\\vec{v}\\) satisfy \\(v_1-2v_2 = 1\\). Setting \\(v_2  = a \\in {\\mathbb R}\\) give \\(\\vec{v} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}a\\). We will choose \\(a=0\\) to give \\(\\vec{v} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\). Note that \\(\\vec{u}\\) and \\(\\vec{v}\\) are linearly independent and hence form a basis for \\({\\mathbb R}^2\\).\nWe look for solutions relative to this basis: \\[\\vec{y}(t) = x_1(t)\\vec{u} + x_2(t)\\vec{v}\\] That is, \\(x_1\\) and \\(x_2\\) are the coordinates of our solution with respect to the basis \\(\\left\\{ \\vec{u},\\vec{v}\\right\\}\\). Substituting this form for \\(\\vec{y}\\) into the differential equation leads to \\[\\begin{align*}\n      x'_1(t)\\vec{u} + x'_2(t)\\vec{v} &= x_1(t)A\\vec{u} + x_2(t)A\\vec{v}  \\\\\n      &= x_1(t)r_1\\vec{u} + x_2(t)\\left(r_1\\vec{v} + \\vec{u}\\right)  \\\\\n      &= \\left(r_1x_1(t) + x_2(t)\\right)\\vec{u} + r_1x_2(t)\\vec{v}\n   \\end{align*}\\] Finally, equating the coefficients on both sides of the equation leads to the simpler system given by Equations (\\(\\ref{simple1}\\)) and (\\(\\ref{simple2}\\)) above, which have solutions \\[\\begin{align*}\n    x_1(t) &=  c_2te^{r_1t}  + c_1e^{r_1t},\\\\\n    x_2(t) &= c_2e^{r_1t}.\n  \\end{align*}\\] This leads to a solution for \\(\\vec{y}\\) of \\[\\vec{y}(t) = \\left(c_2te^{r_1t}  + c_1e^{r_1t}\\right)\\vec{u} + c_2e^{r_1t}\\vec{v}\n                = c_1e^{r_1t}\\vec{u} + c_2\\left(te^{r_1t}\\vec{u} + e^{r_1t}\\vec{v}\\right)\\] For our example we have \\(r_1 = 2\\), \\(\\vec{u} = \\begin{pmatrix} 2 \\\\ 1\\end{pmatrix}\\) and \\(\\vec{v} = \\begin{pmatrix} 1 \\\\ 0\\end{pmatrix}\\). Hence \\[\\vec{y}(t) = c_1e^{r_1t}\\begin{pmatrix} 2 \\\\ 1\\end{pmatrix} + c_2\\left(te^{r_1t}\\begin{pmatrix} 2 \\\\ 1\\end{pmatrix} + e^{r_1t}\\begin{pmatrix} 1 \\\\ 0\\end{pmatrix}\\right)\\]\n\n\n\n4.4.3 The Jordan form\nThe question remains if we have found all the solutions. The answer to this lies in a theorem from linear algebra that states, roughly, that given any \\(n\\times n\\) matrix \\(A\\), there is a linearly independent set of generalized eigenvectors of \\(A\\) that spans \\({\\mathbb R}^n\\). Moreover, proceeding as we have above, we will find such a set. It can then be shown that the solutions we’ve found will be linearly independent and yield the complete general solution to \\(y' = Ay\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-varparm",
    "href": "M3503-lec02-Systems.html#sec-varparm",
    "title": "3  Systems of First order equations",
    "section": "5.1 The method of variation of parameters",
    "text": "5.1 The method of variation of parameters\nConsider the system \\[\\begin{equation}\n  \\frac{d}{dt}y(t) = Ay(t) + f(t), \\qquad y(t_0) = y_0,  \\label{nonhom}\n\\end{equation}\\] where \\(A\\) is a given \\(n\\times n\\) matrix of real numbers and \\(f\\) is a given vector-valued function from \\({\\mathbb R}\\) to \\({\\mathbb R}^n\\) Suppose we know \\(n\\) linearly independent solutions to the homogeneous system \\(y' = Ay\\). Then the fundamental matrix, \\(\\Psi\\), whose columns are these \\(n\\) solutions satisfies \\[\\begin{equation}\n  \\frac{d}{dt}\\Psi(t) = A\\Psi(t) + f(t).\n\\end{equation}\\] Applying the method of variation of parameters, we seek a solution to of the form \\(y(t) = \\Psi(t)u(t)\\). Substituting this form into leads to \\[\\Psi'u + \\Psi u' = A\\Psi u + f.\\] Since \\(\\Psi\\) is a solution to the homogeneous problem, the above equation reduces to \\[ \\Psi u' =  f,\\] and hence, \\[\\begin{equation}u(t) = \\int \\Psi^{-1}(t) f(t) \\, dt.\\end{equation}\\] To satisfy, the initial conditions, we choose the particular solution \\(y_p = \\Psi u\\) satisfying \\(y_p(t_0) = y_0\\). Then \\[\\begin{equation}y(t) = \\Psi(t)\\Psi^{-1}(t_)y_0 + \\Psi(t)\\int_{t_0}^{t} \\Psi^{-1}(\\tau) f(\\tau) \\, d\\tau.\\end{equation}\\]\n\nExample 5.1 Consider the system \\(y' = Ay + f\\) with \\(A = \\begin{pmatrix} 3 & 3 \\\\ 1 & 5 \\end{pmatrix}\\) and \\(f = \\begin{pmatrix} t \\\\ e^{t} \\end{pmatrix}.\\) The characteristic polynomial of \\(A\\) is \\((3-r)(5-r)-3 = (6-r)(6+r)\\). The eigenvectors are \\(r_1 = 6\\) and \\(r_2 = 2\\) with associated eigenvectors \\(u_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) and \\(u_2 = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\). Thus two linearly independent solutions to the homogeneous problem are \\(y_1(t) = e^{6t}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) and \\(y_2(t) = e^{2t}\\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\). One possible fundamental matrix is \\[\\begin{equation}\n    \\Psi(t) = \\begin{pmatrix} e^{6t} & 3e^{2t} \\\\ e^{6t} & -e^{2t} \\end{pmatrix}\n    = \\begin{pmatrix} 1 & 3 \\\\ 1 & -1 \\end{pmatrix}\\begin{pmatrix} e^{6t} & 0 \\\\ 0 & e^{2t} \\end{pmatrix}\n  \\end{equation}\\] Then \\(\\Psi^{-1}(t) = \\frac{1}{4}\\begin{pmatrix} e^{-6t} & 3e^{-6t} \\\\ e^{-2t} & -e^{-2t} \\end{pmatrix}\\), and \\[\\begin{equation}\n    \\Psi(t)\\Psi^{-1}(\\tau) = \\frac{1}{4}\\begin{pmatrix} e^{6(t-\\tau)}+3e^{2(t-\\tau)} & 3e^{6(t-\\tau)}-3e^{2(t-\\tau)} \\\\ e^{6(t-\\tau)}-e^{2(t-\\tau)} & 3e^{6(t-\\tau)}+e^{2(t-\\tau)} \\end{pmatrix}.\n  \\end{equation}\\] The complementary solution satisfying the initial conditions is \\[\\begin{equation}\n    y_c(t) = \\Psi(t)\\Psi^{-1}(0)y_0 = \\frac{1}{4}\\begin{pmatrix} e^{6t}+3e^{2t} & 3e^{6t}-3e^{2t} \\\\ e^{6t}-e^{2t} & 3e^{6t}+e^{2t} \\end{pmatrix}y_0.\n  \\end{equation}\\] The particular solution satisfying \\(y_p(0)=0\\) is \\[\\begin{align*}\n  y_p(t) &= \\int_0^t\\Psi(t)\\Psi^{-1}(\\tau)f(\\tau)\\, d\\tau  \\\\\n  &= \\frac{1}{4}\\int_0^t \\strut \\begin{pmatrix} \\tau  e^{6(t-\\tau)}+3\\tau e^{2(t-\\tau)} + 3e^{6t-5\\tau}-3e^{2t-\\tau} \\\\ \\tau e^{6(t-\\tau)}-\\tau e^{2(t-\\tau)} + 3e^{6t-5\\tau}+e^{2t-\\tau} \\end{pmatrix}\\begin{pmatrix} \\tau \\\\ e^{\\tau} \\end{pmatrix} \\, d\\tau.\\\\\n      &=  {\\small \\begin{pmatrix}1\\\\1\\end{pmatrix}}\\int_0^t\\tfrac{\\tau}{4}  e^{6(t-\\tau)} \\,d\\tau\n        + {\\small \\begin{pmatrix}3\\\\-1\\end{pmatrix}}\\int_0^t \\tfrac{\\tau}{4} e^{2(t-\\tau)} \\,d\\tau\n        + {\\small \\begin{pmatrix}3\\\\3\\end{pmatrix}}\\int_0^t \\tfrac{1}{4}e^{6t-5\\tau} \\,d\\tau\n        - {\\small \\begin{pmatrix}3\\\\-1\\end{pmatrix}}\\int_0^t \\tfrac{1}{4}e^{2t-\\tau} \\,d\\tau \\\\\n      &=  {\\tiny \\begin{pmatrix}1\\\\1\\end{pmatrix}} \\left( \\tfrac{1}{144}-\\tfrac{t}{24} -\\tfrac{1}{144}e^{6t} \\right)\n      + {\\tiny \\begin{pmatrix}3\\\\-1\\end{pmatrix}}  \\left(\\tfrac{1}{16} -\\tfrac{t}{8} -\\tfrac{1}{16}e^{2t} \\right)\n        + {\\tiny \\begin{pmatrix}3\\\\3\\end{pmatrix}}  \\left(\\tfrac{1}{20}e^{6t} - \\tfrac{1}{20}e^t \\right)\n    - {\\tiny \\begin{pmatrix}3\\\\-1\\end{pmatrix}}  \\left(\\tfrac{1}{4}e^{2t} -\\tfrac{1}{4}e^t  \\right) \\\\\n       &=\n          \\tfrac{113}{720} e^{6t}{\\small \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}}\n     -\\tfrac{3}{16}e^{2t} {\\small \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}}\n     -\\tfrac{t}{12}{\\small \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix}}\n     -\\tfrac{1}{36}{\\small  \\begin{pmatrix} 7 \\\\ 2 \\end{pmatrix}}\n     +\\tfrac{1}{5}e^t {\\small \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}}\n     \\end{align*}\\]\nNote the first two terms are in fact homogeneous solutions that arise here to ensure the particular solution is zero when \\(t=0\\). The last three terms are of the form \\(At + B + Ce^t\\) which we would expect from the method of undetermined coefficients, with the caveat that \\(A\\), \\(B\\) and \\(C\\) are vectors.\nThe general solution is simply \\(y(t) = y_c(t) +  y_p(t)\\) with the as-yet unspecified initial conditions taken as a pair of arbitrary constants.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-solution-via-laplace-transforms",
    "href": "M3503-lec02-Systems.html#the-solution-via-laplace-transforms",
    "title": "3  Systems of First order equations",
    "section": "5.2 The solution via Laplace Transforms",
    "text": "5.2 The solution via Laplace Transforms\nApplying the Laplace Transform to leads to \\[\\begin{equation} sY(s) - y_0 = AY(s) + F(s).\\end{equation}\\] Note that \\(Y\\) is a vector whose entries are the transforms of the corresponding entry of \\(y\\). Since the transform is a linear operation (an integration), the matrix is essential pulled out of the integral. Solving for \\(Y\\) we find \\[\\begin{equation}Y(s) = (sI-A)^{-1}\\left(y_0 + F\\right).\\end{equation}\\] Let \\(G(s) = (sI-A)^{-1}\\) and \\(g = {\\mathcal L}^{-1}\\left\\{G\\right\\}\\). Note that \\(G\\) and \\(g\\) are both \\(n\\times n\\) matrices. The solution to the differential equation can be represented as a convolution: \\[\\begin{equation}y(t) = g(t)y_0 + \\int_0^t g(t-\\tau)f(\\tau)\\, d\\tau.\\end{equation}\\]\n\nExample 5.2 Consider the system \\(y' = Ay + f\\) with \\(A = \\begin{pmatrix} 3 & 3 \\\\ 1 & 5 \\end{pmatrix}\\) and \\(f = \\begin{pmatrix} t \\\\ e^{t} \\end{pmatrix}.\\) \\[\\begin{align*}\n    G(s) &= (sI-A)^{-1} \\\\\n         &= \\begin{pmatrix} s-3 & -3 \\\\ -1 & s-5 \\end{pmatrix}\\\\\n     &= \\begin{pmatrix} \\frac{s-5}{(s-6)(s-2)} &\\ & \\frac{3}{(s-6)(s-2)} \\\\[6pt] \\frac{1}{(s-6)(s-2)} &\\ & \\frac{s-3}{(s-6)(s-2)} \\end{pmatrix} \\\\[12pt]\n     &= \\frac{1}{4}\\begin{pmatrix} \\frac{1}{s-6} + \\frac{3}{s-2} &\\ & \\frac{3}{s-6} - \\frac{3}{s-2} \\\\[6pt] \\frac{1}{s-6} - \\frac{1}{s-2} &\\ & \\frac{3}{s-6} + \\frac{1}{s-2} \\end{pmatrix}\\\\[12pt]\n    g(t) &= \\frac{1}{4}\\begin{pmatrix} e^{6t} + 3e^{2t} &\\ & 3e^{6t} - 3e^{2t} \\\\[6pt] e^{6t} - e^{2t} &\\ & 3e^{6t} + e^{2t} \\end{pmatrix}\n  \\end{align*}\\] Note that the system for this example is the same as that of the previous example, and that \\(g(t-\\tau) = \\Psi(t)\\Psi^{-1}(\\tau)\\) where \\(\\Psi\\) is the fundamental matrix from the previous example. Of course, \\(g(t) = \\Psi(t)\\Psi^{-1}(0)\\), which means the rest of the computations follow the previous example exactly. The two methods are identical!\n\n\nExample 5.3  \nSolve the system \\(x' = Ax + g\\) where \\(A = \\begin{pmatrix} 4 & 3 \\\\ -1 & 0 \\end{pmatrix}\\) and \\(g = \\begin{pmatrix} 0 \\\\ t \\end{pmatrix}\\).\nThe matrix \\(A\\) has two eigenvalues, \\(r_1 = 1\\) and \\(r_2=3\\). Two associated eigenvectors are \\(u_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\) and \\(u_2 = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\), respectively. Thus, a fundamental matrix for the system is \\[\\Phi(t) = \\begin{pmatrix} e^t & 3e^{3t} \\\\ -e^t & -e^{3t} \\end{pmatrix}.\\] The method of variation of parameters gives the general solution in the form \\[x(t) = \\Phi(t)\\int \\Phi^{-1}(t) g(t) \\, dt.\\] Direct computations lead to \\[\\begin{align*}\n  \\Phi^{-1}(t) &= \\frac{1}{2e^{4t}} \\begin{pmatrix} -e^{3t} & -3e^{3t} \\\\ e^t & e^t \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -e^{-t} & -3e^{-t} \\\\ e^{-3t} & e^{-3t} \\end{pmatrix}. \\\\\n  \\Phi^{-1}(t)g(t) &= \\frac{1}{2}\\begin{pmatrix} -e^{-t} & -3e^{-t} \\\\ e^{-3t} & e^{-3t} \\end{pmatrix} \\begin{pmatrix}0 \\\\ t \\end{pmatrix}\n                    =  \\begin{pmatrix}  -\\frac{3}{2}te^{-t} \\\\  \\frac{1}{2}te^{-3t} \\end{pmatrix}. \\\\\n  \\int\\Phi^{-1}(t)g(t)\\,dt &=  \\begin{pmatrix}  -\\frac{3}{2}\\int te^{-t} \\,dt \\\\  \\frac{1}{2} \\int te^{-3t}\\, dt \\end{pmatrix}\n                             =  \\begin{pmatrix}  \\frac{3}{2} e^{-t}(t+1) + c_1\\\\  \\frac{1}{18} e^{-3t}(3t+1) + c_2 \\end{pmatrix}. \\\\\n    \\Phi(t)\\int\\Phi^{-1}(t)g(t)\\,dt\n        & =  \\begin{pmatrix} e^t & 3e^{3t} \\\\ -e^t & -e^{3t} \\end{pmatrix} \\begin{pmatrix}  \\frac{3}{2} e^{-t}(t+1) + c_1\\\\  \\frac{1}{18} e^{-3t}(3t+1) + c_2 \\end{pmatrix}. \\\\\n    &=  \\begin{pmatrix} t +  \\frac{4}{3} \\\\ -t - \\frac{13}{9}\\end{pmatrix} + c_1  e^t \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n    + c_2  e^{-3t} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n  \\end{align*}\\]\nNote the last two terms are the complementary solution. We could have ignored the constants of integration and simply added these on afterwards.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Systems of First order equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html",
    "href": "M3503-lec03-SecondOrderODE.html",
    "title": "4  Second order Linear Differential Equations",
    "section": "",
    "text": "4.1 The general theory of second order Linear Differential Equations\nThe first part of this course deals with second order linear differential equations with constant coefficients. However before we zero in on solution techniques for these equations it is worthwhile to take a brief look at the more general theory.\nMany physical problems can be cast as second order differential equations of the form \\(\\ddot{x} = f(t,x,\\dot{x})\\) with \\(\\ddot{x}\\) interpreted as acceleration, \\(x\\) as position, and \\(\\dot{x}\\) as velocity. These often arise as initial value problems, when combined with initial conditions of the form \\(x(0) = x_0\\), \\(\\dot{x}(0) = y_0\\).\nSome cases appear as boundary value problems with the conditions prescribed at two different values of the independent variable. \\(x(0) = a\\), \\(x(1) = b\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#sec-2ndordergt",
    "href": "M3503-lec03-SecondOrderODE.html#sec-2ndordergt",
    "title": "4  Second order Linear Differential Equations",
    "section": "",
    "text": "4.1.1 Differential Equations and Linear Algebra\nA second order linear differential equation has the form. \\[P(x)\\frac{d^2y}{dx^2} + Q(x)\\frac{dy}{dx} + R(x)y = G(x), \\tag{4.1}\\] where the functions \\(P\\), \\(Q\\), \\(R\\) and \\(G\\) are specified. To motivate these as linear we need to review a few concepts from linear algebra. In particular, we need to learn to view functions as vectors.\nOne key concept from linear algebra is a linear combination. Suppose \\(y_1, y_2, \\dots, y_n\\) are \\(n\\) functions defined on some interval \\([a,b]\\). For example, the set \\(1, x, x^2, x^3, x^4, x^5\\) with \\(0 \\le x \\le 1\\) is a set of 6 functions defined on the interval \\([0,1]\\). Using the jargon we learned in our first linear algebra course, we can construct any polynomial of degree 5 or lower by taking linear combinations of the 6 basis functions. Specifically, if \\(c_0, c_1, \\dots, c_5\\) be any 6 numbers, then \\(y(x) = c_0 + c_1x + \\dots + c_5x^5\\) is a polynomial. If \\(c_5\\ne0\\), \\(y\\) is a polynomial of degree 5, otherwise, it has a lower degree. We can do the same constructions for any set of functions. Given a set of functions \\(\\{y_1, y_2, \\dots, y_n\\}\\) all with the same domain, \\([a,b]\\), and a set of scalars \\(\\{c_1, \\dots, c_n\\}\\), the linear combination \\(y = c_1y_1 + \\dots + c_ny_n\\) is a function with the same domain.\nFrom our calculus course, we know that if \\(y_1, \\dot, y_n\\) are all continuous on \\([a,b]\\), then the linear combination is also continuous. In short, functions are vectors, and all the jargon we learned in linear algebra can be applied to functions.\n\nDefinition 4.1 The set of functions \\(\\{y_1, y_2, \\dots, y_n\\}\\) is said to be linearly independent if the only scalars \\(c_1,\\dots,c_n\\) for which \\(c_1y_1 + \\dots + c_ny_n = 0\\) is \\(c_1=c_2=\\dots=c_n=0\\).\n\n\nExample 4.1 To show the two functions \\(\\sin x\\) and \\(\\cos x\\) defined for \\(0 \\le x \\le \\pi\\) are linearly independent, we need to show that \\(c_1\\sin x + c_2\\cos x = 0\\) for all \\(x\\in[0,\\pi]\\) only if \\(c_1=c_2=0\\). Since \\(\\sin 0 = 0\\) and \\(\\cos 0 = 1\\), setting \\(x=0\\) implies \\(c_2=0\\). Similarly, setting \\(x=\\pi/2\\) implies \\(c_1=0\\). Hence, the two functions are linearly independent.\n\nSo why do we call Equation 4.1 a linear differential equation? It is useful to introduce a shorthand notation for the left hand side of Equation 4.1. In more advanced courses this would be introduced as a linear operator and some fancy theorems would be invoked. An operator is simply a mathematical object that maps functions to other functions in the same way the functions of first year calculus map numbers to other numbers. Suppose \\(y\\) is twice differentiable, then we can define a new function, \\(Ly\\), by \\[\\begin{equation}\n  Ly = P\\frac{d^2y}{dx^2} + Q\\frac{dy}{dx} + Ry.\n\\end{equation}\\] We use this to define the operator \\(L\\). Simply put, given a known, twice differentiable function \\(y\\), the operation \\(L\\) returns a new function which we refer to simply as \\(Ly\\). The linearity of the operator refers to the fact that \\(L\\) is a linear transformation. That is, for any two twice-differentiable functions \\(u\\) and \\(v\\) and any two real numbers (scalars) \\(a\\) and \\(b\\), \\(L(au+bv) = aLu + bLv\\).\n\n\n4.1.2 The solution set\nA solution to the second order differential equation Equation 4.1, is any twice differentiable function \\(y(x)\\) that satisfies Equation 4.1. Since we are looking for \\(y\\) as a function of \\(x\\), it makes sense to refer to \\(x\\) as our independent variable and \\(y\\) as our dependent variable. The linearity of the equation makes some more jargon from linear algebra useful. In particular,\n\n\\(P\\), \\(Q\\) and \\(R\\) are the coefficients of the equation,\nthe equation is homogeneous if \\(G=0\\),\nthe equation is nonhomogeneous if \\(G\\ne 0\\),\nthe equation is constant coefficient if \\(P\\), \\(Q\\) and \\(R\\) constants.\n\nThe right hand side, \\(G\\), is typically an applied force or current or something similar. Hence we refer to \\(G\\) as the forcing function.\nThere are two theorems that are relevant here.\n\nTheorem 4.1 If \\(y_1\\) and \\(y_2\\) are solutions of the homogeneous problem then for any constants \\(c_1\\) and \\(c_2\\), the linear combination \\(y = c_1y_1 + c_2y_2\\) is also a solution of the homogeneous problem.\n\nIn the language of linear algebra, any linear combination of two solutions is also a solution. The proof follows almost immediately from substituting the linear combination of the two solutions into the differential equation.\n\nTheorem 4.2 If \\(y_1\\) and \\(y_2\\) are two linearly independent solutions to the homogeneous problem and \\(y_p\\) is any solution to the nonhomogeneous problem, then \\(y = c_1y_1 + c_2y_2 + y_p\\) is also a solution to the nonhomogeneous problem for any constant \\(a\\).\n\nAgain, the proof is simply a matter of plugging the proposed solution into the differential equation.\n\nTheorem 4.3 If \\(y_1\\) and \\(y_2\\) are two linear independent solutions of the homogeneous problem and \\(y_p\\) is any solution to the nonhomogeneous problem, then every solution of the homogeneous problem can be written in the form \\(y = c_1y_1 + c_2y_2 + y_p\\) for some real numbers \\(c_1\\) and \\(c_2\\).\n\nIn short, this theorem states that if we can find two linearly independent solutions, then we know all the solutions to the homogeneous problem, and if we also know any one solution to the nonhomogeneous problem, we know them all. The form of the solution set should look very familiar. It has the same structure as the set of solutions to a system of linear algebraic equations.\nThe proof of this theorem is beyond the scope of this course. However, we will be able to give a sketch of the proof later in the term.\nMany of the theorems from linear algebra apply to the operator \\(L\\). We refer to \\(Ly=0\\) as the homogeneous, or complementary problem, and \\(Ly = G\\) as the nonhomogeneous problem. As with linear systems, if \\(y_p\\) is a solution to the nonhomogeneous problem and \\(y_h\\) is any solution to the homogeneous problem, then \\(y = cy_h + y_p\\) is also a solution to the nonhomogeneous problem, for any constant \\(c\\). This can be easily verified by direct substitution. Further, if \\(y_1\\) and \\(y_2\\) are both solutions to the homogeneous problem, then any function of the form \\(y = c_1y_1 + c_2 y_2\\) is also a solution to the homogeneous problem. Note the slight difference in the wording of these results from the theorems stated above. When \\(L\\) is a second order linear differential operator, such as arises from second order linear differential equation, then it can further be shown that the homogeneous problem \\(Ly=0\\) has a two dimensional solution set. That is, we can always find two linearly independent solutions, \\(y_1\\) and \\(y_2\\) and every solution to \\(Ly=0\\) can be written as a linear combination of these. This follows from a theorem for the existence and uniqueness of solutions to systems of first order differential equations, which will be sketched later in these notes.\nLater in the notes we will also encounter the eigenvalue problem \\(Ly=\\lambda y\\), where we are interested in finding values of the constant \\(\\lambda\\) for which solutions to the eigenvalue problem exist. These typically arise in boundary value problems, and the eigenvalues correspond to modes of oscillation in the physical system.\n\n\n\n\n\n\nFigure 4.1: Mass spring system\n\n\n\n\nExample 4.2 Consider the mass-spring system of Figure Figure 4.1 with a mass of \\(m\\) attached to a spring with spring constant \\(k\\). A differential equation model for the system can be obtained either by energy balance or force balance. The energy of the system is \\[\\begin{align*}\n    \\text{Total Energy} &= \\text{Kinetic Energy} + \\text{Spring Potential} \\\\\n    &= \\frac{1}{2}m\\dot{x}^2 + \\frac{1}{2}kx^2\n  \\end{align*}\\] Differentiating and assuming the total energy is conserved (constant) leads to the second order differential equation \\(0 = m\\dot{x}\\ddot{x} + kx\\dot{x}\\) which simplifies to \\(m\\ddot{x} + kx = 0.\\) Since both \\(k\\) and \\(m\\) are positive constants, the solutions to the equation are functions whose second derivatives are negative multiples of themselves: \\(\\ddot{x} = -\\frac{k}{m}x\\). These are sines and cosines of the appropriate period: \\[x_1(t) = \\cos \\tfrac{k}{m} t, \\qquad x_2(t) = \\sin \\tfrac{k}{m} t.\\] The general solution to the differential equation is any linear combination of these two solutions: \\(x(t) = c_1\\cos \\tfrac{k}{m} t \\, + \\, c_2 \\sin \\tfrac{k}{m} t.\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#sec-2ndordercc",
    "href": "M3503-lec03-SecondOrderODE.html#sec-2ndordercc",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.2 Linear Second order differential equations with constant coefficients",
    "text": "4.2 Linear Second order differential equations with constant coefficients\n\n4.2.1 Theory and Techniques\nConsider the problem\n\\[ay'' + by' +cy = 0 \\tag{4.2}\\]\nBased on our experience with first order equations (a=0), we look for a solution of the form \\(y=e^{rx}\\). If we can find two such solutions, we’ve got them all! First, we substitute the ansatz, \\(y(x) = e^{rx}\\), into Equation 4.1 \\[\\begin{align*}\n  a\\frac{d^2}{dx^2}\\left(e^{rx}\\right) + b\\frac{d}{dx}\\left(e^{rx}\\right) +ce^{rx} &= 0 \\\\\n  ar^2e^{rx} + bre^{rx} +ce^{rx} &= 0 \\\\\n  \\left(ar^2 + br +c\\right)e^{rx} &= 0\n\\end{align*}\\] Factoring out \\(e^{rx}\\) we see that \\(r\\) must satisfy \\[ar^2 + br +c = 0. \\tag{4.3}\\] That is, \\[r = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}.\\]\nThere are three cases to consider.\nCase I: two real and distinct roots (\\(b^2-4ac&gt;0\\))\nThe general solution has the form \\[y = c_1e^{r_1x} + c_2e^{r_2x}\\] where \\(r_1\\) and \\(r_2\\) are the two real roots.\nCase II: one real root (\\(b^2 - 4ac = 0\\))\nHere we only have one independent solution \\(y_1 = c_1e^{rx}\\) with \\(r=-b/(2a)\\).\nAt some point in history, someone stumbled upon a second independent solution, \\(y_2 = xe^{rx}\\), with \\(r=-b/(2a)\\). While this it is not at all obvious why this should be a solution and how someone would come up with it, it is easy to verify that is is a solution and that \\(y_1\\) and \\(y_2\\) are linearly independent.\nWe leave it as an exercise to show, by direct substitution, that \\(y_2\\) solves Equation 4.2. That is, show that \\[a\\frac{d^2}{dx^2}\\left(xe^{rx}\\right) + b\\frac{d}{dx}\\left(xe^{rx}\\right) +cxe^{rx} = 0\\] provided \\(b^2 - 4ac =0\\) and \\(r=b/(2a)\\).\nTo show the two solutions are linearly independent, consider the combination \\[\\alpha e^{rx} + \\beta xe^{rx} = 0.\\]\nThe key concept with linear independence of functions, is that the equality must hold for all \\(x\\) in the domain of the functions. In this case, since \\(e^{rx} &gt; 0\\), it follows that \\(\\alpha + \\beta x = 0\\), which holds only for \\(x=-\\alpha/\\beta\\). Hence the two functions are linearly independent on any interval of nonzero length, which is all we care about.\nIn summary, the general solution in case II has the form \\[y = (c_1+c_2x)e^{rx} \\] with \\(r = -b/(2a)\\).\nCase III: complex roots (\\(b^2 - 4ac &lt; 0\\))\nIn this case, we have two complex roots, which we will express as \\(r_1 = \\alpha - \\beta i\\) and \\(r_2 = \\alpha + \\beta i\\). For those who are comfortable with complex functions, you will be happy to know that the two functions \\(e^{r_1 x}\\) and \\(e^{r_2 x}\\) are indeed linearly independent.\nThe only catch is that we started with a problem involving real variables and real functions and it would be nice to have real solutions. Fortunately, it is possible to show that the real and complex parts of the complex solutions are also linearly independent solutions. Hence, the general solution has the form\n\\[y(x) =  \\left( c_1 \\cos \\beta x  + c_2 \\sin  \\beta x  \\right) e^{\\alpha x}\\]\n\n\n4.2.2 Exercises\nAll of the exercises in Section 7.1 of OpenStax Calculus Volume 3 are good practice.\n\nExercise 4.1 Use Euler’s formula, \\(\\exp(i\\theta) = \\cos \\theta + i\\sin \\theta\\), to show that \\(\\cos \\theta = \\frac{1}{2}\\left(e^{i\\theta}+e^{-i\\theta}\\right)\\) and \\(\\sin \\theta = \\frac{1}{2i}\\left(\\exp(i\\theta)-\\exp(-i\\theta)\\right)\\).\n\n\nExercise 4.2 Use the results of Exercise 4.1 and Theorem 4.1 to show that the general solution of Equation 4.2 can be expressed as \\(y(t) = c_1e^{\\alpha t} \\cos \\beta t  + c_2e^{\\alpha t} \\sin \\beta t\\) if the roots of Equation 4.3 are complex.\n\n\nExercise 4.3 Find the general solutions of \\(y'' - 4 y' + 13 y = 0\\).\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCharacteristic Equation: \\(r^2 - 4r + 13 = 0\\)\nRoots: \\(r = 2 \\pm 3i\\) (complex case)\nGeneral solution, \\(y(t) = c_1e^{2t}\\cos(3t) + c_1e^{2t}\\sin(3t)\\)\n\n\n\n\n\nExercise 4.4 Find the general solutions of \\(y'' - 6 y' + 5 y = 0\\).\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCharacteristic Equation: \\(r^2 - 6r + 5 = 0\\)\nRoots: \\(r \\in \\{5,1\\}\\) (two distinct real roots)\nGeneral solution, \\(y(t) = c_1e^{5t} + c_1e^{t}\\)\n\n\n\n\n\nExercise 4.5 Find the general solutions of \\(y'' + 4 y' + 4 y = 0\\).\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCharacteristic Equation: \\(r^2 + 4r + 4 = 0\\)\nRoots: \\(r = -2\\) (double root)\nGeneral solution, \\(y(t) = c_1e^{-2t} + c_1te^{-2t}\\)\n\n\n\n\n\nExercise 4.6 Solve the Initial value problem \\(2y'' + 5y' - 3y = 0\\), with \\(y(0) = 4\\) and \\(y'(0) = 2\\).\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCharacteristic equation: \\(2r^2 + 5r - 3 = 0\\)\nRoots: \\(r \\in \\{\\frac{1}{2}, -3\\}\\) (distinct real roots)\nGeneral solution, \\(y(t) = c_1e^{\\frac{1}{2}t} + c_2e^{-3t}\\)\nApply initial conditions \\[\\begin{alignat*}{3}\n  y(0) &= &\\ c_1\\  &+ &\\ c_2 &=4\\\\\n  y'(0)&= &\\ \\frac{1}{2}c_1 \\ &- &\\ 3c_2 &= 2\n\\end{alignat*}\\]\nSolve for constants: \\(c_1 = 4\\) and \\(c_2 = 0\\)\nSolution to IVP: \\(y(t) = 4e^{\\frac{1}{2}t}\\)\n\n\n\n\n\nExercise 4.7 Solve the Boundary Value problem \\[y'' - 6y' + 9y = 0, \\qquad y(0) = 3, \\qquad y(1) = 0\\]\n\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nCharacteristic equation: \\(r^2 - 6r + 9 = 0\\)\nRoots: \\(r-3\\), (double root)\nGeneral solution, \\(y(t) = c_1e^{3t} + c_2te^{3t}\\)\nApply initial conditions \\[\\begin{alignat*}{3}\n  y(0)&= &\\ c_1\\  &  &      &=3 \\\\\n  y(1)&= &\\ e^{3}c_1 \\ &+ &\\ e^{3}c_2 &= 0\n\\end{alignat*}\\]\nSolve for constants: \\(c_1 = 3\\) and \\(c_2 = -3\\)\nSolution to BVP: \\(y(t) = 3e^{3t} - 3te^{3t}\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#finding-particular-solutions-of-the-nonhomogeneous-problem",
    "href": "M3503-lec03-SecondOrderODE.html#finding-particular-solutions-of-the-nonhomogeneous-problem",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.3 Finding particular solutions of the nonhomogeneous problem",
    "text": "4.3 Finding particular solutions of the nonhomogeneous problem\nThere are two textbook approaches to finding solutions to a nonhomogeneous linear problem. The first, variation of parameters, gives us a general formula for the solution that works in the constant coefficient case, and, with caveats, in the nonconstant coefficient case. The second approach, undetermined coefficients, is essentially a trick for the constant coefficient problem that works for certain forcing functions but not in general.\n\n4.3.1 Variation of Parameters\nConsider the general linear second order differential equation, \\[a(x)y'' + b(x)y' + c(x)y = g(x) \\tag{4.4}\\] The formulae we derive in this section allow \\(a\\), \\(b\\), and \\(c\\) to depend on \\(x\\), but we’ll drop the explicit dependence in the derivations for the sake of clarity.\nBefore we derive the general formula, it’s worth looking at how we can use a solution to the homogeneous problem to simplify the nonhomogeneous problem.\nSuppose we know one solution, \\(y_1\\), to the homogeneous problem (\\(g=0\\)). We express the general solution as a product of \\(y_1\\) and another function, say \\(u\\), and derive the ODE that \\(u\\) satisfies. I.e., substitute \\(y = uy_1\\) into our nonhomogeneous problem. \\[\\begin{align*}\n  a \\left(u''y_1 + 2u'y_1'+uy_1''\\right) + b\\left(u'y_1+uy_1'\\right) + cuy_1 &= g \\\\\n  a \\left(u''y_1 + 2u'y_1'\\right) + bu'y_1 + auy_1''+buy_1' +cuy_1 &= g \\\\\n  a \\left(u''y_1 + 2u'y_1'\\right) + bu'y_1  = g\n\\end{align*}\\] On that last step, we used the fact that \\(y_1\\) is a solution to the homogeneous problem. The last few terms cancel leaving us with an equation that only involves the derivatives of \\(u\\). Although this is still a second order equation for \\(u\\), we can view it as a first order equation for \\(u'\\). To make this easier to see, gather up the coefficients of \\(u''\\) and \\(u'\\) and let \\(U = u'\\). \\[ay_1 U' + \\left(2ay_1' + by_1\\right)U  = g\\] We can solve this first order linear equation for \\(U\\), integrate \\(U\\) to find \\(u\\), and multiply by \\(y_1\\) to find a solution to the original nonhomogeneous second order problem for \\(y\\).\nNow suppose we know two linearly independent solutions, \\(y_1\\) and \\(y_2\\), to the homogeneous problem Equation 4.4 with \\(g=0\\). We can use the same idea to find a particular solution to the nonhomogeneous problem Equation 4.4. The trick is to look for a solution of the form \\(y(x) = u(x)y_1(x) + v(x)y_2(x)\\). Notice that \\(y' = u'y_1  + uy'_1 + v'y_2  + vy'_2\\). It turns out looking for two functions, \\(u\\) and \\(v\\), gives us enough freedom to choose these functions so that \\(u'y_1  + v'y_2 = 0\\). This simplifies \\(y'\\) to \\(uy'_1 + vy'_2\\), and then \\(y'' = u'y'_1 + uy_1^{''} + v'y'_2 + vy_2^{''}\\). Substituting these into our general ODE yields \\[a \\left(u'y'_1 + uy_1^{''} + v'y'_2 + vy_2^{''}\\right) + b\\left(uy'_1 + vy'_2\\right) + c\\left(uy_1 + vy_2\\right) = g\\] This looks nasty, but recognizing that \\(y_1\\) and \\(y_2\\) satisfy the ODE, it reduces to \\[au'y'_1 + av'y'_2 = g\\] Thus, we’ve converted our second order problem for \\(y\\) into a pair of first order problems for \\(u'\\) and \\(v'\\). In matrix form, the pair of equations, and their solution have a nice structure. \\[\\begin{equation}\n    \\begin{pmatrix} y_1 & y_2 \\\\ y_1' & y_2' \\end{pmatrix}\n    \\begin{pmatrix} u' \\\\ v' \\end{pmatrix}\n        =\\begin{pmatrix} 0 \\\\ \\frac{g}{a} \\end{pmatrix}\n\\end{equation}\\] \\[\\begin{equation}\n    \\begin{pmatrix} u' \\\\ v' \\end{pmatrix}\n        =\\frac{1}{a} \\begin{pmatrix} y_1 & y_2 \\\\ y_1' & y_2' \\end{pmatrix}^{-1}\n        \\begin{pmatrix} 0 \\\\ g \\end{pmatrix}\n\\end{equation}\\] \\[\\begin{equation}\n    \\begin{pmatrix} u' \\\\ v' \\end{pmatrix}\n        =\\frac{1}{a(y_1y_2' - y_1'y_2} \\begin{pmatrix} y_2' & -y_2 \\\\ -y_1' & y_1 \\end{pmatrix}^{-1}\n        \\begin{pmatrix} 0 \\\\ g \\end{pmatrix}\n\\end{equation}\\] On the right hand side we have known functions of \\(x\\). The three functions \\(a\\), \\(y_1\\), and \\(y_2\\) represent our system, and the fourth function, \\(g\\), usually represents something like an external forcing or applied voltage. We’ll see this matrix form again when we study systems of first order equations.\nThe determinant, \\(y_1y_2' - y_1'y_2\\), is known as the Wronskian. Since \\(y_1\\) and \\(y_2\\) were linearly independent, we expect this to be nonzero. This isn’t always the case, but it is likely true for most practical cases you’ll encounter.\nWe can express \\(u\\) and \\(v\\) as integrals and express the solution \\(y\\) as \\[y = -y_1\\int \\frac{y_2g}{a(y_1y_2' - y_1'y_2)}  + y_2\\int \\frac{y_1g}{a(y_1y_2' - y_1'y_2)} \\tag{4.5}\\] We’ve use the two solutions to the homogeneous problem to reduce our nonhomogeneous problem to two integrations of known functions.\n\nExample 4.3 Use the method of variation of parameters to find the general solution to \\(y'' + 9y = \\sec 3x\\).\nFirst, solve the homogeneous problem. The characteristic equation is \\(r^2+9=0\\), which has complex roots, \\(r=\\pm 3i\\). The two solutions are \\(y_1(x) = \\cos(3x)\\) and \\(y_2(x) = \\sin(3x)\\).\nSecond, compute \\(y_1'\\) and \\(y_2'\\) and set up the system for \\(u'\\) and \\(v'\\) \\[\\begin{align*}\n    u'\\cos 3x  + v' \\sin(3x) &= 0 \\\\\n-3u'\\sin 3x  + 3v' \\cos(3x) &= \\sec 3x\n\\end{align*}\\] Isolate \\(u'\\) by multiplying the first equation through by \\(3\\cos(3x)\\), the second through by \\(\\sin(3x)\\) and taking the difference of the results \\[u' = -\\frac{1}{3}\\sin(3x) \\sec(3x)\\] Similarly, we find \\[v' =  -\\frac{1}{3}\\cos(3x) \\sec(3x) = -\\frac{1}{3}\\] Integrate to find \\(u\\) \\[\\begin{align*}\nu(x) &= -\\frac{1}{3}\\int \\sin(3x) \\sec(3x) \\, dx \\\\\n      &= -\\frac{1}{3}\\int \\tan(3x) \\, dx \\\\\n            &= \\frac{1}{9}\\log \\| \\cos 3x  \\|\n\\end{align*}\\] and \\(v = \\frac{x}{3}\\)\nPutting everything together,\n\\[y(x) = c_1\\cos(3x) + c_2\\sin(3x)\n           + \\frac{1}{9}\\cos(3x)\\log \\| \\cos(3x) \\|\n                 + \\frac{1}{3}x\\sin(3x)\\]\n\n\n\n4.3.2 The method of Undetermined Coefficients\nTable 7.2 of Openstax Calculus volume 3 lists the main forms of forcing functions amenable to the method. Be sure to also understand the problem solving strategy outlined below that table.\nTable 5.1 on page 132 of Kalbaugh’s text gives the same information and further examples.\n\n\n4.3.3 Exercises\nAll the problems in Openstax Calculus Volume 3 Section 7.2 are good practice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#initial-and-boundary-value-problems",
    "href": "M3503-lec03-SecondOrderODE.html#initial-and-boundary-value-problems",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.4 Initial and boundary value problems",
    "text": "4.4 Initial and boundary value problems\nThe general form of the solution has two arbitrary constants. Thus it is a two-parameter family of solutions. In practice, we are often interested in a solution satisfying additional constraints. These are typically posed as either initial conditions or boundary conditions. Initial conditions are given as constraints of the form \\(y(0) = y_1\\) and \\(y'(0) = y_2\\) for some specified constants \\(y_1\\) and \\(y_2\\). Boundary conditions are typically given as \\(y(0) = y_0\\) and \\(y(1) = y_1\\), but could involve derivatives of \\(y\\) at the boundaries.\n\nExample 4.4 Find a solution to \\(y'' + 2y' - 3 = 2x\\) satisfying the initial conditions \\(y(0) = 0\\) and \\(y'(0) = 1\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#summary",
    "href": "M3503-lec03-SecondOrderODE.html#summary",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.5 Summary",
    "text": "4.5 Summary\nTo find two linearly independent solutions to a homogeneous second order linear differential equation with constant coefficients, \\(ay'' + by' + cy' = 0,\\) first compute the characteristic roots \\(r = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}.\\) second, determine which of the following three cases holds and write down the appropriate solutions:\nTwo distinct real roots (\\(b^2 &gt; 4ac\\)): \\(y_1 = e^{r_1t} \\qquad y_2 = e^{r_2t}\\) with \\(r_1 = \\frac{-b - \\sqrt{b^2-4ac}}{2a}\\), and \\(r_2 = \\frac{-b + \\sqrt{b^2-4ac}}{2a}\\).\nA double root (\\(b^2 = 4ac\\)): \\(y_1 = e^{rt} \\qquad y_2 = te^{rt}\\) with \\(r = \\frac{-b}{2a}\\).\nComplex roots (\\(b^2 &lt; 4ac\\)): \\(y_1 = e^{\\alpha t} \\cos \\beta t \\qquad y_2 = e^{\\alpha t} \\sin \\beta t,\\) with \\(\\alpha = -\\frac{b}{2a}\\), and \\(\\beta = \\frac{\\sqrt{4ac-b^2}}{2a}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#discussion",
    "href": "M3503-lec03-SecondOrderODE.html#discussion",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.6 Discussion",
    "text": "4.6 Discussion\nNote that we refer to Equation 4.1 as linear when it is linear in the dependent variable. It may be nonlinear in the independent variable. For example,\napplying Kirchoff’s law to a simple electric circuit yields the model \\[LQ''(t) + RQ'(t) + \\frac{1}{C}Q(t) = E(t)\\] where the dependent variable, \\(Q\\), is the charge, and is assumed to depend on the time, \\(t\\). The parameters \\(L\\), \\(R\\) and \\(C\\) are the inductance, resistance and capacitance of the circuit and \\(E\\) is an applied voltage. If the fixed resistance \\(R\\) is replaced by a variable resistance, \\(R(t)\\), the equation is still linear in the dependent variable.\nMore generally, a differential equation is an equation involving a function and some of its derivatives.\nFor example, a simple model for the angular deflection \\(\\theta(t)\\) of a pendulum of length \\(l\\) is \\[l\\theta''(t) + g\\sin \\theta(t) = 0,\\] which arises from balancing kinetic and potential energies. This equation is nonlinear in the dependent variable \\(\\theta\\). For small angles, we can approximate \\(\\sin(\\theta)\\) by \\(\\theta\\) leading to the linear second order constant coefficient model \\[l\\theta''(t) + g\\theta(t) = 0.\\] Most systems are nonlinear. However most analyses begin with the study of linear approximations. Hence the importance of studying linear differential equations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#theoretical-considerations",
    "href": "M3503-lec03-SecondOrderODE.html#theoretical-considerations",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.7 Theoretical Considerations",
    "text": "4.7 Theoretical Considerations\nA good treatment of the existence and uniqueness of solutions to differential equations can be found in Hartman. This material is approachable, but requires a deeper exposure to linear algebra than you received in Math 1503.\nConsider the general Second order linear ODE of Equation 4.1 with \\(G=0\\). Let \\(w(x) = y'(x)\\) and write the single equation as\n\\[P(x)w'(x) + Q(x)w(x) + R(x)y(x) = 0,\\]\nwhich leads to the linear system \\[\\begin{align}\n  w'(x) &=  - \\frac{Q(x)}{P(x)}w(x) - \\frac{R(x)}{P(x)}y(x), \\\\\n  y'(x) &= w(x)\n\\end{align}\\] Hartman’s text deals with the more general equation \\(\\vec{y}' = f(x,\\vec{y})\\) where \\(\\vec{y}\\) and \\(f\\) are vector functions. The result for our simpler linear system is that the initial value problem has a unique solution provided \\(Q/P\\) and \\(R/P\\) are both continuous.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Second order Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "M3503-lec04-LaplaceTransforms.html",
    "href": "M3503-lec04-LaplaceTransforms.html",
    "title": "5  Laplace Transforms",
    "section": "",
    "text": "5.1 Definition and basic properties\nThese notes are far from complete. However, you may find the examples helpful. Your main resource for this material should be Chapter 1 of Tupper’s Notes.\nWe will see two integral transforms in this course: the Fourier Transform and the Laplace Transform. In general, an integral transform takes a function, \\(f\\), of one variable defined on an interval \\([a,b]\\), multiplies it by a kernel, \\(k\\), depending on two variables, and integrates the product over the domain \\([a,b]\\). The result is a function of the second variable appearing in the kernel: \\[F(s) = \\int_a^b k(t,s) f(t)\\, dt\\] The Laplace Transform takes a function \\(f\\) defined on \\((0,\\infty)\\) and transforms it using the kernel \\(k(t,s) = e^{-st}\\). \\[F(s) = \\int_0^\\infty e^{-st} f(t)\\, dt\\] We can and will think of the transform as a function whose inputs are functions of time, and whose outputs are functions of the second variable \\(s\\). We’ll use a fancy L to denote this function. Hence, the Laplace Transform of a function \\(f\\) is the function \\(F\\) defined by \\[F(s) = {\\mathcal L}\\left\\{f\\right\\}(s) = \\int_0^\\infty e^{-st}f(t)\\, dt \\tag{5.1}\\]\nIf we just want to refer to the transformed function we’ll use \\({\\mathcal L}\\left\\{f\\right\\}\\). Sometimes we’ll use the notation \\({\\mathcal L}\\left\\{f(t)\\right\\}\\), which is inconsistent with the convention that \\(f\\) refers to the function and \\(f(t)\\) refers to the function evaluated at a given time \\(t\\). However, the abuse is a convenient one, since it allows us to use \\(f(t-c)\\) to denote the time-shift of \\(f\\). Hence, depending on the context, \\(f(t-c)\\) might refer to both the shifted function or the value of the function.\nA special case of the previous example is \\({\\mathcal L}\\left\\{1\\right\\}(s) = \\frac{1}{s}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Laplace Transforms</span>"
    ]
  },
  {
    "objectID": "M3503-lec04-LaplaceTransforms.html#sec-laplace-def",
    "href": "M3503-lec04-LaplaceTransforms.html#sec-laplace-def",
    "title": "5  Laplace Transforms",
    "section": "",
    "text": "Example 5.1 Find the Laplace Transform of \\(e^{-at}\\) for any constant \\(a\\) \\[\\begin{align}\n  {\\mathcal L}\\left\\{e^{-at}\\right\\}(s)\n    &= \\int_0^\\infty e^{-st}e^{-at}\\, dt \\\\\n    &= \\int_0^\\infty e^{-(s+a)t}\\, dt \\\\\n    &= \\left. -\\frac{1}{s+a}e^{-(s+a)t} \\right|_{t=0}^\\infty \\\\\n    &= -\\frac{1}{s+a} \\quad s+a&gt;0\n\\end{align}\\] Note we treat \\(s\\) and \\(a\\) as constants for the integration step. Think of this as finding the Transform at a given value of \\(s\\). For this example, the integral diverges if \\(s+a\\le0\\), hence, the domain of the transformed function is \\((a,\\infty)\\).\n\n\n\nExample 5.2 Find the Laplace Transform of \\(t\\) \\[\\begin{align}\n  {\\mathcal L}\\left\\{t\\right\\}(s) &= \\int_0^\\infty te^{-st}\\, dt \\\\\n             &= \\left. t\\left(-\\frac{1}{s}e^{-st}\\right)\\right|_{t=0}^\\infty  - \\int_0^\\infty \\left(-\\frac{1}{s}e^{-st}\\right)\\, dt \\\\\n             &= 0  - \\left. \\frac{1}{s^2}e^{-st}\\right|_{t=0}^\\infty \\\\\n             &= \\frac{1}{s^2}, \\quad s&gt;0\n\\end{align}\\] Note we’ve used integration by parts with \\(u = t\\) and \\(dv = e^{-st}dt\\). And again note that the integral diverges if \\(s\\le0\\).\n\n\nExample 5.3 Find the Laplace Transform of \\(t^n\\) \\[\\begin{align}\n  {\\mathcal L}\\left\\{t^n\\right\\}(s) &= \\int_0^\\infty t^ne^{-st}\\, dt \\\\\n             &= \\left. t^n\\left(-\\frac{1}{s}e^{-st}\\right)\\right|_{t=0}^\\infty  - \\int_0^\\infty \\left(nt^{n-1}\\right)\\left(-\\frac{1}{s}e^{-st}\\right)\\, dt \\\\\n             &= 0  + \\frac{n}{s}\\int_0^\\infty t^{n-1}e^{-st}\\, dt , \\quad s &gt; 0\\\\\n             &= 0  + \\frac{n}{s}{\\mathcal L}\\left\\{t^{n-1}\\right\\}\n\\end{align}\\] Repeating this step another \\(n-1\\) times, we should arrive at \\[{\\mathcal L}\\left\\{t^n\\right\\} = \\frac{n!}{s^n}{\\mathcal L}\\left\\{1\\right\\} = \\frac{n!}{s^{n+1}}\\] Indeed, using this formula with \\(n=0\\) and \\(n=1\\) gives the same results as before. To complete the proof by induction, simply note that replacing \\(n\\) by \\(n-1\\) gives \\({\\mathcal L}\\left\\{t^{n-1}\\right\\} = \\frac{(n-1)!}{s^n}\\) and \\[\\frac{n}{s}{\\mathcal L}\\left\\{t^{n-1}\\right\\} =\\frac{n}{s} \\frac{(n-1)!}{s^n} =  \\frac{n!}{s^{n+1}}\\] as expected.\n\n\nExample 5.4 The transforms of sine and cosine can be viewed as special cases of our first example. Simply set \\(a\\) to \\(i\\) and note that \\[\\begin{gather}\n  e^{-it} = \\cos t) - i\\sin t  \\\\\n  {\\mathcal L}\\left\\{e^{-it}\\right\\} = {\\mathcal L}\\left\\{\\cos t  - i\\sin t \\right\\} \\\\\n  {\\mathcal L}\\left\\{e^{-it}\\right\\} = {\\mathcal L}\\left\\{\\cos t \\right\\} - i{\\mathcal L}\\left\\{\\sin t \\right\\} \\\\\n  \\frac{1}{s+i} = {\\mathcal L}\\left\\{\\cos t \\right\\} - i{\\mathcal L}\\left\\{\\sin t \\right\\} \\\\\n  \\frac{s-i}{(s+i)(s-i)} = {\\mathcal L}\\left\\{\\cos t \\right\\} - i{\\mathcal L}\\left\\{\\sin t \\right\\} \\\\\n  \\frac{s-i}{s^2-1} = {\\mathcal L}\\left\\{\\cos t \\right\\} - i{\\mathcal L}\\left\\{\\sin t \\right\\} \\\\\n  \\frac{s}{s^2-1} - i\\frac{1}{s^2-1} = {\\mathcal L}\\left\\{\\cos t \\right\\} - i{\\mathcal L}\\left\\{\\sin t \\right\\}\n\\end{gather}\\] Matching the real and imaginary parts of each side implies \\[\\begin{align}\n  {\\mathcal L}\\left\\{\\cos t \\right\\} &= \\frac{s}{s^2+1} \\\\\n  {\\mathcal L}\\left\\{\\sin t \\right\\} &= \\frac{1}{s^2+1}\n\\end{align}\\]\n\n\nTheorem 5.1 (First Shifting Theorem) If \\({\\mathcal L}\\left\\{f(t)\\right\\} = F(s)\\) for \\(s&gt;c\\), then \\({\\mathcal L}\\left\\{e^{at}f(t)\\right\\} = F(s-a)\\) for \\(s&gt;c+a\\).\n\n\nProof. The result follows immediately from \\[{\\mathcal L}\\left\\{e^{at}f(t)\\right\\} = \\int_0^\\infty e^{at}f(t) e^{-st} \\, ds = \\int_0^\\infty f(t) e^{-(s-a)t} \\, ds\\]\n\n\nExercise 5.1 Use partial fractions (if necessary) and the First Shifting Theorem (if necessary) to find the inverse Laplace transform of \\[\\displaystyle F(s) = \\frac{s+7}{s^2 + 4s + 13}.\\]\n\n\n\\[\\begin{align*}\n  F(s) &= \\frac{s+7}{s^2 + 4s + 13}, \\\\\n             &= \\frac{s+7}{(s+2)^2 + 3^2}. \\\\[5pt]\n  f(t) &= {\\mathcal L}^{-1}\\left\\{\\frac{s+7}{(s+2)^2 + 3^2}\\right\\}, \\\\\n       &= e^{-2t}{\\mathcal L}^{-1}\\left\\{\\frac{s+5}{s^2 + 3^2}\\right\\},  \\qquad \\text{ Shifting $(s+2)\\mapsto s$}\\\\\n       &= e^{-2t}\\left(\\cos{3t} + \\frac{5}{3}\\sin{3t}\\right). \\\\\n\\end{align*}\\]\n\n\nExercise 5.2 Use partial fractions (if necessary) and the First Shifting Theorem (if necessary) to find the inverse Laplace transform of \\[\\displaystyle F(s) = \\frac{s+7}{s^2 + 2s + 5}.\\]\n\n\n\\[\\begin{align*}\n  F(s) &= \\frac{s+7}{s^2 + 2s + 5}, \\\\\n             &= \\frac{s+7}{(s+1)^2 + 2^2}. \\\\[5pt]\n  f(t) &= {\\mathcal L}^{-1}\\left\\{\\frac{s+7}{(s+1)^2 + 2^2}\\right\\}, \\\\\n         &= {\\mathcal L}^{-1}\\left\\{\\frac{(s+1)+6}{(s+1)^2 + 2^2}\\right\\}, \\\\\n       &= e^{-t}{\\mathcal L}^{-1}\\left\\{\\frac{s+6}{s^2 + 2^2}\\right\\},  \\qquad \\text{ Shifting $(s+1)\\mapsto s$}\\\\\n       &= e^{-t}\\left(\\cos{2t} + 3\\sin{2t}\\right). \\\\\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Laplace Transforms</span>"
    ]
  },
  {
    "objectID": "M3503-lec04-LaplaceTransforms.html#sec-laplace-ode",
    "href": "M3503-lec04-LaplaceTransforms.html#sec-laplace-ode",
    "title": "5  Laplace Transforms",
    "section": "5.2 Laplace Transforms and Differential Equations",
    "text": "5.2 Laplace Transforms and Differential Equations\nOur interest in Laplace Transforms for this course is that they are (1) linear and (2) transform linear ordinary differential equations into algebraic equations.\n\nExercise 5.3 Suppose \\({\\mathcal L}\\left\\{f\\right\\} = F\\) and \\({\\mathcal L}\\left\\{g\\right\\} = G\\). Show that \\[{\\mathcal L}\\left\\{af + bg\\right\\} = a{\\mathcal L}\\left\\{f\\right\\}+b{\\mathcal L}\\left\\{g\\right\\}\\] for any constants \\(a\\) and \\(b\\).\n\n\nExercise 5.4 Show that \\({\\mathcal L}\\left\\{y'\\right\\} = s{\\mathcal L}\\left\\{y\\right\\} - y(0)\\).\n\n\nExercise 5.5 Show that \\({\\mathcal L}\\left\\{y''\\right\\} = s^2{\\mathcal L}\\left\\{y\\right\\} - sy(0) - y'(0)\\).\n\nConsider our generic second order constant coefficient ordinary differential equation: \\[\\begin{equation}\n     ay'' + by' + cy = g,\n  \\end{equation}\\] with \\(y(0)\\) and \\(y'(0)\\) specified. Taking the Laplace transform of both sides of the equation leads to \\[a{\\mathcal L}\\left\\{y''\\right\\} + b{\\mathcal L}\\left\\{y'\\right\\} + c{\\mathcal L}\\left\\{y\\right\\} = {\\mathcal L}\\left\\{g\\right\\}.\\] Using \\(Y\\) for \\({\\mathcal L}\\left\\{y\\right\\}\\) and \\(G\\) for \\({\\mathcal L}\\left\\{g\\right\\}\\) and our previous result for the Laplace transform of derivatives results in the equation \\[a\\left( s^2Y(s) - sy(0) - y'(0) \\right) \\ +\\ b\\left(sY(s) - y(0) \\right) \\ +\\ cY(s) = G(s).\\] Thus, the Laplace transform transforms the differential equation for \\(y\\) to an algebraic equation for \\(Y\\). Solving for \\(Y\\), we find \\[\\left( as^2 + bs + c \\right) Y(s) = G(s) + (as+b)y(0) + ay'(0),\\] or \\[Y(s) = H(s)G(s) + H(s) I(s),\\] where \\[H(s) = \\frac{1}{as^2 + bs + c} \\qquad \\text{and} \\qquad I(s) = (as+b)y(0) + ay'(0).\\] Finally, taking the inverse transforms gives the solution to the original problem: \\[y(t) = {\\mathcal L}^{-1}\\left\\{H(s)G(s)\\right\\} \\ +\\ {\\mathcal L}^{-1}\\left\\{H(s)I(s)\\right\\}. \\tag{5.2}\\]\nEquation 5.2 hides the details of the computations of the transforms and nicely shows the structure of the solution. The first part of the solution is the response to the forcing, and the second is the echo of the initial conditions.\n\nExample 5.5 Solve the IVP \\(y''(t) - 4y(t) = 8t^2 - 4\\), with \\(y(0) = 5\\) and \\(y'(0) = 10\\).\nTaking the Laplace transforms of both sides of the equations we find \\[{\\mathcal L}\\left\\{y''\\right\\} - 4{\\mathcal L}\\left\\{y\\right\\} = 8{\\mathcal L}\\left\\{t^2\\right\\} - {\\mathcal L}\\left\\{4\\right\\}.\\]\n\\[\\left(s^2Y(s) - 5s - 10\\right) \\ -\\  4Y(s) = \\frac{16}{s^3} - \\frac{4}{s},\\] which when solved for \\(Y\\) gives \\[\\begin{align*}\n   (s^2 -  4 ) Y(s) &= \\frac{16}{s^3} - \\frac{4}{s} + 5s + 10\\\\\n   Y(s) &= \\frac{16}{s^3(s^2 -  4 )} - \\frac{4}{s(s^2 -  4 )} + \\frac{5s + 10}{s^2 -  4 }\\\\\n        &= \\frac{16 - 4s^2}{s^3(s^2 -  4 )}  \\ +\\  \\frac{5s + 10}{s^2 -  4 }\\\\\n        &= -\\frac{4}{s^3}  \\ +\\  \\frac{5}{s -  2 },\n\\end{align*}\\] where the last step results from cancelling common factors in the numerators and denominators. Inverting leads to \\[y(t) = -{\\mathcal L}^{-1}\\left\\{\\frac{4}{s^3}\\right\\}  \\ +\\  5{\\mathcal L}^{-1}\\left\\{\\frac{1}{s -  2 }\\right\\}  = -2t^2 \\ +\\  5e^{2t}.\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Laplace Transforms</span>"
    ]
  },
  {
    "objectID": "M3503-lec04-LaplaceTransforms.html#sec-laplace-step",
    "href": "M3503-lec04-LaplaceTransforms.html#sec-laplace-step",
    "title": "5  Laplace Transforms",
    "section": "5.3 Step functions",
    "text": "5.3 Step functions\nWe often want to include sudden changes in an applied force into our differential equation models of systems. For example, an electric circuit where a voltage is repeatedly switched on and off. A convenient building block for such functions in the unit step function, \\(u_a\\), defined by \\[u_a(t) = \\begin{cases} 0 & t &lt; a \\\\ 1 & t\\ge a \\end{cases}\\] To model an applied force, \\(f\\), turned-on at a time \\(a&gt;0\\) we can use \\(f(t-a)u_a(t)\\). Notice we’ve shifted \\(f\\) to the right.\nIf \\(F(s) = {\\mathcal L}\\left\\{f(t)\\right\\}\\), then \\[\\begin{align}\n    {\\mathcal L}\\left\\{f(t-a)u_a(t)\\right\\}\n    &= \\int_0^\\infty e^{-st}f(t-a)u_a(t)\\, dt \\\\\n    &= \\int_a^\\infty e^{-st}f(t-a)\\, dt \\\\\n    &= \\int_0^\\infty e^{-s(t+a)}f(t)\\, dt \\\\\n    &= e^{-sa}\\int_0^\\infty e^{-st}f(t)\\, dt \\\\\n    &= e^{-sa}F(s)\n\\end{align}\\]\n\nExercise 5.6 Using the table of Laplace transforms and the Second Shifting Theorem, find the Laplace transform of \\[g(t) = (t-3)^3 u_3(t)\\]\n\n\n\\[\\begin{align*}\n  {\\mathcal L}\\left\\{(t-3)^3 u_3(t)\\right\\}\n  &= e^{-3s}{\\mathcal L}\\left\\{t^3 \\right\\} \\qquad \\text{ shifting $(t-3)\\mapsto t$,}\\\\\n  &= \\frac{3!}{s^4}e^{-3s}\\\\\n  &= \\frac{6e^{-3s}}{s^4}\\\\\n\\end{align*}\\]\n\n\nExercise 5.7 Express the function \\(f\\) defined below using step functions and find its Laplace transform using the Second Shifting Theorem. \\[f(t) =\n   \\begin{cases}\n     t, & 0 \\le t &lt; 4, \\\\\n     0, & 4 \\le t &lt; 5, \\\\\n     1, & t \\ge 5.\n   \\end{cases}\\]\n\n\nFirst, express \\(f\\) using step functions, then use the second shifting theorem. \\[\\begin{align*}\n  f(t) &= t(1 - u_4(t)) + u_5(t) \\\\\n  f(t) &= t - t u_4(t) + u_5(t) \\\\\n  F(s) &= \\frac{1}{s^2} - \\frac{1}{s^2}e^{-4s} + \\frac{1}{s}e^{-5s}\n\\end{align*}\\]\n\n\nExercise 5.8 Use Laplace transforms to solve the initial value problem \\[y'' + 5y' + 6y = f(t), \\quad  y(0)=0, \\quad y'(0)=0,\\] where \\[f(t) =\\begin{cases} 0, & 0 \\le t &lt; 1, \\\\ 1, & t \\ge 1 \\end{cases}\\]\n\n\n\\[\\begin{align*}\n   y'' + 5y' + 6y &= f(t)\\\\\n   y'' + 5y' + 6y &= u_1(t)\\\\\n   {\\mathcal L}\\left\\{y''\\right\\} + 5{\\mathcal L}\\left\\{y'\\right\\} + 6{\\mathcal L}\\left\\{y\\right\\} &= {\\mathcal L}\\left\\{u_1(t)\\right\\}\\\\\n   (s^2Y(s) - sy(0) - y'(0)) + 5(sY(s) - y(0)) + 6Y(s) &= \\frac{e^{-s}}{s} \\\\\n     (s^2+5s+6)Y(s) &= (s + 5)y(0) + \\frac{e^{-s}}{s} \\\\\n   (s^2+5s+6)Y(s) &=  s+5 + \\frac{e^{-s}}{s} \\\\\n   Y(s) &=  \\frac{s+5}{(s^2+5s+6)} + \\frac{e^{-s}}{s(s^2+5s+6)} \\\\\n   Y(s) &=  \\frac{s+5}{(s+3)(s+2)} + \\frac{e^{-s}}{s(s+3)(s+2)} \\\\\n   Y(s) &=  -\\frac{2}{s+3} + \\frac{3}{s+2} + \\frac{e^{-s}}{6s} + \\frac{e^{-s}}{3(s+3)} - \\frac{e^{-s}}{2(s+2)} \\\\\n   y(t) &=  -2e^{-3t} + 3e^{-2} + \\tfrac{1}{6} u_1(t) + \\frac{1}{3}u_1(t)e^{-(t-1)} - \\frac{1}{2}u_1(t)e^{-2(t-1)}\n\\end{align*}\\]\n\n\nExercise 5.9 Use Laplace transforms to solve the initial value problem \\[y'' + 2y' + 6y = f(t), \\quad  y(0)=0, \\quad y'(0)=0,\\] where \\[f(t) =\\begin{cases} 2t, & 0 \\le t &lt; 1, \\\\ 0, & t \\ge 1 \\end{cases}\\]\n\n\nStep 1: express \\(f\\) using step functions and find \\(F(s) = {\\mathcal L}\\left\\{f(t)\\right\\}\\): \\[\\begin{align*}\n    f(t) &= 2t(1-u_1(t)) \\\\\n    F(s) &= {\\mathcal L}\\left\\{2t(1-u_1(t))\\right\\} \\\\\n             &= 2{\\mathcal L}\\left\\{t\\right\\} - 2{\\mathcal L}\\left\\{t u_1(t))\\right\\}\\\\\n       &= 2{\\mathcal L}\\left\\{t\\right\\} - 2{\\mathcal L}\\left\\{((t-1) + 1)u_1(t)\\right\\}\\\\\n       &= \\frac{2}{s^2} - 2e^{-s}{\\mathcal L}\\left\\{t + 1\\right\\}\\\\\n       &= \\frac{2}{s^2} - 2e^{-s}\\big(\\frac{1+s}{s^2} \\big)\n\\end{align*}\\] Step 2: transform the IVP and solve for \\(Y(s) = {\\mathcal L}\\left\\{y(t)\\right\\}\\) \\[\\begin{gather*}\n  y'' + 2y' + 6y = f(t) \\\\\n    {\\mathcal L}\\left\\{y''\\right\\} + 2{\\mathcal L}\\left\\{y'\\right\\} + 6{\\mathcal L}\\left\\{y\\right\\} = F(s)\\\\\n(s^2Y(s) - sy'(0) - y(0)) + 2(sY(s) - y(0)) + 6Y(s) = F(s)\\\\\n(s^2+2s+6)Y(s) = F(s)\\\\\n    Y(s) = G(s) - e^{-s}H(s)\n\\end{gather*}\\] with \\(G(s) = \\dfrac{2}{s^2(s^2+2s+6)}\\) and \\(H(s) = \\dfrac{2+2s}{s^2(s^2+2s+6)}\\)\nStep 3: perform a partial fraction expansions on \\(G\\) and \\(H\\). \\[\\begin{gather*}\n   \\frac{2}{s^2(s^2+2s+6)} = \\frac{A}{s} + \\frac{B}{s^2} + \\frac{C + Ds}{s^2+2s+6} \\\\\n   2 = As(s^2+2s+6) + B(s^2+2s+6) + Cs^2 + Ds^3 \\\\\n   2 = (A+D)s^3 + (2A+B+C)s^2 + (6A+2B)s + (6B)  \n\\end{gather*}\\] Matching the coefficients of each power of \\(s\\) leads to \\(B = 1/3\\), \\(A = -1/9\\), \\(C = -1/9\\), \\(D = 1/9\\). Hence \\[G(s) = \\frac{1}{9}\\left(-\\frac{1}{s} + \\frac{3}{s^2} + \\frac{-1 + s}{s^2+2s+6} \\right) \\] Similarly, for \\(H\\) we obtain \\[2+2s = (A+D)s^3 + (2A+B+C)s^2 + (6A+2B)s + (6B)\\] Matching the coefficients of each power of \\(s\\) leads to \\(B = 1/3\\), \\(A = 2/9\\), \\(C = -7/9\\), \\(D = -2/9\\). \\[H(s) = \\frac{1}{9}\\left(\\frac{2}{s} + \\frac{3}{s^2} + \\frac{-7 -2s}{s^2+2s+6} \\right) \\] Step 4: invert \\(G\\) and \\(H\\)  The tricky term is \\(\\dfrac{-1 + s}{s^2+2s+6}\\).  To invert this, first note the denominator is \\((s+1)^2 + 4\\).  So we write \\(\\dfrac{-1 + s}{s^2+2s+6} = \\dfrac{-2 + (s+1)}{(s+1)^2+5}\\),  which has an inverse Laplace transform of \\(-\\frac{2}{\\sqrt{5}}e^{-t}\\sin(\\sqrt{5}t) + e^{-t}\\cos(\\sqrt{5}t)\\) (lines 10 and 11 in Tupper’s table).  Hence, \\[g(t) = {\\mathcal L}^{-1}\\left\\{G(s)\\right\\} = \\frac{1}{9}\\left(-1 + 3t -e^{-t}\\sin(\\sqrt{5}t) + e^{-t}\\cos(\\sqrt{5}t)\\right)\\] \\[h(t) = {\\mathcal L}^{-1}\\left\\{H(s)\\right\\} = \\frac{1}{9}\\left(2 + 3t -\\sqrt{5}e^{-t}\\sin(\\sqrt{5}t) - 2e^{-t}\\cos(\\sqrt{5}t)\\right)\\] Step 5: combine \\(g\\) and \\(h\\) to make \\(y\\) \\[y(t) = {\\mathcal L}^{-1}\\left\\{G(s)\\right\\} - {\\mathcal L}^{-1}\\left\\{e^{-s}H(s)\\right\\} = g(t) - u_1(t)h(t-1)\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Laplace Transforms</span>"
    ]
  },
  {
    "objectID": "M3503-lec04-LaplaceTransforms.html#sec-laplace-delta",
    "href": "M3503-lec04-LaplaceTransforms.html#sec-laplace-delta",
    "title": "5  Laplace Transforms",
    "section": "5.4 Impulsive forcing",
    "text": "5.4 Impulsive forcing\nWe are often interested in forcing the system with an impulse, a large force applied over a short time period. Define a unit box function, \\[\\delta_a(t) = \\begin{cases}\\frac{1}{2a}, & -a &lt; t &lt; a, \\\\ 0, & \\text{otherwise}. \\end{cases}\\] Note that \\(\\int_{0}^\\infty \\delta_a(t-t_o)  \\, dt = 1\\) provided \\(t_o &gt; a\\). Since we are interested in impulses over very short times (small \\(a\\)), it is natural to think of a limit as \\(a\\rightarrow0\\).\nThe limit of such a sequence of unit boxes is called the Dirac delta function. Conceptually, at least for this course, we can view it as being defined as \\[\\delta(t-t_o) = \\lim_{a\\rightarrow0} \\delta_a(t-t_0).\\] However, this limit is not a function in the sense we are used to and it is certainly not Riemann integrable. The mathematical machinery needed to make sense of this limit is outside the scope of this course. Nevertheless, the concept is interesting and turns out to be useful in practice.\nNote that \\[\\lim_{\\rightarrow0^+} \\int_{0}^\\infty \\delta_a(t-t_o)  \\, dt = 1\\] since the integral is one for every positive \\(a\\). Moreover, for any continuous function \\(f\\) defined on \\([0,\\infty)\\) \\[\\begin{align}\n  \\lim_{a\\rightarrow0^+} \\int_{0}^\\infty f(t) \\delta_a(t-t_o)  \\, dt  \n&= \\lim_{a\\rightarrow0^+} \\frac{1}{2a}\\int_{t_o-a}^{t_o+a} f(t)  \\, dt  \n&= f(t_o)\n\\end{align}\\] The unit impulses behave nicely in the limit as long as they stay inside integrals! In fact, another way to define the delta-function is by this property: \\(\\int_0^\\infty \\delta(t-t_o) f(t) \\, dt = f(t_o)\\) for every continuous function \\(f\\) and time \\(t_o \\ge 0\\).\nIn particular, \\[{\\mathcal L}\\left\\{\\delta(t-t_o)\\right\\} = \\int_0^\\infty e^{-st} \\delta(t-t_o)\\, dt = e^{-st_o}.\\]\nTheoretically, we’ve wandered off the edge of the map. \\(\\delta\\) is not a function in any sense we are familiar with, it’s not Riemann ingegrable, and all the previous lines are nothing more than wishful symbol manipulation. Despite this, the symbol manipulation usually turns out nicely and proves useful.\n\nExample 5.6 Solve the initial value problem \\[y'' + 9y = \\delta(t-\\frac{\\pi}{2}), \\qquad y(0) = 0, \\ y'(0) = 0.\\] Note this is the classic model for a simple pendulum starting at rest at time zero, and given a unit of force concentrated at time \\(\\pi/2\\). Think of tapping the pendulum with a hammer.\nTaking the Laplace transform of both sides yields\n\\[(s^2 + 9) Y(s) = e^{-\\pi s/2}\\] \\[Y(s) = \\frac{e^{-\\pi s/2}}{s^2 + 9}\\] and the inverse transform gives the solution \\[\\begin{align*}\n   y(t) &= \\frac{1}{3}\\sin\\left(3(t-\\pi/2)\\right) u_{\\pi/2}(t) \\\\\n        &= \\begin{cases}\n             0 & 0 \\le t &lt; \\pi/2 \\\\\n             \\frac{1}{3}\\sin\\left(3(t-\\pi/2)\\right), & t \\ge \\pi/2.\n        \\end{cases}\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Laplace Transforms</span>"
    ]
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html",
    "href": "M3503-lec05-FourierSeries.html",
    "title": "6  Fourier Series",
    "section": "",
    "text": "6.1 Vectors",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fourier Series</span>"
    ]
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#inner-products",
    "href": "M3503-lec05-FourierSeries.html#inner-products",
    "title": "6  Fourier Series",
    "section": "6.2 Inner Products",
    "text": "6.2 Inner Products\nAn inner product is an abstraction of the of a scalar projection of one vector onto another. In Euclidian spaces it most often takes the form of a dot product. An inner product allows us to break vectors into orthogonal components and base approximations on projections. This in turn implies that there will be a best approximation. We’ll use this to approximate complex functions by sums of simpler functions, like polynomials, sines, and cosines. Approximating a function, or signal, by a sum of sines and cosines effectively breaks down the function into its component frequencies.\n\nDefinition 6.1 An inner product on a vector space \\(V\\) is a function that assigns a scalar \\(\\langle x,y\\rangle\\) to each pair, \\((x,y)\\), of vectors in \\(V\\) and satisfies the following conditions:\n\n\\(\\langle y,x\\rangle = \\overline{\\langle x,y\\rangle}\\) (conjugate symmetric);\n\\(\\langle x,x\\rangle \\ge 0\\), with equality if and only if \\(x=0\\) (positive definite);\n\\(\\langle a w + b x,y\\rangle = a\\langle x,y\\rangle + b\\langle x,y\\rangle\\) for all \\(w\\), \\(x\\) and \\(y\\) in \\(V\\) and scalars \\(a\\) and \\(b\\) (bilinear).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a real vector space, the first axiom reduces to a straightforward symmetry: \\(\\langle x,y\\rangle = \\langle y,x\\rangle\\).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe notation for inner products varies depending on the application. That’s ok, so does the notation for multiplication.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fourier Series</span>"
    ]
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#the-usual-fourier-series-for-f",
    "href": "M3503-lec05-FourierSeries.html#the-usual-fourier-series-for-f",
    "title": "6  Fourier Series",
    "section": "6.3 The usual Fourier Series for \\(f\\)",
    "text": "6.3 The usual Fourier Series for \\(f\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fourier Series</span>"
    ]
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#a-more-general-view-of-fourier-series",
    "href": "M3503-lec05-FourierSeries.html#a-more-general-view-of-fourier-series",
    "title": "6  Fourier Series",
    "section": "6.4 A more general view of Fourier Series",
    "text": "6.4 A more general view of Fourier Series\nSuppose \\(f\\) is a piecewise continuous function defined on an interval \\([-l,l]\\).\nFurther, suppose the set of function \\(\\{\\phi_0,\\phi_1,\\phi_2,\\dots\\}\\) is orthogonal with respect to some given inner product. Then the fourier series for \\(f\\) with respect to this set is \\(\\displaystyle\\sum_{m=0}^\\infty c_m \\phi_m(x)\\) with \\(c_m = \\dfrac{\\langle f,\\phi_m\\rangle}{\\langle\\phi_m,\\phi_m\\rangle}\\).\nThe usual fourier series for \\(f\\) is obtained when \\(\\phi_n(x) = \\begin{cases}    1 & m=0, \\\\    \\cos(\\frac{m\\pi x}{2l}) & \\text{$m$ even and positive} \\\\ \\sin(\\frac{(m+1)\\pi x}{2l}) & \\text{$m$ odd}  \\end{cases}\\) and the inner product of two functions, \\(u\\) and \\(v\\), is defined as \\(\\langle u,v\\rangle = \\int_{-l}^{l} u(x) v(x) \\, dx\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fourier Series</span>"
    ]
  },
  {
    "objectID": "ode-linear.html",
    "href": "ode-linear.html",
    "title": "7  Linear Systems of Differential Equations",
    "section": "",
    "text": "7.1 Matrix Exponentials\nConsider differential equations of the form \\(\\dot{x} = Ax\\) where \\(A\\) is an \\(n\\times n\\) matrix of real numbers. For convenience, we’ll use \\({\\mathbb R}^{n\\times n}\\) to denote the set of all such matrices.\nIntegrating the equation we have \\(x(t) = x_0 + \\int_0^t Ax(s)\\, dx\\), which we can view as a map on the set \\({\\mathbf C}^1(0,t)\\) of differentiable functions on \\((0,t)\\). If we iterate this map starting with the constant function \\(x_0\\), we find \\[\\begin{aligned}\nx_1 &= x_0 + Ax_0t \\\\\nx_2 &= x_0 + Ax_0t + \\frac{1}{2}A^2x_0t^2 \\\\\nx_3 &= x_0 + Ax_0t + \\frac{1}{2}A^2x_0t^2 + \\frac{1}{6}A^2x_0t^3\n\\end{aligned}\\]\nIf \\(A\\) is a scalar, or a \\(1\\times 1\\) matrix, this iteration generates the familiar series for the exponential. In particular, \\(x_n\\) converges to \\(e^{At}x_0\\).\nWe’ll anticipate that the series also converges in a meaningful way if \\(A\\) is a matrix, and denote the limit by the matrix exponential, \\(e^{At}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Systems of Differential Equations</span>"
    ]
  },
  {
    "objectID": "ode-linear.html#matrix-exponentials",
    "href": "ode-linear.html#matrix-exponentials",
    "title": "7  Linear Systems of Differential Equations",
    "section": "",
    "text": "Exercise 7.1 Show that \\(x_n\\) converges if \\(A\\) is an \\(n\\times n\\) matrix. See the section on the matrix exponential in Hirsch, Smale, and Devaney (2013) for a hint.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Systems of Differential Equations</span>"
    ]
  },
  {
    "objectID": "ode-linear.html#eigenvalues-eigenvectors-and-eigenspaces",
    "href": "ode-linear.html#eigenvalues-eigenvectors-and-eigenspaces",
    "title": "7  Linear Systems of Differential Equations",
    "section": "7.2 Eigenvalues, Eigenvectors, and Eigenspaces",
    "text": "7.2 Eigenvalues, Eigenvectors, and Eigenspaces",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Systems of Differential Equations</span>"
    ]
  },
  {
    "objectID": "ode-linear.html#phase-portraits",
    "href": "ode-linear.html#phase-portraits",
    "title": "7  Linear Systems of Differential Equations",
    "section": "7.3 Phase Portraits",
    "text": "7.3 Phase Portraits\n\nExample 7.1 Construct a phase portrait for the system \\[\\begin{aligned}\n\\dot{x} &= 3x + 4y \\\\\n\\dot{y} &= 3x -  y\n\\end{aligned}\\]\n\nThe eigenvalues are 5 and -3, and the corresponding eigenspaces are the spans of \\(\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 2 \\\\ -3 \\end{pmatrix}\\) respectively.\nThe solutions corresponding to these eigenvalue/eigenvector pairs trace out the eigenspaces in the \\(x\\)-\\(y\\) plane.\n\n\nShow the code\nplot(NULL,xlim=c(-2,2),ylim=c(-2,2),xlab = \"x(t)\", ylab = \"y(t)\")\nabline(0,1/2,col='red')\nabline(0,-3/2,col='green')\n\n\n\n\n\n\n\n\n\nShow the code\np = recordPlot()\n\n\nThe general solutions to the system are linear combinations of the two solutions corresponding to the eigenvalues. Since the eigenvalues have opposite signs, these trace out hyperbolae. To see this, let \\(a(t)\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + b(t) \\begin{pmatrix} 2 \\\\ -3 \\end{pmatrix}\\) with \\(a(t) = a_0e^{5t}\\) and \\(b(t) = b_0e^{-3t}\\) denote the general solution. This is a curve, parameterized by \\(t\\) for \\(-\\infty &lt; t &lt; \\infty\\). The coefficients, \\(a(t)\\) and \\(b(t)\\) satisfy \\(a^3b^5 = a_0^3b_0^5\\). These are hyperbolae in the \\(a\\)-\\(b\\)-plane which are hyperbolae in the \\(x\\)-\\(y\\) plane asymptotic to the two eigenspaces.\n\n\nShow the code\na_0 = 1/5\nb_0 = 1/5\na = a_0*2**seq(-5,5,length=21)\nb = b_0/(a/a_0)**(3/5)\nx = 2*a+2*b\ny = a - 3*b\nreplayPlot(p)\nlines(x,y,col='blue')\nlines(-x,-y,col='blue')\nx = -2*a+2*b\ny = -a - 3*b\nlines(x,y,col='blue')\nlines(-x,-y,col='blue')\n\n\n\n\n\n\n\n\n\nShow the code\n# abline(0,-3/4)\n# abline(0,3)\n# x = seq(-2,2,length=20)   # be sure to skip over zero (why?)\n# segments(x,-3*x/4-.05,x,-3*x/4+.05)\n# segments(x-.05,3*x,x+.05,3*x)\n\n\n```\nThere are two sets of subspaces (i.e., two pairs of lines) that we can use to construct the diagram.\nThe \\(x\\) and \\(y\\) nullclines of the systems are the sets where \\(\\dot{x}=0\\) and \\(\\dot{y}=0\\), respectively. For this example, those are the lines \\(3x+4y=0\\) and \\(3x-y=0\\). Solutions cross these lines vertically (\\(\\dot{x} = 0\\)) and horizontally (\\(\\dot{y}=0\\)), respectively.\n\nExample 7.2  \n\n\nExample 7.3  \n\n\n\n\n\nHirsch, Morris W., Stephen Smale, and Robert L. Devaney. 2013. Differential Equations, Dynamical Systems, and an Introduction to Chaos. Academic press.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Systems of Differential Equations</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hirsch, Morris W., Stephen Smale, and Robert L. Devaney. 2013.\nDifferential Equations, Dynamical Systems, and an Introduction to\nChaos. Academic press.",
    "crumbs": [
      "References"
    ]
  }
]
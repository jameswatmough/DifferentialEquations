[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 3503: Differential Equations for Engineers",
    "section": "",
    "text": "Class Schedule\n\n\n2023 Winter Class Schedule\n\n\n\nDate\nSection\nStewart\nKalbaugh\nTupper\nTopic\n\n\n\n\nMon Jan 09\nChapter 1\n9\n1\n—\nOverview and ODE review\n\n\nTue Jan 10\nChapter 2\n9\n3.2\n—\nSeparable ODE\n\n\nWed Jan 11\nChapter 2\n9\n3.1\n—\nFirst order ODE\n\n\nFri Jan 13\nChapter 1\n—\n4\n—\nGeneral theory, numerical solutions\n\n\nMon Jan 16\nSection 4.1\n17.1\n5\n—\nSecond order constant coefficient ODEs\n\n\nTue Jan 17\nChapter 2\n9\n3.1\n—\nTutorial: first order ODE\n\n\nWed Jan 18\nSection 4.2\n17.1\n5.1\n—\nDistinct real eigenvalues\n\n\nFri Jan 20\nSection 4.2\n17.1\n5.1\n—\nComplex eigenvalues\n\n\nMon Jan 23\nSection 4.2\n17.1\n3.1\n—\nRepeated eigenvalues\n\n\nTue Jan 24\nSection 4.2\n—\n—\n—\nTutorial:\n\n\nWed Jan 25\nSection 5.1\n17.2\n5.3\n—\nVariation of Parameters\n\n\nFri Jan 27\n?sec-undetcoef\n17.2\n5.3\n—\nUndetermined Coefficients\n\n\nMon Jan 30\nChapter 4\n17.3\n5\n—\nInitial Value Problems\n\n\nTue Jan 31\nChapter 4\n—\n5.3\n—\nTutorial:\n\n\nWed Feb 01\nChapter 4\n—\n5\n—\nBoundary Value Problems\n\n\nFri Feb 03\nTest 1\n\n\n\n\n\n\nMon Feb 06\nChapter 5\n—\n—\n1.1-2\nLaplace Transforms: Definitions and basics\n\n\nTue Feb 07\nChapter 5\n—\n—\n—\nTutorial:\n\n\nWed Feb 08\nChapter 5\n—\n—\n1.7\nLaplace Transform: derivatives and ODEs\n\n\nFri Feb 10\nChapter 5\n—\n—\n1.4-5\nLaplace Transform: common transforms\n\n\nMon Feb 13\n?sec-laplace-ivp\n—\n—\n1.8\nLaplace Transform: initial value problems\n\n\nTue Feb 14\n?sec-laplace-ivp\n—\n—\n1.9\nTutorial: Solving IVPs via Laplace Transforms\n\n\nWed Feb 15\n?sec-laplace-step\n—\n—\n1.6\nLaplace Transform: step functions\n\n\nFri Feb 17\n?sec-laplace-delta\n—\n—\n1.10\nLaplace Transform: discontinuous forcing\n\n\nMon Feb 20\nno class\n—\n—\n—\nFamily Day\n\n\nTue Feb 20\n?sec-laplace-ivp\n—\n—\n—\nTutorial: Solving IVPs via Laplace Transforms\n\n\nWed Feb 22\n?sec-laplace-ivp\n—\n—\n—\nTutorial: Solving IVPs via Laplace Transforms\n\n\nFri Feb 24\nTest 2\n\n\n\n\n\n\nMon Feb 27\nChapter 3\n—\n—\n2.1-2\nSystems of IVPs: Theory and geometry\n\n\nTue Feb 28\nChapter 3\n—\n—\n2.3\nTutorial: Review of Eigenvalues and Eigenvectors\n\n\nWed Mar 01\nChapter 3\n—\n—\n2.4\nSystems: distinct real eigenvalues\n\n\nFri Mar 03\nChapter 3\n—\n—\n2.5\nSystems: complex roots\n\n\nMon Mar 06\nno class\n—\n—\n—\nReading Week\n\n\nWed Mar 08\nno class\n—\n—\n—\nReading Week\n\n\nFri Mar 10\nno class\n—\n—\n—\nReading Week\n\n\nMon Mar 13\nChapter 3\n—\n—\n2.6\nSystems: repeated roots\n\n\nTue Mar 14\nChapter 3\n—\n—\n—\nTutorial:\n\n\nWed Mar 15\nChapter 3\n—\n—\n2.7\nMatrix exponentials and fundamental matrices\n\n\nFri Mar 17\nChapter 3\n—\n—\n2.7\nNonhomogeneous problems\n\n\nMon Mar 20\nChapter 6\n—\n—\n3.1\nFourier Series South Africa Online via Teams\n\n\nTue Mar 21\nChapter 6\n—\n—\n—\nTutorial - asynchronous\n\n\nWed Mar 22\nChapter 6\n—\n—\n3.2\nFourier Series South Africa Online via Teams\n\n\nFri Mar 24\nTest 3\n\n\n\n\n\n\nMon Mar 27\nChapter 6\n—\n—\n3.3\nFourier Series South Africa Online via Teams\n\n\nTue Mar 28\nChapter 6\n—\n—\n—\nTutorial - asynchronous\n\n\nWed Mar 29\nChapter 6\n—\n—\n3.4\nFourier Series South Africa Online via Teams\n\n\nFri Mar 31\nChapter 6\n—\n—\n3.5\nFourier Series South Africa Online via Teams\n\n\nMon Apr 03\nChapter 6\n—\n—\n3.6\nFourier Series\n\n\nTue Apr 04\nChapter 6\n—\n—\n—\nTutorial\n\n\nWed Apr 05\nChapter 6\n—\n—\n3.7\nFourier Series\n\n\nFri Apr 07\nno class\n—\n—\n—\nGood Friday\n\n\nMon Apr 10\nno class\n—\n—\n—\nEaster Monday\n\n\nTue Apr 11\nChapter 6\n—\n—\n—\nTutorial\n\n\nWed Apr 12\nChapter 6\n—\n—\n—\nFourier Series"
  },
  {
    "objectID": "M3503-lec01-ODEintro.html#motivating-examples",
    "href": "M3503-lec01-ODEintro.html#motivating-examples",
    "title": "1  Introduction",
    "section": "1.1 Motivating examples",
    "text": "1.1 Motivating examples\nThe art of applied mathematics involves distilling a description of the state a system into relationships between variables. For example, consider a simple pendulum consisting of a mass on a rigid rod constrained to swing in a plane. The various quantities of interest are the mass, \\(m\\), the length of the rod, \\(l\\), the horizontal displacement, \\(x\\), the vertical displacement, \\(y\\), both measured, for example, relative to the fixed end of the rod, and the angle of displacement, \\(\\theta\\). These quantities are not independent. We know \\(x^2+y^2 = l^2\\), and we can choose to measure angle so that \\(x = \\sin \\theta\\) and \\(y = \\cos \\theta\\). Our description so far is static, and our equations are algebraic.\nTo describe the dynamics of the pendulum, we need to extend our mathematical description to include velocities, accelerations, forces and energies. If we neglect the mass of the rod, we can hypothesize that the component of the gravitational force perpendicular to the rod acts to accelerate the mass radially. \\[\\text{mass}\\times\\text{radial acceleration} = \\text{radial force of gravity}\\] \\[ml\\ddot{\\theta} = - m g \\sin \\theta \\tag{1.1}\\] Here \\(g\\) denotes the gravitational constant and we use the double dot notation to denote the second derivative with respect to time. If we assume the angle is small, we can approximate \\(\\sin \\theta\\) by \\(\\theta\\) and Equation Equation 1.1 simplifies to \\[ml\\ddot{\\theta} = - m g \\theta \\tag{1.2}\\]\nOur goal in this course is to understand differential equations like Equation 1.2. We’ll proceed more or less in the following order.\n\nWe’ll start with simpler first order differential equations, some of which you’ll have seen in your Calculus courses.\n\nNext we’ll look at higher order differential equations and systems of first order differential equations.\nMost of our focus will be on linear differential equations and initial value problems.\nWe’ll use Laplace transforms to handle impulsive systems, like circuits with switches.\nWe’ll briefly look at Fourier series and boundary value problems.\n\n\nExample 1.1 The motion of a mass on a spring is modelled as a balance of potential energy, \\(\\frac{1}{2}kx^2\\), which is assumed proportional to the mass’s squared displacement from an equilibrium position, and kinetic energy, \\(\\frac{1}{2}m\\dot{x}^2\\) which is assumed proportional to the mass’s squared velocity, or squared rate of change of displacement.\nConservation of energy implies the sum of these two quantities remains constant. You may have seen this expressed in terms of forces: \\(m\\ddot{x} + kx = 0\\).\nHere the displacement, \\(x\\), is our dependent variable. The independent variable, time, is implicit. We will refer to the mass, \\(m\\), and the spring constant \\(k\\) as our parameters. The differential equation is often accompanied by initial conditions. For example releasing the mass from rest with an initial displacement \\(x_0\\) translates to the conditions \\(x(0) = x_0\\), \\(\\dot{x}(0) = 0\\).\n\n\nExample 1.2 Exponential growth is modelled as a solution to the simple ode \\(y' = ry\\). More complex logistic growth is the solution to the equation \\(y' = ry(1-y/K)\\).\n\n\nExample 1.3 Conservation of charge in an electrical circuit can be modelled as constraints between voltage drops, currents and their rates of change.\n\nAn ordinary differential equation is simply an equation involving an independent variable, say \\(t\\), a dependant variable, \\(y\\), and its derivatives, \\(\\frac{dy}{dt}, \\frac{d^2y}{dt^2}, \\frac{d^3y}{dt^3}, \\dots\\) The order of a differential equation refers to the highest derivative present. For example, \\[\\frac{dy}{dt} = a\\sin(bt)y(1-y)\\] is a first order nonlinear ordinary differential equation; \\[m\\frac{d^2y}{dt^2} + r\\frac{dy}{dt} + ky = a\\cos(t)\\] is a second order linear differential equation. The term linear refers to the linear dependence of the equation on the dependant variable and its derivatives.\nA differential equation, or a system of differential equations is said to be linear if it is linear in the dependent variable(s) and their derivatives. Thus, the equation governing the motion of a damped oscillator subject to an external force \\(F\\), \\(my'' - cy' + ky = F(t)\\), is a linear second order differential equation, even if the forcing term \\(F\\) is nonlinear in the independent variable \\(t\\)."
  },
  {
    "objectID": "M3503-lec01-ODEintro.html#sec-intro-linear",
    "href": "M3503-lec01-ODEintro.html#sec-intro-linear",
    "title": "1  Introduction",
    "section": "1.2 Linear Differential Equations",
    "text": "1.2 Linear Differential Equations\nThe main focus of this course is on linear ordinary differential equations. This is for two simple reasons: (1) the mathematical theory for nonlinear equations is built on the simpler, more complete theory for linear equations, and (2) important concepts, such as system stability and responses to perturbations, are based on linear approximations to nonlinear equations. For example, a balance of forces acting on a mass swinging on a string (a pendulum) leads to the nonlinear differential equation \\[l\\theta''(t) + g\\sin \\theta(t) = 0,\\] where \\(\\theta(t)\\) is the angular displacement of the mass at time \\(t\\), \\(l\\) is the string length and \\(g\\) is our gravitational acceleration. You should quickly verify that the constant function \\(\\theta(t) = 0\\) for all \\(t\\) is a solution to the equation. If the initial displacement is small, we expect the angle to stay close to zero. Thus we approximate the nonlinear term, \\(g\\sin(\\theta)\\), by its tangent line, \\(g\\theta\\), resulting in the more familiar linear differential equation \\[l\\theta''(t) + g\\theta(t) = 0.\\] This one we can solve almost by inspection, since we know that sine and cosine functions behave this way. Indeed, if we guess that \\(\\theta(t) = \\sin(\\omega t)\\) we can find the frequency, \\(\\omega\\), with some simple algebra: \\[\\begin{align*}\n    l\\theta''(t) + g\\theta(t) &= 0 \\\\\n    l\\frac{d^2}{d\\theta^2}\\sin(\\omega t) + g\\sin(\\omega t) &= 0 \\\\\n    -l\\omega^2\\sin(\\omega t) + g\\sin(\\omega t) &= 0 \\\\\n    \\left(-l\\omega^2 + g\\right)\\sin(\\omega t)  &= 0\n\\end{align*}\\] Thus either \\(\\sin(\\omega t) =0\\) for all \\(t\\), implying \\(\\omega = 0\\), or \\(\\omega = \\sqrt{g/l}\\). The first case is the trivial zero solution we already guessed, the second case is the natural frequency of the pendulum. We will cover this in more depth later."
  },
  {
    "objectID": "M3503-lec01-ODEintro.html#sec-intro-systems",
    "href": "M3503-lec01-ODEintro.html#sec-intro-systems",
    "title": "1  Introduction",
    "section": "1.3 Higher order differential equations and systems of differential equations",
    "text": "1.3 Higher order differential equations and systems of differential equations\nAn \\(n^{\\text{th}}\\) order differential equation can be written \\[{\\cal{F}}\\left(t,x,\\frac{dx}{dt},\\dots,\\frac{d^nx}{dt^n}\\right) = 0,\\] or if we solve for the highest derivative, \\[\\frac{d^nx}{dt^n} = F(t,x,\\frac{dx}{dt},\\dots,\\frac{d^i_{n-1}x}{dt^{n-1}}).\\] Such an equation can also be written as a system of first order equations by introducing new dependent variables for the derivatives: \\[\\begin{align*}\n  x'_0 &= x_1,  \\\\\n  x'_1 &= x_2,  \\\\\n  x'_2 &= x_3,  \\\\\n    x'_{n-1} &=  F(t,x_0,x_1,\\dots,x_{n-1}).\n\\end{align*}\\] Here, \\(x_i\\) is the \\(i^{\\text{th}}\\) derivative of \\(x\\), and \\(x_0\\) is our new name for the original variable. Since higher order equations can be cast as systems of first order equations, we will spend much of the course dealing with such systems. Notationally, we just allow the dependent variable to be a vector: \\(x' = f(t,x)\\), where \\(f:{\\mathbb R}\\times{\\mathbb R}^n\\to {\\mathbb R}^n\\).\nGeometrically, the solution to a system of first order equations, \\(x(t)\\), describes a curve in \\({\\mathbb R}^n\\) parameterized by \\(t\\). The derivative \\(x'(t)\\) is the vector tangent to this curve at \\((t,x(t))\\). Hence, the solution to the differential equation is the curve \\(x(t)\\), which is everywhere tangent to \\(f(t,x(t))\\)."
  },
  {
    "objectID": "M3503-lec01-FirstOrderODE.html#sec-direction-field",
    "href": "M3503-lec01-FirstOrderODE.html#sec-direction-field",
    "title": "2  First order differential equations",
    "section": "2.1 Direction fields",
    "text": "2.1 Direction fields\nA solution to a first order differential equation, \\(y' = f(x,y)\\), is a function, \\(y(x)\\), whose slope at \\(x\\) is \\(f(x,y)\\). One way to visualize this is to view \\(f\\) as a direction field. That is, \\(f\\) assigns a slope, or direction, to every point in the \\(x\\)-\\(y\\) plane. The figure below shows the direction field \\(f:(x,y)\\to 2x -1 - 3y\\).\n\n\nShow the code\nfrom matplotlib.pyplot import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnx, ny = .3, .3\nx = np.arange(1, 4, nx)\ny = np.arange(0, 2, ny)\nX, Y = np.meshgrid(x, y)\n\ndy = 2*X -1 - 3*Y\ndx = np.ones(dy.shape)\n\nplot2 = plt.figure()\nplt.quiver(X, Y, dx, dy, \n           color='Teal', \n           headlength=0, headwidth=1)\n\nplt.title('Direction Field')\nplt.show\n\n\n<function matplotlib.pyplot.show(close=None, block=None)>"
  },
  {
    "objectID": "M3503-lec01-FirstOrderODE.html#sec-separable",
    "href": "M3503-lec01-FirstOrderODE.html#sec-separable",
    "title": "2  First order differential equations",
    "section": "2.2 first order separable equations",
    "text": "2.2 first order separable equations\nA first order linear differential equation of the form \\(\\frac{dy}{dt} = f(y)g(t)\\) is said to be separable. It can be solved by a single integration step. \\[\\begin{align*}\n  \\frac{dy}{dt} &= f(y)g(t) \\\\\n  \\frac{1}{f(y)}\\frac{dy}{dt} &= g(t) \\\\\n  \\int \\frac{1}{f(y)}\\frac{dy}{dt}\\,dt &= \\int g(t)\\,dt + C \\\\\n  \\int \\frac{1}{f(y)}\\,dy &= \\int g(t)\\,dt + C\n\\end{align*}\\]\n\nExample 2.1 Find the general solution to the differential equation \\(\\frac{dy}{dt} = ry\\cos(t)\\).\nSeparating variables and integrating leads to \\[\\begin{align}\n    \\int \\frac{1}{y}\\,dy &= \\int r\\cos(t)\\,dt +C \\\\\n    \\log |y| &= r\\sin t  + C \\\\\n    y(t) &= \\pm\\exp(C+r\\sin t )\n\\end{align}\\] The solution is usually given in the form \\(y(t) = y_0e^{r\\sin(t)}\\). Since \\(C\\) is an arbitrary constant, \\(y_0 = \\pm e^C\\) is also arbitrary.\n\n\n\n\n\n\n\nNote\n\n\n\nSince divide by \\(y\\) on our first step in this example, we must assume \\(y\\ne0\\). However, it is easy to see by inspection that \\(y=0\\) is also a solution to the equation.\nHence, we can express the general solution as \\(y(t) = y_0\\exp(r\\sin t)\\) where \\(y_0\\) can be any real number.\n\n\n\nExample 2.2 Find the general solution to the differential equation \\(\\frac{dy}{dx} = \\frac{6x^2}{2y+\\cos y}\\).\nSeparating variables and integrating leads to \\[\\begin{align}\n  \\int (2y+\\cos y)\\,dy&= \\int 6x^2 \\, dx + C \\\\\n  y^2+\\sin y &= 2x^3  + C\n\\end{align}\\] In this case, we are not able to solve for \\(y\\) as a function of \\(x\\). We are left with \\(y\\) defined implicitly as a function of \\(x\\).\n\nDespite not being able to find an analytic expression for \\(y\\) as a function of \\(x\\) in this example, we can still make nice plots of the solutions. The trick is to notice that we can find the inverse of the solution. Several solutions are shown in Figure 2.1\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef inversesoln(y,c): return( np.cbrt((y*y+np.sin(y))/2 - c/2) )\ny = np.linspace(-np.pi,np.pi,100)\nfor c in np.arange(-1.2,1.2,.2):\n  plt.plot(inversesoln(y,c),y)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\n\n\n\nFigure 2.1: Sample solutions for Example 2.2\n\n\n\n\n\nExample 2.3 Solve the differential equation \\(\\frac{dy}{dx} = \\frac{x^2}{y^2}\\).\nSeparating variables and integrating leads to \\(\\frac{y^3}{3} = \\frac{x^3}{3} + C\\). Solving for the dependent variable leads to \\(y = \\sqrt[3]{x^3+3C}\\).\n\n\nExample 2.4 Solve the differential equation \\(\\frac{dy}{dx} = -\\frac{2x}{y}\\).\nSeparating variables and integrating leads to \\(\\frac{y^2}{2} = -x^2 + C\\), which is a family of ellipses.\n\n\nExample 2.5 Solve the differential equation \\(\\frac{du}{dt} = 2 + 2u + t + tu\\).\nHere \\(u\\) is our dependent variable. The equation is both linear and separable so we may choose either method. \\[\\begin{align*}\n   \\frac{du}{dt} &= (2+t)(1+u) \\\\\n   \\int \\frac{du}{1+u} &= \\int (2+t) \\, dt  \\\\\n   \\ln|1+u| &= 2t+\\frac{t^2}{2} + C \\\\\n   |1+u| &= A \\exp(2t+\\frac{t^2}{2}) \\\\\n   u &= -1 \\pm A \\exp(2t+\\frac{t^2}{2})\n\\end{align*}\\] Note the use of \\(\\pm\\) is not necessary. Since \\(A\\) could be positive or negative. It just serves to remind us that integration of \\(1/(1+u)\\) leads to two solutions.\n\n\nExample 2.6 Solve the differential equation \\(xy' + y = y^2\\). This is nonlinear, due to the \\(y^2\\) term. It is separable. \\[\\begin{align*}\n    \\frac{1}{y^2-y} y' &= \\frac{1}{x} \\\\\n    \\int \\frac{dy}{y^2-y} &= \\int \\frac{dx}{x} \\\\\n    \\int \\frac{1}{y-1} - \\frac{1}{y} dy &= \\int \\frac{dx}{x} \\\\\n     \\ln|y-1| - \\ln|y| &= \\ln|x| + C \\\\\n     \\ln\\frac{|y-1|}{|x||y|} &=  C \\\\\n     \\frac{|y-1|}{|x||y|} &=  A\n  \\end{align*}\\] there are in fact several solutions buried in this notation. For example, if \\(0< y < 1\\), then \\(\\frac{|y-1|}{|x||y|} = \\frac{1-y}{xy}\\), and we find \\(1/y - 1 = A/x \\rightarrow y = x/(A+x)\\), \\(A>0\\).\n\nAs a further exercise, sketch a few sample solutions with \\(y< 0\\), \\(0 < y < 1\\) and \\(y>1\\)."
  },
  {
    "objectID": "M3503-lec01-FirstOrderODE.html#sec-1storder-linear",
    "href": "M3503-lec01-FirstOrderODE.html#sec-1storder-linear",
    "title": "2  First order differential equations",
    "section": "2.3 Linear first order differential equations",
    "text": "2.3 Linear first order differential equations\nConsider a differential equation of the form \\(y'(t) +p(t) y(t) = q(t)\\) with initial data \\(y(t_0) = y_0\\). These can be integrated using an integrating factor. The trick is to find a factor, \\(u\\), which turns the left hand side of the equation into the derivative of something familiar. We multiply both sides of the equation by \\(u\\), \\[u(t)y'(t) +u(t)p(t) y(t) = u(t)q(t).\\] Notice that if we can choose \\(u\\) so that \\(up = u'\\), then the left hand side is the derivative of \\(uy\\): \\[\\left(uy\\right)' = uy' +u' y = uy' +up y =  uq.\\] We can solve this with a single integration \\[\\begin{gather}\n    \\frac{d}{dt}\\left(u(t)y(t)\\right) =  u(t)q(t) \\\\\n    u(t)y(t) =  \\int u(s)q(s) \\, ds \\\\\n    y(t) =  \\frac{1}{u(t)} \\int u(s)q(s)\\, ds\n\\end{gather}\\] There is an arbitrary constant resulting from in the integration of \\(uq\\). We’ll use use our initial conditions to determine that once we determined \\(u\\).\nReturning to \\(u'=pu\\), the differential equation for \\(u\\), we see it is separable and has solutions of the form \\(u(t) = \\exp(\\int p)\\). To make the result more readable, define \\(P(t) = \\int_{t_0}^t p(s) \\, ds\\) and take \\(u\\) as \\(u(t) = \\exp(P(t))\\). This is the solution that also satisfies \\(u(t_0)=1\\), which is convenient. Putting this together with our expression for \\(y\\), we find, in terms of \\(P\\), \\[y(t) = e^{-P(t)} \\int e^{P(s)}q(s)\\, ds\\] This is our general solution, since it still has the constant of integration hiding in the integral. Applying the initial conditions we find \\[y(t) = e^{-P(t)} \\left( y_0 + \\int_{t_0}^t e^{P(s)}q(s)\\, ds\\]\n\n\n\n\n\n\nWarning\n\n\n\nI advise against memorizing this formula. Instead memorize the technique. Applying the technique is often simpler than applying the formula, and we’ll extend the ideas behind the method later in the course.\n\n\n\nExample 2.7 Solve the initial value problem \\(t^2y' + 2ty = \\log t, \\ y(1) = 2\\).\nYou can probably guess the integrating factor by noticing the left hand side looks like the product rule applied to \\(t^2y\\). Specifically, \\[\\begin{align*}\n        t^2y' + 2ty &= \\log t \\\\\n        \\dfrac{d}{dt}\\left(t^2y\\right)  &= \\log t \\\\\n        \\left. s^2y(s) \\right|_{s=1}^{s=t}  &= \\int_1^t \\log s \\, ds \\\\\n        t^2y(t) - y(1)  &= t\\log(t) - t + 1 \\\\\n        y(t) &= \\frac{1}{t} \\log(t) - \\frac{1}{t} + \\frac{3}{t^2}    \n\\end{align*}\\]\n\n\n\n\n\n\n\nTip\n\n\n\nIf you don’t immediately recognize the integrating factor, first rewrite the problem as $ y’ + y = t$ The integrating factor can be computed as \\(u(t) = \\exp\\left(\\displaystyle\\int \\frac{2}{t} \\, dt\\right) = t^2\\)"
  },
  {
    "objectID": "M3503-lec02-Systems.html",
    "href": "M3503-lec02-Systems.html",
    "title": "3  Systems of First order equations",
    "section": "",
    "text": "4 Homogeneous Constant Coefficient Linear Systems"
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-general-nonlinear-system-existence-and-uniqueness-of-solutions",
    "href": "M3503-lec02-Systems.html#the-general-nonlinear-system-existence-and-uniqueness-of-solutions",
    "title": "3  Systems of First order equations",
    "section": "3.1 The General Nonlinear System: Existence and Uniqueness of Solutions",
    "text": "3.1 The General Nonlinear System: Existence and Uniqueness of Solutions\nConsider a system of differential equations of the form \\(y'(t) = f(t,y(t))\\), where \\(f:{\\mathbb R}\\times{\\mathbb R}^n\\to{\\mathbb R}^n\\) and we seek a solution \\(y:{\\mathbb R}\\to {\\mathbb R}^n\\). That is, \\(y\\) is a vector, or n-tuple of functions, and the function \\(f\\) takes a vector and a real number as inputs and returns a vector as an output. The equations are often accompanied by initial conditions of the form \\(y(t_0) = y_0\\). The solution is a curve in \\({\\mathbb R}^n\\) parameterized by \\(t\\) and passing through the point \\(y_0\\).\nSufficient conditions to guarantee the existence of a unique solution through any point are that \\(f\\) be continuous in \\(t\\) and differentiable in \\(y\\). However, a weaker condition than differentiability is also known to suffice.\n\nDefinition 3.1 (Lipschitz Continuity) A function \\(f\\) is said to be Lipschitz continuous on some subset \\(U\\) of its domain if there exists a constant \\(M\\) for which \\(\\left\\|f(x)-f(y)\\right\\| < M\\left\\|x-y\\right\\|\\) whenever \\(x,y\\in U\\) with \\(x\\ne y\\).\n\nEquivalently, a function is Lipschitz continuous if the slopes of its secant lines are all bounded. For example, \\(|x|\\) is Lipschitz continuous; \\(x^{2/3}\\) is continuous but not Lipschitz since the secant lines tend towards vertical near the origin. Note, \\(x^{2/3}\\) is (locally) Lipschitz so long as the set of interest is bounded away from the origin. In one dimension, \\(\\left\\|x-y\\right\\|\\) can be assumed to be \\(|x-y|\\). In higher dimensions \\(\\left\\|\\cdot\\right\\|\\) is most often the standard Euclidian distance.\nThe classic result for the existence of solutions to our system of differential equations is as follows.\n\nTheorem 3.1 If \\(f(t,y)\\) is continuous in the independent variable, \\(t\\), and Lipschitz continuous in \\(y\\), then the initial value problem \\(y' = f(t,y)\\), \\(y(t_0) = y_0\\) has a unique solution on some open interval \\(a < t_0 < b\\).\n\nVarious proofs of the above theorem can be found in most advanced books on differential equations.\n\nExample 3.1 An example for which solutions exist, but are not unique is the simple scalar initial value problem \\(y' = y^{2/3}\\), \\(y(0) = 0\\). The function \\(y^{2/3}\\) is continuous for all \\(y\\), but has a cusp at the origin. It is not differentiable, nor is it Lipschitz continuous at the origin. Yet the differential equation is easily solved using the method of separation of variables, leading to the solutions \\(y(t) = \\frac{1}{3}(t+c)^3\\). Taking \\(c=0\\) gives a solution passing through the origin. A second obvious solution is \\(y(t) = 0\\). Moreover, we can splice these solutions together to get an infinite family of solutions to the initial value problem: \\[\\begin{equation}\n    y(t) = \\begin{cases}\n          \\frac{1}{3}(t+c_1)^3 & t < -c_1  \\\\\n          0 & -c_1 \\le t \\le c_2 \\\\\n          \\frac{1}{3}(t-c_2)^3 & t > c_2  \n           \\end{cases}\n   \\end{equation}\\] with \\(c_1\\) and \\(c_2\\) arbitrary nonnegative constants.\n\nSystems of higher order differential equations can be expressed as systems of linear equations by introducing the derivatives as new state variables.\n\nExample 3.2 Express the second order equation \\((t-3)y''(t) + t^2y'(t) + 7y(t) = f(t)\\) as a system of two first order equations.\nLet \\(z = y'\\). Then the second order equation becomes \\((t-3)z'(t) + t^2z(t) + 7y(t) = f(t)\\). This together with the definition of \\(z\\) gives a pair of first order equations. Dividing through by \\((t-3)\\) puts the system in the basic form \\[\\begin{align*}\n    y'(t) &= z(t) \\\\\n    z'(t) &=  -\\frac{t^2}{t-3}z(t) - \\frac{7}{t-3}y(t) + f(t)\n  \\end{align*}\\] The right hand side is linear, and hence differentiable, in \\((y,z)\\) and continuous in \\(t\\) on any interval bounded away from \\(t=3\\)."
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-big-picture",
    "href": "M3503-lec02-Systems.html#the-big-picture",
    "title": "3  Systems of First order equations",
    "section": "4.1 The big picture",
    "text": "4.1 The big picture\nConsider the linear system of ordinary differential equations \\(x' = Ax\\) where \\(x(t) = \\left( x_1(t), x_2(t), \\dots, x_n(t) \\right)^T\\)and \\(A\\) is an \\(n\\times n\\) matrix of real numbers (constants). The general theory for finding all solutions to the linear system is based on an eigenvalue decomposition of the matrix \\(A\\). Let \\(r_1, r_2, \\dots r_m\\) be the \\(m\\) eigenvalues of \\(A\\). The theory of matrices and eigenvalues guarantees that \\(A\\) has at least one eigenvalue, and no more than \\(n\\) distinct eigenvalues, hence we can arrange things so that \\(1 \\le m \\le n\\) and all the \\(m\\) eigenvalues are distinct. That is, \\(r_i=r_j \\leftrightarrow i=j\\). A handy theorem from linear algebra states that there is an invertible matrix \\(S\\), for which \\(S^{-1}AS = \\begin{pmatrix} J_1 & & 0 \\\\ & \\ddots & \\\\ 0 & & J_m \\end{pmatrix}\\).\nWe know from linear algebra that each eigenvalue of \\(A\\) has a multiplicity."
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-diagon",
    "href": "M3503-lec02-Systems.html#sec-diagon",
    "title": "3  Systems of First order equations",
    "section": "4.2 Real and Distinct Eigenvalues",
    "text": "4.2 Real and Distinct Eigenvalues\nConsider the system \\(y'=Ay\\) where \\(A\\) is a real \\(n\\times n\\) matrix, and suppose that \\(A\\) has a real eigenvalue \\(r\\) with an associated eigenvector \\(u\\). Then \\(y(t) = e^{rt}u\\) a solution. This is easy to verify by direct substitution.\nSuppose \\(u_1,\\dots,u_m\\) are \\(m\\) linearly independent eigenvectors of \\(A\\) with associated eigenvalues \\(r_1,\\dots,r_m\\). We look for a solution of the form \\[\\begin{equation}y(t) = \\sum_i^m x_i(t)u_i\\end{equation}\\] Substituting this form into the differential equation leads to \\[\\begin{equation} \\sum_i^m x'_i(t)u_i = A\\sum_i^m x_i(t)u_i = \\sum_i^m r_ix_i(t)u_i \\Rightarrow \\sum_i^m \\left(x'_i(t)-rx_i(t)\\right)u_i = 0 \\end{equation}\\] Since the vectors are linearly independent, it follows that \\(x'_i(t)-rx_i(t) = 0\\), \\(i=1,\\dots,m\\), which has the solutions \\(x_i(t) = c_ie^{r_it}\\), \\(i=1,\\dots,m\\), where each \\(c_i\\) is an arbitrary constant. Thus, \\[\\begin{equation}y(t) = \\sum_i^m c_iu_ie^{r_it} \\end{equation}\\] The effect of choosing the eigenvectors as a basis is to transform the system to a set of \\(m\\) decoupled equations that have well-known solutions."
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-complex",
    "href": "M3503-lec02-Systems.html#sec-complex",
    "title": "3  Systems of First order equations",
    "section": "4.3 Complex Eigenvalues",
    "text": "4.3 Complex Eigenvalues\nConsider the system \\(y'(t) = Ay(t)\\) where \\(A\\) is a real \\(n\\times n\\) matrix, and suppose that \\(A\\) has a complex eigenvalue \\(r = a+bi\\) with an associated eigenvector \\(u = v + iw\\). It follows by direct substitution that \\(v-iw\\) is also an eigenvector of \\(A\\) associated with the eigenvalue \\(a-ib\\).\nWe seek a solution of the form \\(y(t) = x_1(t) v + x_2(t) w\\) where \\(x_1\\) and \\(x_2\\) are scalar functions of \\(t\\). Direct computation yields that \\(A(x_1v + x_2w) = (ax_1 +bx_2)v + (-bx_1+ax_2)w\\). Hence, \\(x_1\\) and \\(x_2\\) must satisfy the pair of differential equations \\[\\begin{align*}\n    x_1'(t) &= ax_1(t) +bx_2(t) \\\\\n    x_2'(t) &= -bx_1(t) +ax_2(t)\n\\end{align*}\\] Since \\(e^{at}\\) is an integrating factor for both the above equations, \\[\\begin{align}\n  \\frac{d}{dt} \\left(x_1e^{-at}\\right) &= bx_2e^{-at}, \\label{intfactx1}\\\\\n\\frac{d}{dt} \\left(x_2e^{-at}\\right) &= -bx_1e^{-at}.\\label{intfactx2}\n\\end{align}\\] Differentiating the first of these equations and eliminating \\(x_2\\) using the second yields \\[\\begin{equation}\n\\frac{d^2}{dt^2} \\left(x_1e^{-at}\\right) = b\\frac{d}{dt}\\left( x_2e^{-at}\\right) = -b^2x_1e^{-at}.\n\\end{equation}\\] Hence, \\(x_1e^{-at} = c_1\\cos(bt) + c_2\\sin(bt)\\), which implies \\(x_1(t)= e^{at}\\left( c_1\\cos(bt) + c_2\\sin(bt)\\right)\\). To compute \\(x_2\\) we use as follows: \\[\\begin{align*}\nbx_2e^{-at}\n&=  \\frac{d}{dt} \\left(x_1e^{-at}\\right) \\\\\n&= \\frac{d}{dt} \\left( c_1\\cos(bt) + c_2\\sin(bt)\\right) \\\\\n&= \\left( -bc_1\\sin(bt) + bc_2\\cos(bt)\\right) \\\\\nx_2(t) &= e^{at} \\left( -c_1\\sin(bt) + c_2\\cos(bt)\\right)\n\\end{align*}\\] A nice way to memorize these solutions is to write them in a matrix form. \\[\\begin{equation}\n  \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} = e^{at}\\begin{pmatrix} \\cos(bt) & \\sin(bt) \\\\ -\\sin(bt) & \\cos(bt) \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\n\\end{equation}\\] The matrix on the right is the rotation matrix corresponding to counterclockwise rotation by the angle \\(bt\\). Hence, the solutions form a spiral in the \\(u\\)-\\(v\\) plane.\nReturning to the original coordinates, \\[\\begin{equation}\n  y(t) = c_1e^{at}\\left(v\\cos(bt) - w\\sin(bt)\\right) + c_2e^{at}\\left(v\\sin(bt) + w\\cos(bt)\\right)  \n\\end{equation}\\] Note that up until now we have not specified the size of the original system. The solutions found above work regardless of the number of equations in the original system. If \\(A\\) is a \\(2\\times2\\) matrix, then the solution can be expressed compactly by defining \\(S\\) as the matrix whose columns are \\(v\\) and \\(w\\). In this case \\(y = Sx = e^{at}SR(bt)C\\) where \\(R(\\theta)\\) is the rotation matrix through angle \\(\\theta\\) and \\(C\\) is the transpose of \\(\\begin{pmatrix} c_1 & c_2\\end{pmatrix}\\). If we have initial conditions at \\(t=0\\), then \\(y(0) = SC\\), since \\(R(0)\\) is the identity (rotation by zero). Thus, \\(C= S^{-1}y(0)\\) and \\[\\begin{equation}\n  y(t) = e^{at}SR(bt)S^{-1}y(0).\n\\end{equation}\\]"
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-geneig",
    "href": "M3503-lec02-Systems.html#sec-geneig",
    "title": "3  Systems of First order equations",
    "section": "4.4 Repeated Eigenvalues",
    "text": "4.4 Repeated Eigenvalues\nWe have seen how to find solutions if we know a sufficient number of linearly independent eigenvectors. In this section we tackle the case where the geometric multiplicity of an eigenvalue (the dimension of the Eigenspace) is less than its algebraic multiplicity. We do this by first finding a general solution for a simple case and then derive a method to transform, through a change of basis, any other system to a similar simple system."
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-second-simplest-case",
    "href": "M3503-lec02-Systems.html#the-second-simplest-case",
    "title": "3  Systems of First order equations",
    "section": "4.5 The second simplest case",
    "text": "4.5 The second simplest case\nConsider the system \\[\\begin{align}\n  \\dot{x}_1(t) &= rx_1(t) + x_2(t),  \\label{simple1}\\\\\n  \\dot{x}_2(t) &= rx_2(t), \\label{simple2}\n\\end{align}\\] which when written in matrix form looks like \\[ \\begin{pmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\end{pmatrix}  \n  = \\begin{pmatrix} r & 1 \\\\ 0 & r \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}  \\]\nThe matrix \\(J = \\begin{pmatrix} r & 1 \\\\ 0 & r \\end{pmatrix}\\) has a single eigenvalue \\(r\\) with multiplicity 2. However, all eigenvectors of \\(J\\) are multiples of \\(\\vec{u} = \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix}\\). Hence, the eigenvalue \\(r\\) has geometric multiplicity one.\nTo solve the system, first integrate to obtain \\(x_2(t) = c_2e^{rt}\\), then substitute this into and integrate a second time using the integrating factor \\(e^{-rt}\\) to obtain \\[\\begin{align*}\n  \\dot{x}_1(t) = rx_1(t) &+ c_2e^{rt} \\\\\n  \\dot{x}_1(t) - rx_1(t) &=  c_2e^{rt} \\\\\n  \\frac{d}{dt}\\left({x}_1(t)e^{-rt}\\right) &=  c_2 \\\\\n  {x}_1(t)e^{-rt} &=  c_2t  + c_1\\\\\n  {x}_1(t) &=  c_2te^{rt}  + c_1e^{rt}\n\\end{align*}\\] This solution can be written in vector form as \\[\\begin{equation}\n  \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix}\n    = c_1 e^{rt}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    + c_2 \\left( te^{rt}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}  + e^{rt}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right)\n\\end{equation}\\] or in matrix form as \\[\\begin{equation}\n  \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix}\n    = \\begin{pmatrix} e^{rt}  & te^{rt}\\\\ 0 & e^{rt} \\end{pmatrix}\n      \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\n\\end{equation}\\] Both forms will be convenient at times."
  },
  {
    "objectID": "M3503-lec02-Systems.html#generalized-eigenvectors",
    "href": "M3503-lec02-Systems.html#generalized-eigenvectors",
    "title": "3  Systems of First order equations",
    "section": "4.6 Generalized Eigenvectors",
    "text": "4.6 Generalized Eigenvectors\n\nDefinition 4.1 (Generalized eigenvector) A generalized eigenvector of \\(A\\) is a vector \\(v\\) for which \\[\\left(A-rI\\right)^m v = 0\\] for some positive integer \\(m\\).\n\nComments:\n\nClearly, any eigenvector of \\(A\\) is a generalized eigenvector (with \\(m=1\\)).\nIf \\(v\\) is a generalized eigenvector of \\(A\\), then the scalar \\(\\lambda\\) in the definition is an eigenvalue of \\(A\\). It also follows that \\(u = \\left(A-\\lambda I\\right)^{m-1}v\\) is an associated eigenvector.\n\n\nTheorem 4.1 If \\(\\lambda\\) is an eigenvalue of \\(A\\) with algebraic multiplicity \\(k\\), then the null space of \\((A-\\lambda I)^k\\) has dimension \\(k\\).\n\n\nTheorem 4.2 Every matrix \\(A\\) has a full set of generalized eigenvectors. That is, there is a (possibly complex) linearly independent set \\(v_1,\\dots,v_n\\) of generalized eigenvectors for every \\(n\\times n\\) matrix \\(A\\).\n\n\nExample 4.1 Consider the linear system \\(\\vec{y}' = A\\vec{y}\\) with \\(\\vec{y}(0) = \\vec{y}_0\\) and \\[\\begin{equation} A = \\begin{pmatrix} 4 & -4 \\\\  1 & 0 \\end{pmatrix} \\end{equation}\\] The characteristic polynomial for \\(A\\) is \\((4-r)(-r) -(-4)(1) = r^2 - 4r + 4 = (r-2)^2\\). Hence, \\(r_1 = 2\\) is an eigenvalue of \\(A\\) with algebraic multiplicity 2. To find the eigenvectors of \\(A\\) associated with \\(r_1\\) we construct and simplify\n\\[(A -r_1I) = \\begin{pmatrix} 4-2 & -4 \\\\  1 & 0-2 \\end{pmatrix}  \n               = \\begin{pmatrix} 2 & -4 \\\\  1 & -2 \\end{pmatrix}\n               \\sim \\begin{pmatrix} 1 & -2 \\\\  0 & 0 \\end{pmatrix}\\] Thus the solutions to \\((A-2I)\\vec{u} = 0\\) are all multiples of \\(\\vec{u} = \\begin{pmatrix} 2 & 1 \\end{pmatrix}.\\) This is a one dimensional subspace. Hence \\(r_1\\) has geometric multiplicity 1. As it turns out, the particular generalized eigenvector we want is a solution to \\[(A -r_1I)\\vec{v}  = \\vec{u}.\\] \\(\\vec{v}\\) will be a generalized eigenvector since \\[(A -r_1I)^2\\vec{v}  = (A -r_1I)\\vec{u} = 0.\\] To find \\(\\vec{v}\\), we reduce the augmented matrix \\[\\left(\\begin{array}{cc|c} 2 & -4 & 2 \\\\ 1 & -2 & 1 \\end{array} \\right)\n     \\sim \\left(\\begin{array}{cc|c}  1 & -2 & 1\\\\ 0 & 0 & 0 \\end{array} \\right)\\] Thus, the entries of \\(\\vec{v}\\) satisfy \\(v_1-2v_2 = 1\\). Setting \\(v_2 = a \\in {\\mathbb R}\\) give \\(\\vec{v} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}a\\). We will choose \\(a=0\\) to give \\(\\vec{v} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\). Note that \\(\\vec{u}\\) and \\(\\vec{v}\\) are linearly independent and hence form a basis for \\({\\mathbb R}^2\\).\nWe look for solutions relative to this basis: \\[\\vec{y}(t) = x_1(t)\\vec{u} + x_2(t)\\vec{v}\\] That is, \\(x_1\\) and \\(x_2\\) are the coordinates of our solution with respect to the basis \\(\\left\\{ \\vec{u},\\vec{v}\\right\\}\\). Substituting this form for \\(\\vec{y}\\) into the differential equation leads to \\[\\begin{align*}\n      x'_1(t)\\vec{u} + x'_2(t)\\vec{v} &= x_1(t)A\\vec{u} + x_2(t)A\\vec{v}  \\\\\n      &= x_1(t)r_1\\vec{u} + x_2(t)\\left(r_1\\vec{v} + \\vec{u}\\right)  \\\\\n      &= \\left(r_1x_1(t) + x_2(t)\\right)\\vec{u} + r_1x_2(t)\\vec{v}\n   \\end{align*}\\] Finally, equating the coefficients on both sides of the equation leads to the simpler system given by Equations (\\(\\ref{simple1}\\)) and (\\(\\ref{simple2}\\)) above, which have solutions \\[\\begin{align*}\n    x_1(t) &=  c_2te^{r_1t}  + c_1e^{r_1t},\\\\  \n    x_2(t) &= c_2e^{r_1t}.\n  \\end{align*}\\] This leads to a solution for \\(\\vec{y}\\) of \\[\\vec{y}(t) = \\left(c_2te^{r_1t}  + c_1e^{r_1t}\\right)\\vec{u} + c_2e^{r_1t}\\vec{v}\n                = c_1e^{r_1t}\\vec{u} + c_2\\left(te^{r_1t}\\vec{u} + e^{r_1t}\\vec{v}\\right)\\] For our example we have \\(r_1 = 2\\), \\(\\vec{u} = \\begin{pmatrix} 2 \\\\ 1\\end{pmatrix}\\) and \\(\\vec{v} = \\begin{pmatrix} 1 \\\\ 0\\end{pmatrix}\\). Hence \\[\\vec{y}(t) = c_1e^{r_1t}\\begin{pmatrix} 2 \\\\ 1\\end{pmatrix} + c_2\\left(te^{r_1t}\\begin{pmatrix} 2 \\\\ 1\\end{pmatrix} + e^{r_1t}\\begin{pmatrix} 1 \\\\ 0\\end{pmatrix}\\right)\\]"
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-jordan-form",
    "href": "M3503-lec02-Systems.html#the-jordan-form",
    "title": "3  Systems of First order equations",
    "section": "4.7 The Jordan form",
    "text": "4.7 The Jordan form\nThe question remains if we have found all the solutions. The answer to this lies in a theorem from linear algebra that states, roughly, that given any \\(n\\times n\\) matrix \\(A\\), there is a linearly independent set of generalized eigenvectors of \\(A\\) that spans \\({\\mathbb R}^n\\). Moreover, proceeding as we have above, we will find such a set. It can then be shown that the solutions we’ve found will be linearly independent and yield the complete general solution to \\(y' = Ay\\)."
  },
  {
    "objectID": "M3503-lec02-Systems.html#sec-varparm",
    "href": "M3503-lec02-Systems.html#sec-varparm",
    "title": "3  Systems of First order equations",
    "section": "5.1 The method of variation of parameters",
    "text": "5.1 The method of variation of parameters\nConsider the system \\[\\begin{equation}\n  \\frac{d}{dt}y(t) = Ay(t) + f(t), \\qquad y(t_0) = y_0,  \\label{nonhom}\n\\end{equation}\\] where \\(A\\) is a given \\(n\\times n\\) matrix of real numbers and \\(f\\) is a given vector-valued function from \\({\\mathbb R}\\) to \\({\\mathbb R}^n\\) Suppose we know \\(n\\) linearly independent solutions to the homogeneous system \\(y' = Ay\\). Then the fundamental matrix, \\(\\Psi\\), whose columns are these \\(n\\) solutions satisfies \\[\\begin{equation}\n  \\frac{d}{dt}\\Psi(t) = A\\Psi(t) + f(t).\n\\end{equation}\\] Applying the method of variation of parameters, we seek a solution to of the form \\(y(t) = \\Psi(t)u(t)\\). Substituting this form into leads to \\[\\Psi'u + \\Psi u' = A\\Psi u + f.\\] Since \\(\\Psi\\) is a solution to the homogeneous problem, the above equation reduces to \\[ \\Psi u' =  f,\\] and hence, \\[\\begin{equation}u(t) = \\int \\Psi^{-1}(t) f(t) \\, dt.\\end{equation}\\] To satisfy, the initial conditions, we choose the particular solution \\(y_p = \\Psi u\\) satisfying \\(y_p(t_0) = y_0\\). Then \\[\\begin{equation}y(t) = \\Psi(t)\\Psi^{-1}(t_)y_0 + \\Psi(t)\\int_{t_0}^{t} \\Psi^{-1}(\\tau) f(\\tau) \\, d\\tau.\\end{equation}\\]\n\nExample 5.1 Consider the system \\(y' = Ay + f\\) with \\(A = \\begin{pmatrix} 3 & 3 \\\\ 1 & 5 \\end{pmatrix}\\) and \\(f = \\begin{pmatrix} t \\\\ e^{t} \\end{pmatrix}.\\) The characteristic polynomial of \\(A\\) is \\((3-r)(5-r)-3 = (6-r)(6+r)\\). The eigenvectors are \\(r_1 = 6\\) and \\(r_2 = 2\\) with associated eigenvectors \\(u_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) and \\(u_2 = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\). Thus two linearly independent solutions to the homogeneous problem are \\(y_1(t) = e^{6t}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) and \\(y_2(t) = e^{2t}\\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\). One possible fundamental matrix is \\[\\begin{equation}\n    \\Psi(t) = \\begin{pmatrix} e^{6t} & 3e^{2t} \\\\ e^{6t} & -e^{2t} \\end{pmatrix}\n    = \\begin{pmatrix} 1 & 3 \\\\ 1 & -1 \\end{pmatrix}\\begin{pmatrix} e^{6t} & 0 \\\\ 0 & e^{2t} \\end{pmatrix}\n  \\end{equation}\\] Then \\(\\Psi^{-1}(t) = \\frac{1}{4}\\begin{pmatrix} e^{-6t} & 3e^{-6t} \\\\ e^{-2t} & -e^{-2t} \\end{pmatrix}\\), and\n\\[\\begin{equation}\n    \\Psi(t)\\Psi^{-1}(\\tau) = \\frac{1}{4}\\begin{pmatrix} e^{6(t-\\tau)}+3e^{2(t-\\tau)} & 3e^{6(t-\\tau)}-3e^{2(t-\\tau)} \\\\ e^{6(t-\\tau)}-e^{2(t-\\tau)} & 3e^{6(t-\\tau)}+e^{2(t-\\tau)} \\end{pmatrix}.\n  \\end{equation}\\] The complementary solution satisfying the initial conditions is \\[\\begin{equation}\n    y_c(t) = \\Psi(t)\\Psi^{-1}(0)y_0 = \\frac{1}{4}\\begin{pmatrix} e^{6t}+3e^{2t} & 3e^{6t}-3e^{2t} \\\\ e^{6t}-e^{2t} & 3e^{6t}+e^{2t} \\end{pmatrix}y_0.\n  \\end{equation}\\] The particular solution satisfying \\(y_p(0)=0\\) is \\[\\begin{align*}\n  y_p(t) &= \\int_0^t\\Psi(t)\\Psi^{-1}(\\tau)f(\\tau)\\, d\\tau  \\\\\n  &= \\frac{1}{4}\\int_0^t \\strut \\begin{pmatrix} \\tau  e^{6(t-\\tau)}+3\\tau e^{2(t-\\tau)} + 3e^{6t-5\\tau}-3e^{2t-\\tau} \\\\ \\tau e^{6(t-\\tau)}-\\tau e^{2(t-\\tau)} + 3e^{6t-5\\tau}+e^{2t-\\tau} \\end{pmatrix}\\begin{pmatrix} \\tau \\\\ e^{\\tau} \\end{pmatrix} \\, d\\tau.\\\\\n      &=  {\\small \\begin{pmatrix}1\\\\1\\end{pmatrix}}\\int_0^t\\tfrac{\\tau}{4}  e^{6(t-\\tau)} \\,d\\tau\n        + {\\small \\begin{pmatrix}3\\\\-1\\end{pmatrix}}\\int_0^t \\tfrac{\\tau}{4} e^{2(t-\\tau)} \\,d\\tau\n        + {\\small \\begin{pmatrix}3\\\\3\\end{pmatrix}}\\int_0^t \\tfrac{1}{4}e^{6t-5\\tau} \\,d\\tau\n        - {\\small \\begin{pmatrix}3\\\\-1\\end{pmatrix}}\\int_0^t \\tfrac{1}{4}e^{2t-\\tau} \\,d\\tau \\\\\n      &=  {\\tiny \\begin{pmatrix}1\\\\1\\end{pmatrix}} \\left( \\tfrac{1}{144}-\\tfrac{t}{24} -\\tfrac{1}{144}e^{6t} \\right)\n      + {\\tiny \\begin{pmatrix}3\\\\-1\\end{pmatrix}}  \\left(\\tfrac{1}{16} -\\tfrac{t}{8} -\\tfrac{1}{16}e^{2t} \\right)\n        + {\\tiny \\begin{pmatrix}3\\\\3\\end{pmatrix}}  \\left(\\tfrac{1}{20}e^{6t} - \\tfrac{1}{20}e^t \\right)\n    - {\\tiny \\begin{pmatrix}3\\\\-1\\end{pmatrix}}  \\left(\\tfrac{1}{4}e^{2t} -\\tfrac{1}{4}e^t  \\right) \\\\\n       &=\n          \\tfrac{113}{720} e^{6t}{\\small \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}}\n     -\\tfrac{3}{16}e^{2t} {\\small \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}}\n     -\\tfrac{t}{12}{\\small \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix}}\n     -\\tfrac{1}{36}{\\small  \\begin{pmatrix} 7 \\\\ 2 \\end{pmatrix}}\n     +\\tfrac{1}{5}e^t {\\small \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}}\n     \\end{align*}\\]\nNote the first two terms are in fact homogeneous solutions that arise here to ensure the particular solution is zero when \\(t=0\\).\nThe last three terms are of the form \\(At + B + Ce^t\\) which we would expect from the method of undetermined coefficients, with the caveat that \\(A\\), \\(B\\) and \\(C\\) are vectors.\nThe general solution is simply \\(y(t) = y_c(t) + y_p(t)\\) with the as-yet unspecified initial conditions taken as a pair of arbitrary constants."
  },
  {
    "objectID": "M3503-lec02-Systems.html#the-solution-via-laplace-transforms",
    "href": "M3503-lec02-Systems.html#the-solution-via-laplace-transforms",
    "title": "3  Systems of First order equations",
    "section": "5.2 The solution via Laplace Transforms",
    "text": "5.2 The solution via Laplace Transforms\nApplying the Laplace Transform to leads to \\[\\begin{equation} sY(s) - y_0 = AY(s) + F(s).\\end{equation}\\] Note that \\(Y\\) is a vector whose entries are the transforms of the corresponding entry of \\(y\\). Since the transform is a linear operation (an integration), the matrix is essential pulled out of the integral. Solving for \\(Y\\) we find \\[\\begin{equation}Y(s) = (sI-A)^{-1}\\left(y_0 + F\\right).\\end{equation}\\] Let \\(G(s) = (sI-A)^{-1}\\) and \\(g = {\\mathcal L}^{-1}\\left\\{G\\right\\}\\). Note that \\(G\\) and \\(g\\) are both \\(n\\times n\\) matrices. The solution to the differential equation can be represented as a convolution: \\[\\begin{equation}y(t) = g(t)y_0 + \\int_0^t g(t-\\tau)f(\\tau)\\, d\\tau.\\end{equation}\\]\n\nExample 5.2 Consider the system \\(y' = Ay + f\\) with \\(A = \\begin{pmatrix} 3 & 3 \\\\ 1 & 5 \\end{pmatrix}\\) and \\(f = \\begin{pmatrix} t \\\\ e^{t} \\end{pmatrix}.\\) \\[\\begin{align*}  \n    G(s) &= (sI-A)^{-1} \\\\\n         &= \\begin{pmatrix} s-3 & -3 \\\\ -1 & s-5 \\end{pmatrix}\\\\\n     &= \\begin{pmatrix} \\frac{s-5}{(s-6)(s-2)} &\\ & \\frac{3}{(s-6)(s-2)} \\\\[6pt] \\frac{1}{(s-6)(s-2)} &\\ & \\frac{s-3}{(s-6)(s-2)} \\end{pmatrix} \\\\[12pt]\n     &= \\frac{1}{4}\\begin{pmatrix} \\frac{1}{s-6} + \\frac{3}{s-2} &\\ & \\frac{3}{s-6} - \\frac{3}{s-2} \\\\[6pt] \\frac{1}{s-6} - \\frac{1}{s-2} &\\ & \\frac{3}{s-6} + \\frac{1}{s-2} \\end{pmatrix}\\\\[12pt]\n    g(t) &= \\frac{1}{4}\\begin{pmatrix} e^{6t} + 3e^{2t} &\\ & 3e^{6t} - 3e^{2t} \\\\[6pt] e^{6t} - e^{2t} &\\ & 3e^{6t} + e^{2t} \\end{pmatrix}\n  \\end{align*}\\]\nNote that the system for this example is the same as that of the previous example, and that \\(g(t-\\tau) = \\Psi(t)\\Psi^{-1}(\\tau)\\) where \\(\\Psi\\) is the fundamental matrix from the previous example. Of course, \\(g(t) = \\Psi(t)\\Psi^{-1}(0)\\), which means the rest of the computations follow the previous example exactly. The two methods are identical!\n\n\nExample 5.3  \nSolve the system \\(x' = Ax + g\\) where \\(A = \\begin{pmatrix} 4 & 3 \\\\ -1 & 0 \\end{pmatrix}\\) and \\(g = \\begin{pmatrix} 0 \\\\ t \\end{pmatrix}\\).\nThe matrix \\(A\\) has two eigenvalues, \\(r_1 = 1\\) and \\(r_2=3\\). Two associated eigenvectors are \\(u_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\) and \\(u_2 = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\\), respectively. Thus, a fundamental matrix for the system is \\[\\Phi(t) = \\begin{pmatrix} e^t & 3e^{3t} \\\\ -e^t & -e^{3t} \\end{pmatrix}.\\] The method of variation of parameters gives the general solution in the form \\[x(t) = \\Phi(t)\\int \\Phi^{-1}(t) g(t) \\, dt.\\] Direct computations lead to \\[\\begin{align*}\n  \\Phi^{-1}(t) &= \\frac{1}{2e^{4t}} \\begin{pmatrix} -e^{3t} & -3e^{3t} \\\\ e^t & e^t \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -e^{-t} & -3e^{-t} \\\\ e^{-3t} & e^{-3t} \\end{pmatrix}. \\\\\n  \\Phi^{-1}(t)g(t) &= \\frac{1}{2}\\begin{pmatrix} -e^{-t} & -3e^{-t} \\\\ e^{-3t} & e^{-3t} \\end{pmatrix} \\begin{pmatrix}0 \\\\ t \\end{pmatrix}\n                    =  \\begin{pmatrix}  -\\frac{3}{2}te^{-t} \\\\  \\frac{1}{2}te^{-3t} \\end{pmatrix}. \\\\\n  \\int\\Phi^{-1}(t)g(t)\\,dt &=  \\begin{pmatrix}  -\\frac{3}{2}\\int te^{-t} \\,dt \\\\  \\frac{1}{2} \\int te^{-3t}\\, dt \\end{pmatrix}\n                             =  \\begin{pmatrix}  \\frac{3}{2} e^{-t}(t+1) + c_1\\\\  \\frac{1}{18} e^{-3t}(3t+1) + c_2 \\end{pmatrix}. \\\\\n    \\Phi(t)\\int\\Phi^{-1}(t)g(t)\\,dt\n        & =  \\begin{pmatrix} e^t & 3e^{3t} \\\\ -e^t & -e^{3t} \\end{pmatrix} \\begin{pmatrix}  \\frac{3}{2} e^{-t}(t+1) + c_1\\\\  \\frac{1}{18} e^{-3t}(3t+1) + c_2 \\end{pmatrix}. \\\\\n    &=  \\begin{pmatrix} t +  \\frac{4}{3} \\\\ -t - \\frac{13}{9}\\end{pmatrix} + c_1  e^t \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n    + c_2  e^{-3t} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n  \\end{align*}\\]\nNote the last two terms are the complementary solution. We could have ignored the constants of integration and simply added these on afterwards."
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#sec-2ndordergt",
    "href": "M3503-lec03-SecondOrderODE.html#sec-2ndordergt",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.1 The general theory of second order Linear Differential Equations",
    "text": "4.1 The general theory of second order Linear Differential Equations\nThe first part of this course deals with second order linear differential equations with constant coefficients. However before we zero in on solution techniques for these equations it is worthwhile to take a brief look at the more general theory.\nMany physical problems can be cast as second order differential equations of the form \\(\\ddot{x} = f(t,x,\\dot{x})\\) with \\(\\ddot{x}\\) interpreted as acceleration, \\(x\\) as position, and \\(\\dot{x}\\) as velocity. These often arise as initial value problems, when combined with initial conditions of the form \\(x(0) = x_0\\), \\(\\dot{x}(0) = y_0\\).\nSome cases appear as boundary value problems with the conditions prescribed at two different values of the independent variable. \\(x(0) = a\\), \\(x(1) = b\\).\n\n4.1.1 Differential Equations and Linear Algebra\nA second order linear differential equation has the form. \\[P(x)\\frac{d^2y}{dx^2} + Q(x)\\frac{dy}{dx} + R(x)y = G(x), \\tag{4.1}\\] where the functions \\(P\\), \\(Q\\), \\(R\\) and \\(G\\) are specified. To motivate these as linear we need to review a few concepts from linear algebra. In particular, we need to learn to view functions as vectors.\nOne key concept from linear algebra is a linear combination. Suppose \\(y_1, y_2, \\dots, y_n\\) are \\(n\\) functions defined on some interval \\([a,b]\\). For example, the set \\(1, x, x^2, x^3, x^4, x^5\\) with \\(0 \\le x \\le 1\\) is a set of 6 functions defined on the interval \\([0,1]\\). Using the jargon we learned in our first linear algebra course, we can construct any polynomial of degree 5 or lower by taking linear combinations of the 6 basis functions. Specifically, if \\(c_0, c_1, \\dots, c_5\\) be any 6 numbers, then \\(y(x) = c_0 + c_1x + \\dots + c_5x^5\\) is a polynomial. If \\(c_5\\ne0\\), \\(y\\) is a polynomial of degree 5, otherwise, it has a lower degree. We can do the same constructions for any set of functions. Given a set of functions \\(\\{y_1, y_2, \\dots, y_n\\}\\) all with the same domain, \\([a,b]\\), and a set of scalars \\(\\{c_1, \\dots, c_n\\}\\), the linear combination \\(y = c_1y_1 + \\dots + c_ny_n\\) is a function with the same domain.\nFrom our calculus course, we know that if \\(y_1, \\dot, y_n\\) are all continuous on \\([a,b]\\), then the linear combination is also continuous. In short, functions are vectors, and all the jargon we learned in linear algebra can be applied to functions.\n\nDefinition 4.1 The set of functions \\(\\{y_1, y_2, \\dots, y_n\\}\\) is said to be linearly independent if the only scalars \\(c_1,\\dots,c_n\\) for which \\(c_1y_1 + \\dots + c_ny_n = 0\\) is \\(c_1=c_2=\\dots=c_n=0\\).\n\n\nExample 4.1 To show the two functions \\(\\sin x\\) and \\(\\cos x\\) defined for \\(0 \\le x \\le \\pi\\) are linearly independent, we need to show that \\(c_1\\sin x + c_2\\cos x = 0\\) for all \\(x\\in[0,\\pi]\\) only if \\(c_1=c_2=0\\). Since \\(\\sin 0 = 0\\) and \\(\\cos 0 = 1\\), setting \\(x=0\\) implies \\(c_2=0\\). Similarly, setting \\(x=\\pi/2\\) implies \\(c_1=0\\). Hence, the two functions are linearly independent.\n\nSo why do we call Equation 4.1 a linear differential equation? It is useful to introduce a shorthand notation for the left hand side of Equation 4.1. In more advanced courses this would be introduced as a linear operator and some fancy theorems would be invoked. An operator is simply a mathematical object that maps functions to other functions in the same way the functions of first year calculus map numbers to other numbers. Suppose \\(y\\) is twice differentiable, then we can define a new function, \\(Ly\\), by \\[\\begin{equation}\n  Ly = P\\frac{d^2y}{dx^2} + Q\\frac{dy}{dx} + Ry.\n\\end{equation}\\] We use this to define the operator \\(L\\). Simply put, given a known, twice differentiable function \\(y\\), the operation \\(L\\) returns a new function which we refer to simply as \\(Ly\\). The linearity of the operator refers to the fact that \\(L\\) is a linear transformation. That is, for any two twice-differentiable functions \\(u\\) and \\(v\\) and any two real numbers (scalars) \\(a\\) and \\(b\\), \\(L(au+bv) = aLu + bLv\\).\n\n\n4.1.2 The solution set\nA solution to the second order differential equation Equation 4.1, is any twice differentiable function \\(y(x)\\) that satisfies Equation 4.1. Since we are looking for \\(y\\) as a function of \\(x\\), it makes sense to refer to \\(x\\) as our independent variable and \\(y\\) as our dependent variable. The linearity of the equation makes some more jargon from linear algebra useful. In particular,\n\n\\(P\\), \\(Q\\) and \\(R\\) are the coefficients of the equation,\nthe equation is homogeneous if \\(G=0\\),\nthe equation is nonhomogeneous if \\(G\\ne 0\\),\nthe equation is constant coefficient if \\(P\\), \\(Q\\) and \\(R\\) constants.\n\nThe right hand side, \\(G\\), is typically an applied force or current or something similar. Hence we refer to \\(G\\) as the forcing function.\nThere are two theorems that are relevant here.\n\nTheorem 4.1 If \\(y_1\\) and \\(y_2\\) are solutions of the homogeneous problem then for any constants \\(c_1\\) and \\(c_2\\), the linear combination \\(y = c_1y_1 + c_2y_2\\) is also a solution of the homogeneous problem.\n\nIn the language of linear algebra, any linear combination of two solutions is also a solution. The proof follows almost immediately from substituting the linear combination of the two solutions into the differential equation.\n\nTheorem 4.2 If \\(y_1\\) and \\(y_2\\) are two linearly independent solutions to the homogeneous problem and \\(y_p\\) is any solution to the nonhomogeneous problem, then \\(y = c_1y_1 + c_2y_2 + y_p\\) is also a solution to the nonhomogeneous problem for any constant \\(a\\).\n\nAgain, the proof is simply a matter of plugging the proposed solution into the differential equation.\n\nTheorem 4.3 If \\(y_1\\) and \\(y_2\\) are two linear independent solutions of the homogeneous problem and \\(y_p\\) is any solution to the nonhomogeneous problem, then every solution of the homogeneous problem can be written in the form \\(y = c_1y_1 + c_2y_2 + y_p\\) for some real numbers \\(c_1\\) and \\(c_2\\).\n\nIn short, this theorem states that if we can find two linearly independent solutions, then we know all the solutions to the homogeneous problem, and if we also know any one solution to the nonhomogeneous problem, we know them all. The form of the solution set should look very familiar. It has the same structure as the set of solutions to a system of linear algebraic equations.\nThe proof of this theorem is beyond the scope of this course. However, we will be able to give a sketch of the proof later in the term.\nMany of the theorems from linear algebra apply to the operator \\(L\\). We refer to \\(Ly=0\\) as the homogeneous, or complementary problem, and \\(Ly = G\\) as the nonhomogeneous problem. As with linear systems, if \\(y_p\\) is a solution to the nonhomogeneous problem and \\(y_h\\) is any solution to the homogeneous problem, then \\(y = cy_h + y_p\\) is also a solution to the nonhomogeneous problem, for any constant \\(c\\). This can be easily verified by direct substitution. Further, if \\(y_1\\) and \\(y_2\\) are both solutions to the homogeneous problem, then any function of the form \\(y = c_1y_1 + c_2 y_2\\) is also a solution to the homogeneous problem. Note the slight difference in the wording of these results from the theorems stated above. When \\(L\\) is a second order linear differential operator, such as arises from second order linear differential equation, then it can further be shown that the homogeneous problem \\(Ly=0\\) has a two dimensional solution set. That is, we can always find two linearly independent solutions, \\(y_1\\) and \\(y_2\\) and every solution to \\(Ly=0\\) can be written as a linear combination of these. This follows from a theorem for the existence and uniqueness of solutions to systems of first order differential equations, which will be sketched later in these notes.\nLater in the notes we will also encounter the eigenvalue problem \\(Ly=\\lambda y\\), where we are interested in finding values of the constant \\(\\lambda\\) for which solutions to the eigenvalue problem exist. These typically arise in boundary value problems, and the eigenvalues correspond to modes of oscillation in the physical system.\n\n\n\n\n\nFigure 4.1: Mass spring system\n\n\n\nExample 4.2 Consider the mass-spring system of Figure Figure 4.1 with a mass of \\(m\\) attached to a spring with spring constant \\(k\\). A differential equation model for the system can be obtained either by energy balance or force balance. The energy of the system is \\[\\begin{align*}\n    \\text{Total Energy} &= \\text{Kinetic Energy} + \\text{Spring Potential} \\\\\n    &= \\frac{1}{2}m\\dot{x}^2 + \\frac{1}{2}kx^2\n  \\end{align*}\\] Differentiating and assuming the total energy is conserved (constant) leads to the second order differential equation \\(0 = m\\dot{x}\\ddot{x} + kx\\dot{x}\\) which simplifies to \\(m\\ddot{x} + kx = 0.\\) Since both \\(k\\) and \\(m\\) are positive constants, the solutions to the equation are functions whose second derivatives are negative multiples of themselves: \\(\\ddot{x} = -\\frac{k}{m}x\\). These are sines and cosines of the appropriate period: \\[x_1(t) = \\cos \\tfrac{k}{m} t, \\qquad x_2(t) = \\sin \\tfrac{k}{m} t.\\] The general solution to the differential equation is any linear combination of these two solutions: \\(x(t) = c_1\\cos \\tfrac{k}{m} t \\, + \\, c_2 \\sin \\tfrac{k}{m} t.\\)"
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#sec-2ndordercc",
    "href": "M3503-lec03-SecondOrderODE.html#sec-2ndordercc",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.2 Linear Second order differential equations with constant coefficients",
    "text": "4.2 Linear Second order differential equations with constant coefficients\nConsider the problem\n\\[ay'' + by' +cy = 0 \\tag{4.2}\\]\nBased on our experience with first order equations (a=0), we look for a solution of the form \\(y=e^{rx}\\). If we can find two such solutions, we’ve got them all! First, we substitute the ansatz, \\(y(x) = e^{rx}\\), into Equation 4.1 \\[\\begin{align*}\n  a\\frac{d^2}{dx^2}\\left(e^{rx}\\right) + b\\frac{d}{dx}\\left(e^{rx}\\right) +ce^{rx} &= 0 \\\\\n  ar^2e^{rx} + bre^{rx} +ce^{rx} &= 0 \\\\\n  \\left(ar^2 + br +c\\right)e^{rx} &= 0\n\\end{align*}\\] Factoring out \\(e^{rx}\\) we see that \\(r\\) must satisfy \\(ar^2 + br +c = 0\\). That is, \\[r = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}.\\]\nThere are three cases to consider.\nCase I: two real and distinct roots (\\(b^2-4ac>0\\))\nThe general solution has the form \\[y = c_1e^{r_1x} + c_2e^{r_2x}\\] where \\(r_1\\) and \\(r_2\\) are the two real roots.\nCase II: one real root (\\(b^2 - 4ac = 0\\))\nHere we only have one independent solution \\(y_1 = c_1e^{rx}\\) with \\(r=-b/(2a)\\).\nAt some point in history, someone stumbled upon a second independent solution, \\(y_2 = xe^{rx}\\), with \\(r=-b/(2a)\\). While this it is not at all obvious why this should be a solution and how someone would come up with it, it is easy to verify that is is a solution and that \\(y_1\\) and \\(y_2\\) are linearly independent.\nWe leave it as an exercise to show, by direct substitution, that \\(y_2\\) solves Equation 4.2. That is, show that \\[a\\frac{d^2}{dx^2}\\left(xe^{rx}\\right) + b\\frac{d}{dx}\\left(xe^{rx}\\right) +cxe^{rx} = 0\\] provided \\(b^2 - 4ac =0\\) and \\(r=b/(2a)\\).\nTo show the two solutions are linearly independent, consider the combination \\[\\alpha e^{rx} + \\beta xe^{rx} = 0.\\]\nThe key concept with linear independence of functions, is that the equality must hold for all \\(x\\) in the domain of the functions. In this case, since \\(e^{rx} > 0\\), it follows that \\(\\alpha + \\beta x = 0\\), which holds only for \\(x=-\\alpha/\\beta\\). Hence the two functions are linearly independent on any interval of nonzero length, which is all we care about.\nIn summary, the general solution in case II has the form \\[y = (c_1+c_2x)e^{rx} \\] with \\(r = -b/(2a)\\).\nCase III: complex roots (\\(b^2 - 4ac < 0\\))\nIn this case, we have two complex roots, which we will express as \\(r_1 = \\alpha - \\beta i\\) and \\(r_2 = \\alpha + \\beta i\\). For those who are comfortable with complex functions, you will be happy to know that the two functions \\(e^{r_1 x}\\) and \\(e^{r_2 x}\\) are indeed linearly independent.\nThe only catch is that we started with a problem involving real variables and real functions and it would be nice to have real solutions. Fortunately, it is possible to show that the real and complex parts of the complex solutions are also linearly independent solutions. Hence, the general solution has the form\n\\[y(x) =  \\left( c_1 \\cos \\beta x  + c_2 \\sin  \\beta x  \\right) e^{\\alpha x}\\]"
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#initial-and-boundary-value-problems",
    "href": "M3503-lec03-SecondOrderODE.html#initial-and-boundary-value-problems",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.3 Initial and boundary value problems",
    "text": "4.3 Initial and boundary value problems\nThe general form of the solution has two arbitrary constants. Thus it is a two-parameter family of solutions. In practice, we are often interested in a solution satisfying additional constraints. These are typically posed as either initial conditions or boundary conditions. Initial conditions are given as constraints of the form \\(y(0) = y_1\\) and \\(y'(0) = y_2\\) for some specified constants \\(y_1\\) and \\(y_2\\). Boundary conditions are typically given as \\(y(0) = y_0\\) and \\(y(1) = y_1\\), but could involve derivatives of \\(y\\) at the boundaries.\n\nExample 4.3 Find a solution to \\(y'' + 2y' - 3 = 2x\\) satisfying the initial conditions \\(y(0) = 0\\) and \\(y'(0) = 1\\)."
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#summary",
    "href": "M3503-lec03-SecondOrderODE.html#summary",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.4 Summary",
    "text": "4.4 Summary\nTo find two linearly independent solutions to a homogeneous second order linear differential equation with constant coefficients, \\(ay'' + by' + cy' = 0,\\) first compute the characteristic roots \\(r = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}.\\) second, determine which of the following three cases holds and write down the appropriate solutions:\nTwo distinct real roots (\\(b^2 > 4ac\\)): \\(y_1 = e^{r_1t} \\qquad y_2 = e^{r_2t}\\) with \\(r_1 = \\frac{-b - \\sqrt{b^2-4ac}}{2a}\\), and \\(r_2 = \\frac{-b + \\sqrt{b^2-4ac}}{2a}\\).\nA double root (\\(b^2 = 4ac\\)): \\(y_1 = e^{rt} \\qquad y_2 = te^{rt}\\) with \\(r = \\frac{-b}{2a}\\).\nComplex roots (\\(b^2 < 4ac\\)): \\(y_1 = e^{\\alpha t} \\cos \\beta t \\qquad y_2 = e^{\\alpha t} \\sin \\beta t,\\) with \\(\\alpha = -\\frac{b}{2a}\\), and \\(\\beta = \\frac{\\sqrt{4ac-b^2}}{2a}\\)."
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#discussion",
    "href": "M3503-lec03-SecondOrderODE.html#discussion",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.5 Discussion",
    "text": "4.5 Discussion\nNote that we refer to Equation 4.1 as linear when it is linear in the dependent variable. It may be nonlinear in the independent variable. For example,\napplying Kirchoff’s law to a simple electric circuit yields the model \\[LQ''(t) + RQ'(t) + \\frac{1}{C}Q(t) = E(t)\\] where the dependent variable, \\(Q\\), is the charge, and is assumed to depend on the time, \\(t\\). The parameters \\(L\\), \\(R\\) and \\(C\\) are the inductance, resistance and capacitance of the circuit and \\(E\\) is an applied voltage. If the fixed resistance \\(R\\) is replaced by a variable resistance, \\(R(t)\\), the equation is still linear in the dependent variable.\nMore generally, a differential equation is an equation involving a function and some of its derivatives.\nFor example, a simple model for the angular deflection \\(\\theta(t)\\) of a pendulum of length \\(l\\) is \\[l\\theta''(t) + g\\sin \\theta(t) = 0,\\] which arises from balancing kinetic and potential energies. This equation is nonlinear in the dependent variable \\(\\theta\\). For small angles, we can approximate \\(\\sin(\\theta)\\) by \\(\\theta\\) leading to the linear second order constant coefficient model \\[l\\theta''(t) + g\\theta(t) = 0.\\] Most systems are nonlinear. However most analyses begin with the study of linear approximations. Hence the importance of studying linear differential equations."
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#theoretical-considerations",
    "href": "M3503-lec03-SecondOrderODE.html#theoretical-considerations",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.6 Theoretical Considerations",
    "text": "4.6 Theoretical Considerations\nA good treatment of the existence and uniqueness of solutions to differential equations can be found in Hartman. This material is approachable, but requires a deeper exposure to linear algebra than you received in Math 1503.\nConsider the general Second order linear ODE of Equation 4.1 with \\(G=0\\). Let \\(w(x) = y'(x)\\) and write the single equation as\n\\[P(x)w'(x) + Q(x)w(x) + R(x)y(x) = 0,\\]\nwhich leads to the linear system \\[\\begin{align}\n  w'(x) &=  - \\frac{Q(x)}{P(x)}w(x) - \\frac{R(x)}{P(x)}y(x), \\\\\n  y'(x) &= w(x)\n\\end{align}\\] Hartman’s text deals with the more general equation \\(\\vec{y}' = f(x,\\vec{y})\\) where \\(\\vec{y}\\) and \\(f\\) are vector functions. The result for our simpler linear system is that the initial value problem has a unique solution provided \\(Q/P\\) and \\(R/P\\) are both continuous."
  },
  {
    "objectID": "M3503-lec03-SecondOrderODE.html#finding-particular-solutions-of-the-nonhomogeneous-problem",
    "href": "M3503-lec03-SecondOrderODE.html#finding-particular-solutions-of-the-nonhomogeneous-problem",
    "title": "4  Second order Linear Differential Equations",
    "section": "4.7 Finding particular solutions of the nonhomogeneous problem",
    "text": "4.7 Finding particular solutions of the nonhomogeneous problem\n\n4.7.1 Reduction of order\nConsider the general linear second order differential equation, \\(P(x)y'' + Q(x)y' + R(x)y = G(x)\\), and suppose we know one solution, \\(y_1\\) to the homogeneous problem (\\(G=0\\)). A well-established trick to finding more solutions to the differential equation is to look for solutions of the form \\(y(x) = u(x)y_1(x)\\).\n\\[\\begin{align*}\n  P \\left(u''y_1 + 2u'y_1'+uy_1''\\right) + Q\\left(u'y_1+uy_1'\\right) + Ruy_1 &= G \\\\\n  P \\left(u''y_1 + 2u'y_1'\\right) + Qu'y_1 + Puy_1''+Quy_1' +Ruy_1 &= G \\\\\n  P \\left(u''y_1 + 2u'y_1'\\right) + Qu'y_1  = G\n\\end{align*}\\] Since \\(y_1\\) is a known solution, to the nonhomogeneous problem, the last few terms cancel leaving us with an equation that only involves the derivatives of \\(u\\). Setting \\(v = u'\\) reduces the order to a first order differential equation in \\(v\\): \\(Py_1 v' + \\left(2y_1' + Qy_1\\right)v = G.\\) This equation is first order and linear, and can be solved using an integrating factor."
  },
  {
    "objectID": "M3503-lec04-LaplaceTransforms.html#definition-and-basic-properties",
    "href": "M3503-lec04-LaplaceTransforms.html#definition-and-basic-properties",
    "title": "5  Laplace Transforms",
    "section": "5.1 Definition and basic properties",
    "text": "5.1 Definition and basic properties\nWe will see two integral transfoms in this course: the Fourier Transform and the Laplace Transform. In general, an integral transform takes a fuction, \\(f\\), of one variable defined on an interval \\([a,b]\\), multplies it by a kernel, \\(k\\), depending on two variables, and integrates the product over the domain \\([a,b]\\). The result is a function of the second variable appearing in the kernel: \\[\\begin{equation}\n  F(s) = \\int_a^b k(t,s) f(t)\\, dt\n\\end{equation}\\] The Laplace Transform takes a function \\(f\\) defined on \\((0,\\infty)\\) and transforms it using the kernel \\(k(t,s) = e^{-st}\\). \\[\\begin{equation}\n  F(s) = \\int_0^\\infty e^{-st} f(t)\\, dt\n\\end{equation}\\] We can and will think of the transform as a function whose inputs are functions of time, and whose outputs are functions of the second variable \\(s\\). We’ll use a fancy L to denote this function. Hence, the Laplace Transform of a function \\(f\\) is the function \\(F\\) defined by \\[F(s) = {\\mathcal L}\\left\\{f\\right\\}(s) = \\int_0^\\infty e^{-st}f(t)\\, dt\\]"
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#vectors",
    "href": "M3503-lec05-FourierSeries.html#vectors",
    "title": "6  Fourier Series",
    "section": "6.1 Vectors",
    "text": "6.1 Vectors"
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#inner-products",
    "href": "M3503-lec05-FourierSeries.html#inner-products",
    "title": "6  Fourier Series",
    "section": "6.2 Inner Products",
    "text": "6.2 Inner Products"
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#the-usual-fourier-series-for-f",
    "href": "M3503-lec05-FourierSeries.html#the-usual-fourier-series-for-f",
    "title": "6  Fourier Series",
    "section": "6.3 The usual Fourier Series for \\(f\\)",
    "text": "6.3 The usual Fourier Series for \\(f\\)"
  },
  {
    "objectID": "M3503-lec05-FourierSeries.html#a-more-general-view-of-fourier-series",
    "href": "M3503-lec05-FourierSeries.html#a-more-general-view-of-fourier-series",
    "title": "6  Fourier Series",
    "section": "6.4 A more general view of Fourier Series",
    "text": "6.4 A more general view of Fourier Series\nSuppose \\(f\\) is a piecewise continuous function defined on an interval \\([-l,l]\\).\nFurther, suppose the set of function \\(\\{\\phi_0,\\phi_1,\\phi_2,\\dots\\}\\) is orthogonal with respect to some given inner product. Then the fourier series for \\(f\\) with respect to this set is \\(\\displaystyle\\sum_{m=0}^\\infty c_m \\phi_m(x)\\) with \\(c_m = \\dfrac{\\langle f,\\phi_m\\rangle}{\\langle\\phi_m,\\phi_m\\rangle}\\).\nThe usual fourier series for \\(f\\) is obtained when \\(\\phi_n(x) = \\begin{cases} 1 & m=0, \\\\ \\cos(\\frac{m\\pi x}{2l}) & \\text{$m$ even and positive} \\\\ \\sin(\\frac{(m+1)\\pi x}{2l}) & \\text{$m$ odd} \\end{cases}\\) and the inner product of two functions, \\(u\\) and \\(v\\), is defined as \\(\\langle u,v\\rangle = \\int_{-l}^{l} u(x) v(x) \\, dx\\)."
  }
]